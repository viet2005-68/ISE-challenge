{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bc10c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY='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'\n",
    "FIRECRAWL_API_KEY='fc-e36282d62d1c4ee8971ec0649f3862c7'\n",
    "BASE_URL='https://api.groq.com/openai/v1'\n",
    "MODEL=\"meta-llama/llama-4-scout-17b-16e-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "be6ff6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/mnt/g/Ai agents course/LangGraph/venv/lib/python3.12/site-packages', '..', '..', '..', '..']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "print(sys.path)\n",
    "from tools.fetching_description_from_huggingface import fetching_description_from_huggingface\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import Tool\n",
    "from firecrawl import FirecrawlApp\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "049f9c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task = pd.read_csv(\"../Test/data/ISE - AutoCode Challenge 2 Public - Public task.csv\")\n",
    "df_model = pd.read_csv(\"../Test/data/ISE - AutoCode Challenge 2 Public - Model zoo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9ae8b146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STT</th>\n",
       "      <th>Task</th>\n",
       "      <th>Validation data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nNháº­n dáº¡ng chá»¯ viáº¿t ta...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nTrong thá»i Ä‘áº¡i thÃ´ng t...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nÄÃ¢y lÃ  má»™t nhiá»‡m vá»¥ há»“...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nÄiá»u quan trá»ng lÃ  cÃ¡c...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nTrong nhiá»u á»©ng dá»¥ng t...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>ğŸ¾ Story: Aliceâ€™s Magical Pet Photo Box\\r\\nAlic...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STT                                               Task Validation data\n",
       "0    1  \"Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nNháº­n dáº¡ng chá»¯ viáº¿t ta...            Link\n",
       "1    2  Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nTrong thá»i Ä‘áº¡i thÃ´ng t...            Link\n",
       "2    3  Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nÄÃ¢y lÃ  má»™t nhiá»‡m vá»¥ há»“...            Link\n",
       "3    4  Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nÄiá»u quan trá»ng lÃ  cÃ¡c...            Link\n",
       "4    5  Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nTrong nhiá»u á»©ng dá»¥ng t...            Link\n",
       "5    6  ğŸ¾ Story: Aliceâ€™s Magical Pet Photo Box\\r\\nAlic...            Link"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f1db413a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_list(df):\n",
    "    model_strings = []\n",
    "    for i, row in df.iterrows():\n",
    "        desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        url = str(row[2])\n",
    "        model_strings.append(f\"{i + 1} - {desc} [More info]({url})\")\n",
    "    return \"\\n\".join(model_strings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2db60a",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "df5acb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraper(url: str) -> str:\n",
    "    \"\"\"Get more details about the model using the model url\"\"\"\n",
    "    # scraper = FirecrawlApp(api_key=FIRECRAWL_API_KEY)\n",
    "    # scrape_status = scraper.scrape_url(\n",
    "    #     url,\n",
    "    #     formats=['markdown']\n",
    "    # )\n",
    "    # return scrape_status.markdown\n",
    "    result = fetching_description_from_huggingface(url)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "633f0e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraperParameters(BaseModel):\n",
    "    url: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2e796564",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = web_scraper('https://huggingface.co/thanhtlx/image_classification_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "05774828",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"web_scrapper\",\n",
    "        func=web_scraper,\n",
    "        description=\"Use to get more detail about the model\",\n",
    "        args_schema=WebScraperParameters\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb297f",
   "metadata": {},
   "source": [
    "# Output Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4858f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tasks(BaseModel):\n",
    "    subtask_one: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 1\"\n",
    "    )\n",
    "    subtask_two: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 2\"\n",
    "    )\n",
    "    subtask_three: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 3\"\n",
    "    )\n",
    "    subtask_four: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 4\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "96d9d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInfo(BaseModel):\n",
    "    model: str = Field(..., description=\"Model name and description\")\n",
    "    url: str = Field(..., description=\"URL to the model\")\n",
    "\n",
    "class ModelSelection(BaseModel):\n",
    "    models: List[ModelInfo] = Field(..., description=\"List of available models with name and URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "31669a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelingOutput(BaseModel):\n",
    "    model_description: str = Field(\n",
    "        ...,\n",
    "        description=\"The detailed description of the model.\"\n",
    "    )\n",
    "    model_input_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed input format\"\n",
    "    )\n",
    "    model_output_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed output format\"\n",
    "    )\n",
    "    model_requirements: str = Field(\n",
    "        ...,\n",
    "        description=\"Requirement libraries need to install to use the model.\"\n",
    "    )\n",
    "    model_sample_code: str = Field(\n",
    "        ...,\n",
    "        description=\"Example code to use the model\"\n",
    "    )\n",
    "\n",
    "class ModelList(BaseModel):\n",
    "    models: List[ModelingOutput] = Field(\n",
    "        ...,\n",
    "        description=\"List of suitable model for given task\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696b2753",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b99422e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI( \n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e591bec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: HumanMessage | None\n",
    "    data: str | None\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    subtasks: Tasks\n",
    "    models: ModelSelection\n",
    "    model_description: str\n",
    "    model_detailed_list: ModelList\n",
    "    output_classes: str | None\n",
    "    code: str | None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feb0de9",
   "metadata": {},
   "source": [
    "## Task Formulation Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "756a713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_agent(state: AgentState) -> AgentState:\n",
    "    structured_llm = llm.with_structured_output(Tasks)\n",
    "    system_prompt = \"\"\"\n",
    "        You are an expert in task formulation. Given a machine learning task, you will\n",
    "        help divide the task into these subtasks:\n",
    "        - Subtask1: for identifying task description and classify the problem type.\n",
    "        - Subtask2: for identifying the problem requirements as described in the \"YÃªu cáº§u cá»¥ thá»ƒ cáº§n Ä‘áº¡t Ä‘Æ°á»£c\" (or other equivalent) section, including: problem requirements, expected input and expected output.\n",
    "        - Subtask3: for identifying the input format of the problem, as being described in the \"Äá»‹nh dáº¡ng dá»¯ liá»‡u Ä‘áº§u vÃ o cho bÃ i toÃ¡n tá»•ng thá»ƒ\" (or other equivalent) section.\n",
    "        - Subtask4: for identifying the output format of the problem with the given classes, as being described in the \"Äá»‹nh dáº¡ng káº¿t quáº£ Ä‘áº§u ra mong muá»‘n cho bÃ i toÃ¡n tá»•ng thá»ƒ\" (or other equivalent) section.\n",
    "\n",
    "        You **MUST** follow these guidelines:\n",
    "        - Do not mistake the output classes with the one described in the problem requirements. The output classes **MUST** contain the classes described in the \"Äá»‹nh dáº¡ng káº¿t quáº£ Ä‘áº§u ra mong muá»‘n cho bÃ i toÃ¡n tá»•ng thá»ƒ\" section.\n",
    "        - Describe the subtasks as specific as possible.\n",
    "        \"\"\"\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    messages.append(state['task'])\n",
    "    result = structured_llm.invoke(messages)\n",
    "    state['subtasks'] = result\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f370b0a",
   "metadata": {},
   "source": [
    "### Testing task formulation agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f0a610a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = df_task[\"Task\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "9518d8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"task\": HumanMessage(content=task),\n",
    "    \"messages\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4685d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = classify_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e30062ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Identify the task description and classify the problem type. The task is to '\n",
      " \"build an intelligent system that can analyze each image in Alice's magical \"\n",
      " 'box and decide what it contains. The problem type is image classification '\n",
      " 'with object detection and emotion recognition.')\n",
      "(\"Identify the problem requirements as described in the 'YÃªu cáº§u cá»¥ thá»ƒ cáº§n \"\n",
      " \"Ä‘áº¡t Ä‘Æ°á»£c' section. The system should:\\n\"\n",
      " '  - Detect all visible dogs in an image and identify their breed.\\n'\n",
      " '  - Return the most frequent breed in the image if dogs are present.\\n'\n",
      " \"  - Look at a cat's facial expression and determine its most likely emotion \"\n",
      " 'if a single cat is present.\\n'\n",
      " \"  - Return 'unsure' if the image is unclear or no visible pet is present.\\n\"\n",
      " '  - Save the result to a file named output.csv with the format: id (image '\n",
      " \"filename without extension), prediction (most frequent dog breed, cat's \"\n",
      " \"emotion, or 'unsure').\")\n",
      "('Identify the input format of the problem. The images are stored in the '\n",
      " \"directory 'images/' and are in .jpg or .png format, typically containing a \"\n",
      " 'pet in the foreground.')\n",
      "('Identify the output format of the problem. The output should be saved to a '\n",
      " 'file named output.csv with the following format:\\n'\n",
      " '  - id: the image filename without extension (e.g., 001.png â†’ id = 001)\\n'\n",
      " '  - prediction: \\n'\n",
      " '    - The most frequent dog breed (if dogs are present)\\n'\n",
      " \"    - The cat's emotion (if a single cat is present)\\n\"\n",
      " \"    - 'unsure' (if the content is ambiguous or invalid)\")\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(state['subtasks'].subtask_one)\n",
    "pprint.pprint(state['subtasks'].subtask_two)\n",
    "pprint.pprint(state['subtasks'].subtask_three)\n",
    "pprint.pprint(state['subtasks'].subtask_four)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53441aef",
   "metadata": {},
   "source": [
    "## Model Selection Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a7a81ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_parser = PydanticOutputParser(pydantic_object=ModelSelection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b1e8ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the suitable model for a given task.\n",
    "                    Given:\n",
    "                    - A user description about the task\n",
    "                    - A list of available model (names and links)\n",
    "                    Your job is to choose ALL suitable models provided in the list for user's specific tasks.\n",
    "                    Return the answer in the format\n",
    "                    {structured_output}\n",
    "                    Here are the list of model:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]).partial(structured_output=model_selection_parser.get_format_instructions())\n",
    "\n",
    "    chain = prompt | llm | model_selection_parser\n",
    "\n",
    "    result = chain.invoke({\"input\": state['task'], \"model_list\": format_model_list(df_model)})\n",
    "    state['models'] = result\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8ef9e0",
   "metadata": {},
   "source": [
    "### Testing model selection agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "2b627753",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14401/7332665.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
      "/tmp/ipykernel_14401/7332665.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  url = str(row[2])\n"
     ]
    }
   ],
   "source": [
    "state = model_selection_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "5e26c92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelInfo(model='PhÃ¢n loáº¡i hÃ¬nh áº£nh giá»‘ng chÃ³', url='https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit'),\n",
      " ModelInfo(model='Nháº­n diá»‡n cáº£m xÃºc mÃ¨o', url='https://huggingface.co/semihdervis/cat-emotion-classifier'),\n",
      " ModelInfo(model='PhÃ¢n loáº¡i hÃ¬nh áº£nh thÃ nh100 lá»›p Ä‘á»‘i tÆ°á»£ng chi tiáº¿t (fine-grained)', url='https://huggingface.co/zhaospei/Model_6')]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(state['models'].models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6461f0b3",
   "metadata": {},
   "source": [
    "## Model Selection using Tool Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c39fcf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_using_tool_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the **Best-fit model** for a given task.\n",
    "                    Given:\n",
    "                    - A user task description\n",
    "                    - A list of available models (names and url)\n",
    "                    - A tool that fetches model details from a provided URL (it help gathers model description, input/output format, code sample usage)\n",
    "\n",
    "                    Your job is to:\n",
    "                    1. Use the **provided tool** to retrieve real details about each candidate model:\n",
    "                        - âœ… Model description\n",
    "                        - ğŸ“¥ Input format\n",
    "                        - ğŸ“¤ Output format\n",
    "                        - ğŸ› ï¸ Library requirements\n",
    "                        - ğŸ§ª Code sample (usage code snippet)\n",
    "                    2. Select the best model for user given task based on the data that the tool gives you.\n",
    "\n",
    "                    âš ï¸ VERY IMPORTANT RULES\n",
    "\n",
    "                    - âŒ DO NOT assume or invent any part of the model's description, input/output format, requirements or code\n",
    "                    - âŒ DO NOT generate fake code or use your own knowledge about the model\n",
    "                    - âœ… ONLY use the **actual output** returned from the tool\n",
    "                    - âœ… Include tool content in your final answer exactly as returned (especially code)\n",
    "\n",
    "                    ## ğŸ§  Output Structure (Final Answer)\n",
    "\n",
    "                    After retrieving tool results, choose best-fit model for the task (You may need to choose more than one model for some tasks), only choose the model \n",
    "                    that strictly relevant to the task and output the model result in the following format:\n",
    "\n",
    "                    **âœ… Model name and link**  \n",
    "                    `<model name>` â€” `<link>`\n",
    "\n",
    "                    **ğŸ“ Description (from tool):**  \n",
    "                    <model description>\n",
    "\n",
    "                    **ğŸ“¥ Input format (from tool):**  \n",
    "                    <description of expected input>\n",
    "\n",
    "                    **ğŸ“¤ Output format (from tool):**  \n",
    "                    <description of model output>\n",
    "\n",
    "                    **ğŸ› ï¸ Library Requirements (from tool) **\n",
    "                    <requirements to use the model>\n",
    "\n",
    "                    **ğŸ§ª Example code (from tool):**  \n",
    "                    ```python\n",
    "                    <exact code snippet from tool>\n",
    "\n",
    "                    Here are the list of models and there corresponding URL:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"ai\", \"{agent_scratchpad}\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "    result = executor.invoke({\"input\": state['task'], \"model_list\": state['models'].models})\n",
    "    state['model_description'] = result['output']\n",
    "    return state\n",
    "    # return {\"output\": result[\"output\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b68d61",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "6e0140d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_3'}`\n",
      "responded: Given the task description and the list of available models, I will first use the `web_scrapper` function to retrieve details about each candidate model.\n",
      "\n",
      "I have two models:\n",
      "- Dá»± Ä‘oÃ¡n giÃ¡ trung bÃ¬nh nhÃ  á»Ÿ cÃ¡c quáº­n Cali â€” https://huggingface.co/zhaospei/Model_3\n",
      "- Dá»± Ä‘oÃ¡n giÃ¡ trung bÃ¬nh nhÃ  á»Ÿ cÃ¡c quáº­n Cali â€” https://huggingface.co/zhaospei/Model_2\n",
      "\n",
      "Let's start by retrieving details about the first model:\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "ğŸ  MÃ´ hÃ¬nh Dá»± Ä‘oÃ¡n GiÃ¡ NhÃ  California (MLP)\n",
      "ğŸ“ MÃ´ táº£\n",
      "ÄÃ¢y lÃ  má»™t mÃ´ hÃ¬nh multi-layer perceptron (MLP) Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u California Housing Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  trung bÃ¬nh (MedHouseVal). MÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng PyTorch, lÃ  báº£n chuyá»ƒn thá»ƒ tá»« mÃ´ hÃ¬nh TensorFlow trong ChÆ°Æ¡ng 10 cá»§a cuá»‘n Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow cá»§a Aurelien Geron.\n",
      "ğŸ“Œ Nhiá»‡m vá»¥\n",
      "Dá»± Ä‘oÃ¡n giÃ¡ nhÃ  dá»±a trÃªn cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o dáº¡ng báº£ng (tabular regression).\n",
      "ğŸ“¥ Äáº§u vÃ o\n",
      "Sá»‘ chiá»u: [batch_size, 8]\n",
      "Kiá»ƒu dá»¯ liá»‡u: torch.FloatTensor\n",
      "CÃ¡c Ä‘áº·c trÆ°ng:\n",
      "'MedInc' â€“ Thu nháº­p trung vá»‹\n",
      "'HouseAge' â€“ Tuá»•i trung bÃ¬nh cá»§a cÄƒn nhÃ \n",
      "'AveRooms' â€“ Sá»‘ phÃ²ng trung bÃ¬nh\n",
      "'AveBedrms' â€“ Sá»‘ phÃ²ng ngá»§ trung bÃ¬nh\n",
      "'Population' â€“ DÃ¢n sá»‘\n",
      "'AveOccup' â€“ Sá»‘ ngÆ°á»i trung bÃ¬nh trÃªn má»—i há»™\n",
      "'Latitude' â€“ VÄ© Ä‘á»™\n",
      "'Longitude' â€“ Kinh Ä‘á»™\n",
      "ğŸ“¤ Äáº§u ra\n",
      "Kiá»ƒu: torch.FloatTensor cÃ³ shape [batch_size, 1]\n",
      "Ã nghÄ©a: Má»™t giÃ¡ trá»‹ thá»±c duy nháº¥t dá»± Ä‘oÃ¡n giÃ¡ nhÃ  trung bÃ¬nh cho má»—i máº«u.\n",
      "ğŸ›  YÃªu cáº§u thÆ° viá»‡n\n",
      "CÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t báº±ng:\n",
      "pip install torch scikit-learn huggingface_hub\n",
      "ğŸ§ª Sá»­ dá»¥ng mÃ´ hÃ¬nh\n",
      "DÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ vá» cÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from huggingface_hub import PyTorchModelHubMixin\n",
      "# Táº¡o Ä‘áº§u vÃ o máº«u ngáº«u nhiÃªn (chuáº©n hÃ³a giáº£ Ä‘á»‹nh ~ N(0,1))\n",
      "x_input = torch.randn(1, 8)  # 1 máº«u, 8 Ä‘áº·c trÆ°ng\n",
      "print(\"Mock input:\")\n",
      "print(x_input)\n",
      "# Äá»‹nh nghÄ©a láº¡i mÃ´ hÃ¬nh\n",
      "class MLP(nn.Module, PyTorchModelHubMixin):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.fc1 = nn.Linear(8, 50)\n",
      "        self.fc2 = nn.Linear(50, 50)\n",
      "        self.fc3 = nn.Linear(50, 50)\n",
      "        self.fc4 = nn.Linear(50, 1)\n",
      "    def forward(self, x):\n",
      "        act = torch.relu(self.fc1(x))\n",
      "        act = torch.relu(self.fc2(act))\n",
      "        act = torch.relu(self.fc3(act))\n",
      "        return self.fc4(act)\n",
      "# Táº£i mÃ´ hÃ¬nh tá»« Hugging Face Hub\n",
      "model = MLP.from_pretrained(\"zhaospei/Model_3\")\n",
      "model.eval()\n",
      "# Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh\n",
      "with torch.no_grad():\n",
      "    prediction = model(x_input)\n",
      "print(f\"GiÃ¡ nhÃ  dá»± Ä‘oÃ¡n (mock input): {prediction.item():.3f}\")\n",
      "ğŸ”— Tham kháº£o\n",
      "PyTorchModelHubMixin Documentation\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_2'}`\n",
      "responded: Based on the task description and the requirements, I will now proceed to analyze the models provided.\n",
      "\n",
      "I have retrieved the details for the first model. Now, let's retrieve the details for the second model:\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "ğŸ  MÃ´ hÃ¬nh Wide & Deep Neural Network - Dá»± Ä‘oÃ¡n GiÃ¡ NhÃ  California\n",
      "ğŸ“ MÃ´ táº£\n",
      "ÄÃ¢y lÃ  má»™t mÃ´ hÃ¬nh Wide & Deep Neural Network Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u California Housing Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  trung bÃ¬nh (MedHouseVal). MÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng PyTorch, dá»±a trÃªn kiáº¿n trÃºc trong cuá»‘n Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow cá»§a AurÃ©lien GÃ©ron.\n",
      "ğŸ“Œ Nhiá»‡m vá»¥\n",
      "Dá»± Ä‘oÃ¡n giÃ¡ nhÃ  dá»±a trÃªn dá»¯ liá»‡u báº£ng (tabular regression) vá»›i 8 Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o.\n",
      "ğŸ“¥ Äáº§u vÃ o\n",
      "Sá»‘ chiá»u: [batch_size, 8] \n",
      "Kiá»ƒu dá»¯ liá»‡u: torch.FloatTensor \n",
      "CÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o:\n",
      "'MedInc' â€“ Thu nháº­p trung vá»‹\n",
      "'HouseAge' â€“ Tuá»•i trung bÃ¬nh cá»§a cÄƒn nhÃ \n",
      "'AveRooms' â€“ Sá»‘ phÃ²ng trung bÃ¬nh\n",
      "'AveBedrms' â€“ Sá»‘ phÃ²ng ngá»§ trung bÃ¬nh\n",
      "'Population' â€“ DÃ¢n sá»‘\n",
      "'AveOccup' â€“ Sá»‘ ngÆ°á»i trung bÃ¬nh trÃªn má»—i há»™\n",
      "'Latitude' â€“ VÄ© Ä‘á»™\n",
      "'Longitude' â€“ Kinh Ä‘á»™\n",
      "ğŸ“¤ Äáº§u ra\n",
      "Kiá»ƒu: torch.FloatTensor cÃ³ shape [batch_size, 1] \n",
      "Ã nghÄ©a: GiÃ¡ nhÃ  trung bÃ¬nh dá»± Ä‘oÃ¡n (giÃ¡ trá»‹ thá»±c).\n",
      "ğŸ§ª CÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh\n",
      "DÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ vá» cÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh vá»›i dá»¯ liá»‡u Ä‘áº§u vÃ o giáº£ láº­p:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from huggingface_hub import PyTorchModelHubMixin\n",
      "# Táº¡o dá»¯ liá»‡u Ä‘áº§u vÃ o giáº£ láº­p (batch 1, 8 features)\n",
      "x_input = torch.randn(1, 8)\n",
      "print(\"Mock input:\")\n",
      "print(x_input)\n",
      "# Äá»‹nh nghÄ©a mÃ´ hÃ¬nh Wide & Deep Neural Network\n",
      "class WideAndDeepNet(nn.Module, PyTorchModelHubMixin):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.hidden1 = nn.Linear(6, 30)\n",
      "        self.hidden2 = nn.Linear(30, 30)\n",
      "        self.main_head = nn.Linear(35, 1)\n",
      "        self.aux_head = nn.Linear(30, 1)\n",
      "        self.main_loss_fn = nn.MSELoss(reduction='sum')\n",
      "        self.aux_loss_fn = nn.MSELoss(reduction='sum')\n",
      "    def forward(self, input_wide, input_deep, label=None):\n",
      "        act = torch.relu(self.hidden1(input_deep))\n",
      "        act = torch.relu(self.hidden2(act))\n",
      "        concat = torch.cat([input_wide, act], dim=1)\n",
      "        main_output = self.main_head(concat)\n",
      "        aux_output = self.aux_head(act)\n",
      "        if label is not None:\n",
      "            main_loss = self.main_loss_fn(main_output.squeeze(), label)\n",
      "            aux_loss = self.aux_loss_fn(aux_output.squeeze(), label)\n",
      "        return WideAndDeepNetOutput(main_output=main_output, aux_output=aux_output)\n",
      "# Táº£i mÃ´ hÃ¬nh tá»« Hugging Face Hub\n",
      "model = WideAndDeepNet.from_pretrained(\"sadhaklal/wide-and-deep-net-california-housing-v3\")\n",
      "model.eval()\n",
      "# Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh\n",
      "with torch.no_grad():\n",
      "    prediction = model(x_input)\n",
      "print(f\"GiÃ¡ nhÃ  dá»± Ä‘oÃ¡n (mock input): {prediction.item():.3f}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_3'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "ğŸ  MÃ´ hÃ¬nh Dá»± Ä‘oÃ¡n GiÃ¡ NhÃ  California (MLP)\n",
      "ğŸ“ MÃ´ táº£\n",
      "ÄÃ¢y lÃ  má»™t mÃ´ hÃ¬nh multi-layer perceptron (MLP) Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u California Housing Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  trung bÃ¬nh (MedHouseVal). MÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng PyTorch, lÃ  báº£n chuyá»ƒn thá»ƒ tá»« mÃ´ hÃ¬nh TensorFlow trong ChÆ°Æ¡ng 10 cá»§a cuá»‘n Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow cá»§a Aurelien Geron.\n",
      "ğŸ“Œ Nhiá»‡m vá»¥\n",
      "Dá»± Ä‘oÃ¡n giÃ¡ nhÃ  dá»±a trÃªn cÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o dáº¡ng báº£ng (tabular regression).\n",
      "ğŸ“¥ Äáº§u vÃ o\n",
      "Sá»‘ chiá»u: [batch_size, 8]\n",
      "Kiá»ƒu dá»¯ liá»‡u: torch.FloatTensor\n",
      "CÃ¡c Ä‘áº·c trÆ°ng:\n",
      "'MedInc' â€“ Thu nháº­p trung vá»‹\n",
      "'HouseAge' â€“ Tuá»•i trung bÃ¬nh cá»§a cÄƒn nhÃ \n",
      "'AveRooms' â€“ Sá»‘ phÃ²ng trung bÃ¬nh\n",
      "'AveBedrms' â€“ Sá»‘ phÃ²ng ngá»§ trung bÃ¬nh\n",
      "'Population' â€“ DÃ¢n sá»‘\n",
      "'AveOccup' â€“ Sá»‘ ngÆ°á»i trung bÃ¬nh trÃªn má»—i há»™\n",
      "'Latitude' â€“ VÄ© Ä‘á»™\n",
      "'Longitude' â€“ Kinh Ä‘á»™\n",
      "ğŸ“¤ Äáº§u ra\n",
      "Kiá»ƒu: torch.FloatTensor cÃ³ shape [batch_size, 1]\n",
      "Ã nghÄ©a: Má»™t giÃ¡ trá»‹ thá»±c duy nháº¥t dá»± Ä‘oÃ¡n giÃ¡ nhÃ  trung bÃ¬nh cho má»—i máº«u.\n",
      "ğŸ›  YÃªu cáº§u thÆ° viá»‡n\n",
      "CÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t báº±ng:\n",
      "pip install torch scikit-learn huggingface_hub\n",
      "ğŸ§ª Sá»­ dá»¥ng mÃ´ hÃ¬nh\n",
      "DÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ vá» cÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from huggingface_hub import PyTorchModelHubMixin\n",
      "# Táº¡o Ä‘áº§u vÃ o máº«u ngáº«u nhiÃªn (chuáº©n hÃ³a giáº£ Ä‘á»‹nh ~ N(0,1))\n",
      "x_input = torch.randn(1, 8)  # 1 máº«u, 8 Ä‘áº·c trÆ°ng\n",
      "print(\"Mock input:\")\n",
      "print(x_input)\n",
      "# Äá»‹nh nghÄ©a láº¡i mÃ´ hÃ¬nh\n",
      "class MLP(nn.Module, PyTorchModelHubMixin):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.fc1 = nn.Linear(8, 50)\n",
      "        self.fc2 = nn.Linear(50, 50)\n",
      "        self.fc3 = nn.Linear(50, 50)\n",
      "        self.fc4 = nn.Linear(50, 1)\n",
      "    def forward(self, x):\n",
      "        act = torch.relu(self.fc1(x))\n",
      "        act = torch.relu(self.fc2(act))\n",
      "        act = torch.relu(self.fc3(act))\n",
      "        return self.fc4(act)\n",
      "# Táº£i mÃ´ hÃ¬nh tá»« Hugging Face Hub\n",
      "model = MLP.from_pretrained(\"zhaospei/Model_3\")\n",
      "model.eval()\n",
      "# Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh\n",
      "with torch.no_grad():\n",
      "    prediction = model(x_input)\n",
      "print(f\"GiÃ¡ nhÃ  dá»± Ä‘oÃ¡n (mock input): {prediction.item():.3f}\")\n",
      "ğŸ”— Tham kháº£o\n",
      "PyTorchModelHubMixin Documentation\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_2'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "ğŸ  MÃ´ hÃ¬nh Wide & Deep Neural Network - Dá»± Ä‘oÃ¡n GiÃ¡ NhÃ  California\n",
      "ğŸ“ MÃ´ táº£\n",
      "ÄÃ¢y lÃ  má»™t mÃ´ hÃ¬nh Wide & Deep Neural Network Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u California Housing Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  trung bÃ¬nh (MedHouseVal). MÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng PyTorch, dá»±a trÃªn kiáº¿n trÃºc trong cuá»‘n Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow cá»§a AurÃ©lien GÃ©ron.\n",
      "ğŸ“Œ Nhiá»‡m vá»¥\n",
      "Dá»± Ä‘oÃ¡n giÃ¡ nhÃ  dá»±a trÃªn dá»¯ liá»‡u báº£ng (tabular regression) vá»›i 8 Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o.\n",
      "ğŸ“¥ Äáº§u vÃ o\n",
      "Sá»‘ chiá»u: [batch_size, 8] \n",
      "Kiá»ƒu dá»¯ liá»‡u: torch.FloatTensor \n",
      "CÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o:\n",
      "'MedInc' â€“ Thu nháº­p trung vá»‹\n",
      "'HouseAge' â€“ Tuá»•i trung bÃ¬nh cá»§a cÄƒn nhÃ \n",
      "'AveRooms' â€“ Sá»‘ phÃ²ng trung bÃ¬nh\n",
      "'AveBedrms' â€“ Sá»‘ phÃ²ng ngá»§ trung bÃ¬nh\n",
      "'Population' â€“ DÃ¢n sá»‘\n",
      "'AveOccup' â€“ Sá»‘ ngÆ°á»i trung bÃ¬nh trÃªn má»—i há»™\n",
      "'Latitude' â€“ VÄ© Ä‘á»™\n",
      "'Longitude' â€“ Kinh Ä‘á»™\n",
      "ğŸ“¤ Äáº§u ra\n",
      "Kiá»ƒu: torch.FloatTensor cÃ³ shape [batch_size, 1] \n",
      "Ã nghÄ©a: GiÃ¡ nhÃ  trung bÃ¬nh dá»± Ä‘oÃ¡n (giÃ¡ trá»‹ thá»±c).\n",
      "ğŸ§ª CÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh\n",
      "DÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ vá» cÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh vá»›i dá»¯ liá»‡u Ä‘áº§u vÃ o giáº£ láº­p:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from huggingface_hub import PyTorchModelHubMixin\n",
      "# Táº¡o dá»¯ liá»‡u Ä‘áº§u vÃ o giáº£ láº­p (batch 1, 8 features)\n",
      "x_input = torch.randn(1, 8)\n",
      "print(\"Mock input:\")\n",
      "print(x_input)\n",
      "# Äá»‹nh nghÄ©a mÃ´ hÃ¬nh Wide & Deep Neural Network\n",
      "class WideAndDeepNet(nn.Module, PyTorchModelHubMixin):\n",
      "    def __init__(self):\n",
      "        super().__init__()\n",
      "        self.hidden1 = nn.Linear(6, 30)\n",
      "        self.hidden2 = nn.Linear(30, 30)\n",
      "        self.main_head = nn.Linear(35, 1)\n",
      "        self.aux_head = nn.Linear(30, 1)\n",
      "        self.main_loss_fn = nn.MSELoss(reduction='sum')\n",
      "        self.aux_loss_fn = nn.MSELoss(reduction='sum')\n",
      "    def forward(self, input_wide, input_deep, label=None):\n",
      "        act = torch.relu(self.hidden1(input_deep))\n",
      "        act = torch.relu(self.hidden2(act))\n",
      "        concat = torch.cat([input_wide, act], dim=1)\n",
      "        main_output = self.main_head(concat)\n",
      "        aux_output = self.aux_head(act)\n",
      "        if label is not None:\n",
      "            main_loss = self.main_loss_fn(main_output.squeeze(), label)\n",
      "            aux_loss = self.aux_loss_fn(aux_output.squeeze(), label)\n",
      "        return WideAndDeepNetOutput(main_output=main_output, aux_output=aux_output)\n",
      "# Táº£i mÃ´ hÃ¬nh tá»« Hugging Face Hub\n",
      "model = WideAndDeepNet.from_pretrained(\"sadhaklal/wide-and-deep-net-california-housing-v3\")\n",
      "model.eval()\n",
      "# Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh\n",
      "with torch.no_grad():\n",
      "    prediction = model(x_input)\n",
      "print(f\"GiÃ¡ nhÃ  dá»± Ä‘oÃ¡n (mock input): {prediction.item():.3f}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mBased on the task description and the requirements, I will now proceed to analyze the models provided.\n",
      "\n",
      "I have retrieved the details for both models. Now, let's compare and select the best model for the task.\n",
      "\n",
      "**Model 1:**\n",
      "`Dá»± Ä‘oÃ¡n giÃ¡ trung bÃ¬nh nhÃ  á»Ÿ cÃ¡c quáº­n Cali` â€” `https://huggingface.co/zhaospei/Model_3`\n",
      "\n",
      "**Description (from tool):**\n",
      "ÄÃ¢y lÃ  má»™t mÃ´ hÃ¬nh multi-layer perceptron (MLP) Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u California Housing Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  trung bÃ¬nh (MedHouseVal). MÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng PyTorch, lÃ  báº£n chuyá»ƒn thá»ƒ tá»« mÃ´ hÃ¬nh TensorFlow trong ChÆ°Æ¡ng10 cá»§a cuá»‘n Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow cá»§a Aurelien Geron.\n",
      "\n",
      "**Input format (from tool):**\n",
      "Sá»‘ chiá»u: [batch_size,8]\n",
      "Kiá»ƒu dá»¯ liá»‡u: torch.FloatTensor\n",
      "CÃ¡c Ä‘áº·c trÆ°ng:\n",
      "'MedInc' â€“ Thu nháº­p trung vá»‹\n",
      "'HouseAge' â€“ Tuá»•i trung bÃ¬nh cá»§a cÄƒn nhÃ \n",
      "'AveRooms' â€“ Sá»‘ phÃ²ng trung bÃ¬nh\n",
      "'AveBedrms' â€“ Sá»‘ phÃ²ng ngá»§ trung bÃ¬nh\n",
      "'Population' â€“ DÃ¢n sá»‘\n",
      "'AveOccup' â€“ Sá»‘ ngÆ°á»i trung bÃ¬nh trÃªn má»—i há»™\n",
      "'Latitude' â€“ VÄ© Ä‘á»™\n",
      "'Longitude' â€“ Kinh Ä‘á»™\n",
      "\n",
      "**Output format (from tool):**\n",
      "Kiá»ƒu: torch.FloatTensor cÃ³ shape [batch_size,1]\n",
      "Ã nghÄ©a: Má»™t giÃ¡ trá»‹ thá»±c duy nháº¥t dá»± Ä‘oÃ¡n giÃ¡ nhÃ  trung bÃ¬nh cho má»—i máº«u.\n",
      "\n",
      "**Library Requirements (from tool)**\n",
      "CÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t báº±ng:\n",
      "pip install torch scikit-learn huggingface_hub\n",
      "\n",
      "**Example code (from tool):**\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from huggingface_hub import PyTorchModelHubMixin\n",
      "\n",
      "# Táº¡o Ä‘áº§u vÃ o máº«u ngáº«u nhiÃªn (chuáº©n hÃ³a giáº£ Ä‘á»‹nh ~ N(0,1))\n",
      "x_input = torch.randn(1,8) #1 máº«u,8 Ä‘áº·c trÆ°ng\n",
      "print(\"Mock input:\")\n",
      "print(x_input)\n",
      "\n",
      "# Äá»‹nh nghÄ©a láº¡i mÃ´ hÃ¬nh\n",
      "class MLP(nn.Module, PyTorchModelHubMixin):\n",
      " def __init__(self):\n",
      " super().__init__()\n",
      " self.fc1 = nn.Linear(8,50)\n",
      " self.fc2 = nn.Linear(50,50)\n",
      " self.fc3 = nn.Linear(50,50)\n",
      " self.fc4 = nn.Linear(50,1)\n",
      " def forward(self, x):\n",
      " act = torch.relu(self.fc1(x))\n",
      " act = torch.relu(self.fc2(act))\n",
      " act = torch.relu(self.fc3(act))\n",
      " return self.fc4(act)\n",
      "\n",
      "# Táº£i mÃ´ hÃ¬nh tá»« Hugging Face Hub\n",
      "model = MLP.from_pretrained(\"zhaospei/Model_3\")\n",
      "model.eval()\n",
      "\n",
      "# Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh\n",
      "with torch.no_grad():\n",
      " prediction = model(x_input)\n",
      "print(f\"GiÃ¡ nhÃ  dá»± Ä‘oÃ¡n (mock input): {prediction.item():.3f}\")\n",
      "```\n",
      "\n",
      "**Model 2:**\n",
      "`Dá»± Ä‘oÃ¡n giÃ¡ trung bÃ¬nh nhÃ  á»Ÿ cÃ¡c quáº­n Cali` â€” `https://huggingface.co/zhaospei/Model_2`\n",
      "\n",
      "**Description (from tool):**\n",
      "ÄÃ¢y lÃ  má»™t mÃ´ hÃ¬nh Wide & Deep Neural Network Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u California Housing Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  trung bÃ¬nh (MedHouseVal). MÃ´ hÃ¬nh Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng PyTorch, dá»±a trÃªn kiáº¿n trÃºc trong cuá»‘n Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow cá»§a AurÃ©lien GÃ©ron.\n",
      "\n",
      "**Input format (from tool):**\n",
      "Sá»‘ chiá»u: [batch_size,8]\n",
      "Kiá»ƒu dá»¯ liá»‡u: torch.FloatTensor\n",
      "CÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o:\n",
      "'MedInc' â€“ Thu nháº­p trung vá»‹\n",
      "'HouseAge' â€“ Tuá»•i trung bÃ¬nh cá»§a cÄƒn nhÃ \n",
      "'AveRooms' â€“ Sá»‘ phÃ²ng trung bÃ¬nh\n",
      "'AveBedrms' â€“ Sá»‘ phÃ²ng ngá»§ trung bÃ¬nh\n",
      "'Population' â€“ DÃ¢n sá»‘\n",
      "'AveOccup' â€“ Sá»‘ ngÆ°á»i trung bÃ¬nh trÃªn má»—i há»™\n",
      "'Latitude' â€“ VÄ© Ä‘á»™\n",
      "'Longitude' â€“ Kinh Ä‘á»™\n",
      "\n",
      "**Output format (from tool):**\n",
      "Kiá»ƒu: torch.FloatTensor cÃ³ shape [batch_size,1]\n",
      "Ã nghÄ©a: GiÃ¡ nhÃ  trung bÃ¬nh dá»± Ä‘oÃ¡n (giÃ¡ trá»‹ thá»±c).\n",
      "\n",
      "**Library Requirements (from tool)**\n",
      "CÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t báº±ng:\n",
      "pip install torch scikit-learn huggingface_hub\n",
      "\n",
      "**Example code (from tool):**\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "from huggingface_hub import PyTorchModelHubMixin\n",
      "\n",
      "# Táº¡o dá»¯ liá»‡u Ä‘áº§u vÃ o giáº£ láº­p (batch1,8 features)\n",
      "x_input = torch.randn(1,8)\n",
      "print(\"Mock input:\")\n",
      "print(x_input)\n",
      "\n",
      "# Äá»‹nh nghÄ©a mÃ´ hÃ¬nh Wide & Deep Neural Network\n",
      "class WideAndDeepNet(nn.Module, PyTorchModelHubMixin):\n",
      " def __init__(self):\n",
      " super().__init__()\n",
      " self.hidden1 = nn.Linear(6,30)\n",
      " self.hidden2 = nn.Linear(30,30)\n",
      " self.main_head = nn.Linear(35,1)\n",
      " self.aux_head = nn.Linear(30,1)\n",
      " self.main_loss_fn = nn.MSELoss(reduction='sum')\n",
      " self.aux_loss_fn = nn.MSELoss(reduction='sum')\n",
      " def forward(self, input_wide, input_deep, label=None):\n",
      " act = torch.relu(self.hidden1(input_deep))\n",
      " act = torch.relu(self.hidden2(act))\n",
      " concat = torch.cat([input_wide, act], dim=1)\n",
      " main_output = self.main_head(concat)\n",
      " aux_output = self.aux_head(act)\n",
      " if label is not None:\n",
      " main_loss = self.main_loss_fn(main_output.squeeze(), label)\n",
      " aux_loss = self.aux_loss_fn(aux_output.squeeze(), label)\n",
      " return WideAndDeepNetOutput(main_output=main_output, aux_output=aux_output)\n",
      "\n",
      "# Táº£i mÃ´ hÃ¬nh tá»« Hugging Face Hub\n",
      "model = WideAndDeepNet.from_pretrained(\"sadhaklal/wide-and-deep-net-california-housing-v3\")\n",
      "model.eval()\n",
      "\n",
      "# Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh\n",
      "with torch.no_grad():\n",
      " prediction = model(x_input)\n",
      "print(f\"GiÃ¡ nhÃ  dá»± Ä‘oÃ¡n (mock input): {prediction.item():.3f}\")\n",
      "```\n",
      "\n",
      "Both models are suitable for the task, but they need to be adjusted to classify the house prices into three categories: \"low\", \"medium\", and \"high\".\n",
      "\n",
      "I will select **Model 1:** `Dá»± Ä‘oÃ¡n giÃ¡ trung bÃ¬nh nhÃ  á»Ÿ cÃ¡c quáº­n Cali` â€” `https://huggingface.co/zhaospei/Model_3` as the best model for this task.\n",
      "\n",
      "The model can be used as follows:\n",
      "\n",
      "1. Load the model and the necessary libraries.\n",
      "2. Define a function to classify the predicted house prices into three categories: \"low\", \"medium\", and \"high\".\n",
      "3. Use the model to predict the house prices for the test data.\n",
      "4. Classify the predicted prices and save the results to a CSV file.\n",
      "\n",
      "Here is an example code snippet:\n",
      "```python\n",
      "import torch\n",
      "import pandas as pd\n",
      "from huggingface_hub import PyTorchModelHubMixin\n",
      "\n",
      "# Load the model\n",
      "model = MLP.from_pretrained(\"zhaospei/Model_3\")\n",
      "model.eval()\n",
      "\n",
      "# Define a function to classify the predicted house prices\n",
      "def classify_price(price):\n",
      " if price < 1:\n",
      " return \"low\"\n",
      " elif 1 <= price < 2.5:\n",
      " return \"medium\"\n",
      " else:\n",
      " return \"high\"\n",
      "\n",
      "# Load the test data\n",
      "test_data = pd.read_csv(\"test.csv\")\n",
      "\n",
      "# Predict the house prices\n",
      "with torch.no_grad():\n",
      " predictions = model(torch.tensor(test_data.drop(\"ID\", axis=1).values).float())\n",
      "\n",
      "# Classify the predicted prices\n",
      "classified_prices = [classify_price(prediction.item()) for prediction in predictions]\n",
      "\n",
      "# Save the results to a CSV file\n",
      "output_data = pd.DataFrame({\"ID\": test_data[\"ID\"], \"MedHouseVal\": classified_prices})\n",
      "output_data.to_csv(\"output.csv\", index=False)\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "state = model_selection_using_tool_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "3bdc5ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Based on the task description and the requirements, I will now proceed to '\n",
      " 'analyze the models provided.\\n'\n",
      " '\\n'\n",
      " \"I have retrieved the details for both models. Now, let's compare and select \"\n",
      " 'the best model for the task.\\n'\n",
      " '\\n'\n",
      " '**Model 1:**\\n'\n",
      " '`Dá»± Ä‘oÃ¡n giÃ¡ trung bÃ¬nh nhÃ  á»Ÿ cÃ¡c quáº­n Cali` â€” '\n",
      " '`https://huggingface.co/zhaospei/Model_3`\\n'\n",
      " '\\n'\n",
      " '**Description (from tool):**\\n'\n",
      " 'ÄÃ¢y lÃ  má»™t mÃ´ hÃ¬nh multi-layer perceptron (MLP) Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ '\n",
      " 'liá»‡u California Housing Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  trung bÃ¬nh (MedHouseVal). MÃ´ hÃ¬nh '\n",
      " 'Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng PyTorch, lÃ  báº£n chuyá»ƒn thá»ƒ tá»« mÃ´ hÃ¬nh TensorFlow trong '\n",
      " 'ChÆ°Æ¡ng10 cá»§a cuá»‘n Hands-On Machine Learning with Scikit-Learn, Keras, and '\n",
      " 'TensorFlow cá»§a Aurelien Geron.\\n'\n",
      " '\\n'\n",
      " '**Input format (from tool):**\\n'\n",
      " 'Sá»‘ chiá»u: [batch_size,8]\\n'\n",
      " 'Kiá»ƒu dá»¯ liá»‡u: torch.FloatTensor\\n'\n",
      " 'CÃ¡c Ä‘áº·c trÆ°ng:\\n'\n",
      " \"'MedInc' â€“ Thu nháº­p trung vá»‹\\n\"\n",
      " \"'HouseAge' â€“ Tuá»•i trung bÃ¬nh cá»§a cÄƒn nhÃ \\n\"\n",
      " \"'AveRooms' â€“ Sá»‘ phÃ²ng trung bÃ¬nh\\n\"\n",
      " \"'AveBedrms' â€“ Sá»‘ phÃ²ng ngá»§ trung bÃ¬nh\\n\"\n",
      " \"'Population' â€“ DÃ¢n sá»‘\\n\"\n",
      " \"'AveOccup' â€“ Sá»‘ ngÆ°á»i trung bÃ¬nh trÃªn má»—i há»™\\n\"\n",
      " \"'Latitude' â€“ VÄ© Ä‘á»™\\n\"\n",
      " \"'Longitude' â€“ Kinh Ä‘á»™\\n\"\n",
      " '\\n'\n",
      " '**Output format (from tool):**\\n'\n",
      " 'Kiá»ƒu: torch.FloatTensor cÃ³ shape [batch_size,1]\\n'\n",
      " 'Ã nghÄ©a: Má»™t giÃ¡ trá»‹ thá»±c duy nháº¥t dá»± Ä‘oÃ¡n giÃ¡ nhÃ  trung bÃ¬nh cho má»—i máº«u.\\n'\n",
      " '\\n'\n",
      " '**Library Requirements (from tool)**\\n'\n",
      " 'CÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t báº±ng:\\n'\n",
      " 'pip install torch scikit-learn huggingface_hub\\n'\n",
      " '\\n'\n",
      " '**Example code (from tool):**\\n'\n",
      " '```python\\n'\n",
      " 'import torch\\n'\n",
      " 'import torch.nn as nn\\n'\n",
      " 'from huggingface_hub import PyTorchModelHubMixin\\n'\n",
      " '\\n'\n",
      " '# Táº¡o Ä‘áº§u vÃ o máº«u ngáº«u nhiÃªn (chuáº©n hÃ³a giáº£ Ä‘á»‹nh ~ N(0,1))\\n'\n",
      " 'x_input = torch.randn(1,8) #1 máº«u,8 Ä‘áº·c trÆ°ng\\n'\n",
      " 'print(\"Mock input:\")\\n'\n",
      " 'print(x_input)\\n'\n",
      " '\\n'\n",
      " '# Äá»‹nh nghÄ©a láº¡i mÃ´ hÃ¬nh\\n'\n",
      " 'class MLP(nn.Module, PyTorchModelHubMixin):\\n'\n",
      " ' def __init__(self):\\n'\n",
      " ' super().__init__()\\n'\n",
      " ' self.fc1 = nn.Linear(8,50)\\n'\n",
      " ' self.fc2 = nn.Linear(50,50)\\n'\n",
      " ' self.fc3 = nn.Linear(50,50)\\n'\n",
      " ' self.fc4 = nn.Linear(50,1)\\n'\n",
      " ' def forward(self, x):\\n'\n",
      " ' act = torch.relu(self.fc1(x))\\n'\n",
      " ' act = torch.relu(self.fc2(act))\\n'\n",
      " ' act = torch.relu(self.fc3(act))\\n'\n",
      " ' return self.fc4(act)\\n'\n",
      " '\\n'\n",
      " '# Táº£i mÃ´ hÃ¬nh tá»« Hugging Face Hub\\n'\n",
      " 'model = MLP.from_pretrained(\"zhaospei/Model_3\")\\n'\n",
      " 'model.eval()\\n'\n",
      " '\\n'\n",
      " '# Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " ' prediction = model(x_input)\\n'\n",
      " 'print(f\"GiÃ¡ nhÃ  dá»± Ä‘oÃ¡n (mock input): {prediction.item():.3f}\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '**Model 2:**\\n'\n",
      " '`Dá»± Ä‘oÃ¡n giÃ¡ trung bÃ¬nh nhÃ  á»Ÿ cÃ¡c quáº­n Cali` â€” '\n",
      " '`https://huggingface.co/zhaospei/Model_2`\\n'\n",
      " '\\n'\n",
      " '**Description (from tool):**\\n'\n",
      " 'ÄÃ¢y lÃ  má»™t mÃ´ hÃ¬nh Wide & Deep Neural Network Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ '\n",
      " 'liá»‡u California Housing Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  trung bÃ¬nh (MedHouseVal). MÃ´ hÃ¬nh '\n",
      " 'Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng PyTorch, dá»±a trÃªn kiáº¿n trÃºc trong cuá»‘n Hands-On Machine '\n",
      " 'Learning with Scikit-Learn, Keras, and TensorFlow cá»§a AurÃ©lien GÃ©ron.\\n'\n",
      " '\\n'\n",
      " '**Input format (from tool):**\\n'\n",
      " 'Sá»‘ chiá»u: [batch_size,8]\\n'\n",
      " 'Kiá»ƒu dá»¯ liá»‡u: torch.FloatTensor\\n'\n",
      " 'CÃ¡c Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o:\\n'\n",
      " \"'MedInc' â€“ Thu nháº­p trung vá»‹\\n\"\n",
      " \"'HouseAge' â€“ Tuá»•i trung bÃ¬nh cá»§a cÄƒn nhÃ \\n\"\n",
      " \"'AveRooms' â€“ Sá»‘ phÃ²ng trung bÃ¬nh\\n\"\n",
      " \"'AveBedrms' â€“ Sá»‘ phÃ²ng ngá»§ trung bÃ¬nh\\n\"\n",
      " \"'Population' â€“ DÃ¢n sá»‘\\n\"\n",
      " \"'AveOccup' â€“ Sá»‘ ngÆ°á»i trung bÃ¬nh trÃªn má»—i há»™\\n\"\n",
      " \"'Latitude' â€“ VÄ© Ä‘á»™\\n\"\n",
      " \"'Longitude' â€“ Kinh Ä‘á»™\\n\"\n",
      " '\\n'\n",
      " '**Output format (from tool):**\\n'\n",
      " 'Kiá»ƒu: torch.FloatTensor cÃ³ shape [batch_size,1]\\n'\n",
      " 'Ã nghÄ©a: GiÃ¡ nhÃ  trung bÃ¬nh dá»± Ä‘oÃ¡n (giÃ¡ trá»‹ thá»±c).\\n'\n",
      " '\\n'\n",
      " '**Library Requirements (from tool)**\\n'\n",
      " 'CÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t báº±ng:\\n'\n",
      " 'pip install torch scikit-learn huggingface_hub\\n'\n",
      " '\\n'\n",
      " '**Example code (from tool):**\\n'\n",
      " '```python\\n'\n",
      " 'import torch\\n'\n",
      " 'import torch.nn as nn\\n'\n",
      " 'from huggingface_hub import PyTorchModelHubMixin\\n'\n",
      " '\\n'\n",
      " '# Táº¡o dá»¯ liá»‡u Ä‘áº§u vÃ o giáº£ láº­p (batch1,8 features)\\n'\n",
      " 'x_input = torch.randn(1,8)\\n'\n",
      " 'print(\"Mock input:\")\\n'\n",
      " 'print(x_input)\\n'\n",
      " '\\n'\n",
      " '# Äá»‹nh nghÄ©a mÃ´ hÃ¬nh Wide & Deep Neural Network\\n'\n",
      " 'class WideAndDeepNet(nn.Module, PyTorchModelHubMixin):\\n'\n",
      " ' def __init__(self):\\n'\n",
      " ' super().__init__()\\n'\n",
      " ' self.hidden1 = nn.Linear(6,30)\\n'\n",
      " ' self.hidden2 = nn.Linear(30,30)\\n'\n",
      " ' self.main_head = nn.Linear(35,1)\\n'\n",
      " ' self.aux_head = nn.Linear(30,1)\\n'\n",
      " \" self.main_loss_fn = nn.MSELoss(reduction='sum')\\n\"\n",
      " \" self.aux_loss_fn = nn.MSELoss(reduction='sum')\\n\"\n",
      " ' def forward(self, input_wide, input_deep, label=None):\\n'\n",
      " ' act = torch.relu(self.hidden1(input_deep))\\n'\n",
      " ' act = torch.relu(self.hidden2(act))\\n'\n",
      " ' concat = torch.cat([input_wide, act], dim=1)\\n'\n",
      " ' main_output = self.main_head(concat)\\n'\n",
      " ' aux_output = self.aux_head(act)\\n'\n",
      " ' if label is not None:\\n'\n",
      " ' main_loss = self.main_loss_fn(main_output.squeeze(), label)\\n'\n",
      " ' aux_loss = self.aux_loss_fn(aux_output.squeeze(), label)\\n'\n",
      " ' return WideAndDeepNetOutput(main_output=main_output, '\n",
      " 'aux_output=aux_output)\\n'\n",
      " '\\n'\n",
      " '# Táº£i mÃ´ hÃ¬nh tá»« Hugging Face Hub\\n'\n",
      " 'model = '\n",
      " 'WideAndDeepNet.from_pretrained(\"sadhaklal/wide-and-deep-net-california-housing-v3\")\\n'\n",
      " 'model.eval()\\n'\n",
      " '\\n'\n",
      " '# Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " ' prediction = model(x_input)\\n'\n",
      " 'print(f\"GiÃ¡ nhÃ  dá»± Ä‘oÃ¡n (mock input): {prediction.item():.3f}\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'Both models are suitable for the task, but they need to be adjusted to '\n",
      " 'classify the house prices into three categories: \"low\", \"medium\", and '\n",
      " '\"high\".\\n'\n",
      " '\\n'\n",
      " 'I will select **Model 1:** `Dá»± Ä‘oÃ¡n giÃ¡ trung bÃ¬nh nhÃ  á»Ÿ cÃ¡c quáº­n Cali` â€” '\n",
      " '`https://huggingface.co/zhaospei/Model_3` as the best model for this task.\\n'\n",
      " '\\n'\n",
      " 'The model can be used as follows:\\n'\n",
      " '\\n'\n",
      " '1. Load the model and the necessary libraries.\\n'\n",
      " '2. Define a function to classify the predicted house prices into three '\n",
      " 'categories: \"low\", \"medium\", and \"high\".\\n'\n",
      " '3. Use the model to predict the house prices for the test data.\\n'\n",
      " '4. Classify the predicted prices and save the results to a CSV file.\\n'\n",
      " '\\n'\n",
      " 'Here is an example code snippet:\\n'\n",
      " '```python\\n'\n",
      " 'import torch\\n'\n",
      " 'import pandas as pd\\n'\n",
      " 'from huggingface_hub import PyTorchModelHubMixin\\n'\n",
      " '\\n'\n",
      " '# Load the model\\n'\n",
      " 'model = MLP.from_pretrained(\"zhaospei/Model_3\")\\n'\n",
      " 'model.eval()\\n'\n",
      " '\\n'\n",
      " '# Define a function to classify the predicted house prices\\n'\n",
      " 'def classify_price(price):\\n'\n",
      " ' if price < 1:\\n'\n",
      " ' return \"low\"\\n'\n",
      " ' elif 1 <= price < 2.5:\\n'\n",
      " ' return \"medium\"\\n'\n",
      " ' else:\\n'\n",
      " ' return \"high\"\\n'\n",
      " '\\n'\n",
      " '# Load the test data\\n'\n",
      " 'test_data = pd.read_csv(\"test.csv\")\\n'\n",
      " '\\n'\n",
      " '# Predict the house prices\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " ' predictions = model(torch.tensor(test_data.drop(\"ID\", '\n",
      " 'axis=1).values).float())\\n'\n",
      " '\\n'\n",
      " '# Classify the predicted prices\\n'\n",
      " 'classified_prices = [classify_price(prediction.item()) for prediction in '\n",
      " 'predictions]\\n'\n",
      " '\\n'\n",
      " '# Save the results to a CSV file\\n'\n",
      " 'output_data = pd.DataFrame({\"ID\": test_data[\"ID\"], \"MedHouseVal\": '\n",
      " 'classified_prices})\\n'\n",
      " 'output_data.to_csv(\"output.csv\", index=False)\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(state['model_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a26f2",
   "metadata": {},
   "source": [
    "## Output Parser Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "dbc20e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=ModelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "3e00d79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser_llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "418663c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_parser_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a smart AI tasked with extracting structured technical details about a machine learning model from a reasoning result.\n",
    "                    You are given a detailed text description about a model (or a list of model).\n",
    "                    Your goal is to fill the following fields using the data about chosen models based on the text:\n",
    "\n",
    "                    - `model_description`: A detailed explanation of what the model is and what it does.\n",
    "                    - `model_input_format`: A detailed description of the model's input format, including dimensions, data types, and expected preprocessing if mentioned.\n",
    "                    - `model_output_format`: A detailed description of the output format including dimensions, data types, label name and its meaning.\n",
    "                    - `model_requirements`: A detailed description about the requirements needed to be sastified in order to use the model\n",
    "                    - `model_sample_code`: A sample code on how to use the model.\n",
    "\n",
    "                    You MUSTN'T create any data on your own, only using the data provided in the text.\n",
    "\n",
    "                    Return the data as a JSON object matching the following structure:\n",
    "                    {formatted_output}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        ('system', system_prompt),\n",
    "        ('human', \"{input}\")\n",
    "    ]).partial(formatted_output=parser.get_format_instructions())\n",
    "    chain = prompt | llm | parser\n",
    "    # response = chain.invoke({\"input\": res2['output']})\n",
    "    # return response\n",
    "    response = chain.invoke({\"input\": state['model_description']})\n",
    "    state['model_detailed_list'] = response\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be5f89f",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "694c9eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = output_parser_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "2a3370dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0:\n",
      "('ÄÃ¢y lÃ  má»™t mÃ´ hÃ¬nh multi-layer perceptron (MLP) Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn táº­p dá»¯ '\n",
      " 'liá»‡u California Housing Ä‘á»ƒ dá»± Ä‘oÃ¡n giÃ¡ nhÃ  trung bÃ¬nh (MedHouseVal). MÃ´ hÃ¬nh '\n",
      " 'Ä‘Æ°á»£c xÃ¢y dá»±ng báº±ng PyTorch, lÃ  báº£n chuyá»ƒn thá»ƒ tá»« mÃ´ hÃ¬nh TensorFlow trong '\n",
      " 'ChÆ°Æ¡ng10 cá»§a cuá»‘n Hands-On Machine Learning with Scikit-Learn, Keras, and '\n",
      " 'TensorFlow cá»§a Aurelien Geron.')\n",
      "('Sá»‘ chiá»u: [batch_size,8]\\n'\n",
      " 'Kiá»ƒu dá»¯ liá»‡u: torch.FloatTensor\\n'\n",
      " 'CÃ¡c Ä‘áº·c trÆ°ng:\\n'\n",
      " \"'MedInc' â€“ Thu nháº­p trung vá»‹\\n\"\n",
      " \"'HouseAge' â€“ Tuá»•i trung bÃ¬nh cá»§a cÄƒn nhÃ \\n\"\n",
      " \"'AveRooms' â€“ Sá»‘ phÃ²ng trung bÃ¬nh\\n\"\n",
      " \"'AveBedrms' â€“ Sá»‘ phÃ²ng ngá»§ trung bÃ¬nh\\n\"\n",
      " \"'Population' â€“ DÃ¢n sá»‘\\n\"\n",
      " \"'AveOccup' â€“ Sá»‘ ngÆ°á»i trung bÃ¬nh trÃªn má»—i há»™\\n\"\n",
      " \"'Latitude' â€“ VÄ© Ä‘á»™\\n\"\n",
      " \"'Longitude' â€“ Kinh Ä‘á»™\")\n",
      "('Kiá»ƒu: torch.FloatTensor cÃ³ shape [batch_size,1]\\n'\n",
      " 'Ã nghÄ©a: Má»™t giÃ¡ trá»‹ thá»±c duy nháº¥t dá»± Ä‘oÃ¡n giÃ¡ nhÃ  trung bÃ¬nh cho má»—i máº«u.')\n",
      "('CÃ i Ä‘áº·t thÆ° viá»‡n cáº§n thiáº¿t báº±ng:\\n'\n",
      " 'pip install torch scikit-learn huggingface_hub')\n",
      "('```python\\n'\n",
      " 'import torch\\n'\n",
      " 'import torch.nn as nn\\n'\n",
      " 'from huggingface_hub import PyTorchModelHubMixin\\n'\n",
      " '\\n'\n",
      " '# Táº¡o Ä‘áº§u vÃ o máº«u ngáº«u nhiÃªn (chuáº©n hÃ³a giáº£ Ä‘á»‹nh ~ N(0,1))\\n'\n",
      " 'x_input = torch.randn(1,8) #1 máº«u,8 Ä‘áº·c trÆ°ng\\n'\n",
      " 'print(\"Mock input:\")\\n'\n",
      " 'print(x_input)\\n'\n",
      " '\\n'\n",
      " '# Äá»‹nh nghÄ©a láº¡i mÃ´ hÃ¬nh\\n'\n",
      " 'class MLP(nn.Module, PyTorchModelHubMixin):\\n'\n",
      " ' def __init__(self):\\n'\n",
      " ' super().__init__()\\n'\n",
      " ' self.fc1 = nn.Linear(8,50)\\n'\n",
      " ' self.fc2 = nn.Linear(50,50)\\n'\n",
      " ' self.fc3 = nn.Linear(50,50)\\n'\n",
      " ' self.fc4 = nn.Linear(50,1)\\n'\n",
      " ' def forward(self, x):\\n'\n",
      " ' act = torch.relu(self.fc1(x))\\n'\n",
      " ' act = torch.relu(self.fc2(act))\\n'\n",
      " ' act = torch.relu(self.fc3(act))\\n'\n",
      " ' return self.fc4(act)\\n'\n",
      " '\\n'\n",
      " '# Táº£i mÃ´ hÃ¬nh tá»« Hugging Face Hub\\n'\n",
      " 'model = MLP.from_pretrained(\"zhaospei/Model_3\")\\n'\n",
      " 'model.eval()\\n'\n",
      " '\\n'\n",
      " '# Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " ' prediction = model(x_input)\\n'\n",
      " 'print(f\"GiÃ¡ nhÃ  dá»± Ä‘oÃ¡n (mock input): {prediction.item():.3f}\")\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "ans = state['model_detailed_list']\n",
    "for i in range(len(ans.models)):\n",
    "    print(f\"Model {i}:\")\n",
    "    pprint.pprint(ans.models[i].model_description)\n",
    "    pprint.pprint(ans.models[i].model_input_format)\n",
    "    pprint.pprint(ans.models[i].model_output_format)\n",
    "    pprint.pprint(ans.models[i].model_requirements)\n",
    "    pprint.pprint(ans.models[i].model_sample_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06aa998c",
   "metadata": {},
   "source": [
    "## Output Labeling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "57408607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_agent(state: AgentState) -> AgentState:\n",
    "    prompt = r\"\"\"\n",
    "        Your are a specialist in machine learning. Your task is to identify the absolute classes of the given problem description, following with an output description.\n",
    "        ### Input:\n",
    "        - Problem description: {problem_description}\n",
    "        - Output description: {output_description}\n",
    "\n",
    "        You must return an array, strictly following these guidelines:\n",
    "        - Understand the context from the given problem description.\n",
    "        - Extract the class names from the output description. You **must not** invent new labels or translate the class names. Use them exactly as given in the output description.\n",
    "        - Create an array containing the classes.\n",
    "\n",
    "        You must return only the array containing those classes, without any formatting.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = prompt.format(\n",
    "        problem_description = state['task'],\n",
    "        output_description = state[\"subtasks\"].subtask_four\n",
    "    )\n",
    "    response = llm.invoke(prompt)\n",
    "    return {**state, \"output_classes\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184732f6",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b22aaec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unsure\n",
      "happy\n",
      "angry\n",
      "curious\n",
      "relaxed \n",
      "unsure \n",
      "bulldog \n",
      "beagle \n",
      "poodle \n",
      "pug \n",
      "chihuahua \n",
      "shih tzu \n",
      "corgi \n",
      "cat \n",
      "dog\n"
     ]
    }
   ],
   "source": [
    "state = output_agent(state)\n",
    "print(state['output_classes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f3c846",
   "metadata": {},
   "source": [
    "## Coding Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "4ff6c1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coding_agent(state: AgentState) -> AgentState:\n",
    "    base_prompt = r\"\"\"\n",
    "    You are a specialist in Machine Learning. Your task is to generate a *fully functional with all necessary imports and dependencies* codebase in *Python* that can be executed flawlessly.\n",
    "\n",
    "    You will be provided with:\n",
    "    - A problem description\n",
    "    - An input specification\n",
    "    - An output specification\n",
    "    - A description of one or many models that you needed to use\n",
    "    - A list of model needed to use to solve the problem\n",
    "\n",
    "    ### Input:\n",
    "    - Problem description: {problem_description}\n",
    "    - Problem input specification: {problem_input_description}\n",
    "    - Problem output specification: {problem_output_description}\n",
    "    - ML Model list and there detailed description: {model_list}\n",
    "    - Output classes: {output_classes}\n",
    "\n",
    "    ### Guidelines\n",
    "\n",
    "    You *must* strictly follow the following guidelines:\n",
    "    - The preprocessing step should be suitable for the data type.\n",
    "    - The postprocessing step should notices the differences between the data returned by the model and the output requirements. You must extract and use the exact class labels as defined in the output specification.\n",
    "    - Do *not invent new labels or translate* the class names. Use them exactly as given.\n",
    "    - You *must* make sure that your codebase can be executed flawlessly that would not encounter any errors or exceptions.\n",
    "    - **Important**: The code will run automatically without requiring or involving any human input during execution.\"\n",
    "\n",
    "    Your implementation *must strictly follow* the structure below:\n",
    "    1. *Imports*: All required libraries.\n",
    "    2. *Preprocessing*: Handle and transform the input as defined.\n",
    "    3. *Inference logic*: Use the described model for prediction. You *must* use tqdm or similar logging library to track progress.\n",
    "    4. *Postprocessing*: Format or transform the raw output into the final result as described.\n",
    "\n",
    "    You must *not* include any explanations, markdown, or logging outside what is required by the problem.\n",
    "\n",
    "    Return *only* the complete Python codebase, and you **MUST NOT** include a main function in any way. Wrap it with:\n",
    "    \\`\\`\\`python\n",
    "    # code here\n",
    "    \\`\\`\\`\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = base_prompt.format(\n",
    "        problem_description=state[\"task\"],\n",
    "        problem_input_description=state['subtasks'].subtask_three,\n",
    "        problem_output_description=state['subtasks'].subtask_four,\n",
    "        model_description=state[\"model_description\"],\n",
    "        model_list=state['model_detailed_list'].models,\n",
    "        output_classes=state[\"output_classes\"],\n",
    "    )\n",
    "    response = llm.invoke(prompt)\n",
    "    state['code'] = response.content\n",
    "    # return {**state, \"code\": response.content}\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738d8e9e",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "34c8d3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import os\n",
      "import csv\n",
      "from PIL import Image\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from tqdm import tqdm\n",
      "import numpy as np\n",
      "\n",
      "# Define constants\n",
      "IMAGE_FOLDER = 'images'\n",
      "OUTPUT_FILE = 'output.csv'\n",
      "\n",
      "# Load pre-trained model and processor\n",
      "model_name = \"thanhtlx/image_classification_01\"\n",
      "model = ViTForImageClassification.from_pretrained(model_name)\n",
      "processor = ViTImageProcessor.from_pretrained(model_name)\n",
      "\n",
      "# Function to check if a number is prime\n",
      "def is_prime(n):\n",
      "    if n <= 1:\n",
      "        return False\n",
      "    if n == 2:\n",
      "        return True\n",
      "    if n % 2 == 0:\n",
      "        return False\n",
      "    max_divisor = int(np.sqrt(n)) + 1\n",
      "    for d in range(3, max_divisor, 2):\n",
      "        if n % d == 0:\n",
      "            return False\n",
      "    return True\n",
      "\n",
      "# Preprocess and make predictions\n",
      "with open(OUTPUT_FILE, 'w', newline='') as csvfile:\n",
      "    fieldnames = ['file_name', 'prediction']\n",
      "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
      "    writer.writeheader()\n",
      "\n",
      "    for file_name in tqdm(os.listdir(IMAGE_FOLDER)):\n",
      "        if file_name.endswith(\".png\") or file_name.endswith(\".jpg\"):\n",
      "            image_path = os.path.join(IMAGE_FOLDER, file_name)\n",
      "            image = Image.open(image_path)\n",
      "\n",
      "            # Preprocess image\n",
      "            inputs = processor(images=image, return_tensors=\"pt\")\n",
      "\n",
      "            # Make prediction\n",
      "            with torch.no_grad():\n",
      "                outputs = model(**inputs)\n",
      "                predicted_class = outputs.logits.argmax(-1).item()\n",
      "\n",
      "            # Postprocess prediction\n",
      "            prediction = \"sá»‘ nguyÃªn tá»‘\" if is_prime(predicted_class) else \"khÃ´ng nguyÃªn tá»‘\"\n",
      "\n",
      "            # Write to CSV\n",
      "            writer.writerow({'file_name': file_name, 'prediction': prediction})\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "state = coding_agent(state)\n",
    "print(state['code'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72309b19",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "b980157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_agent(state: AgentState):\n",
    "    state = {\n",
    "        \"messages\": [],\n",
    "        \"task\": state[\"task\"],\n",
    "        \"data\": None,\n",
    "        \"subtasks\": None,\n",
    "        \"models\": None,\n",
    "        \"model_detailed_list\": None,\n",
    "        \"output_classes\": None,\n",
    "        \"code\": None\n",
    "    }\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b9ee679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "graph.add_node(\"init_node\", init_agent)\n",
    "graph.add_node(\"formulation_node\", classify_agent)\n",
    "graph.add_node(\"model_selection_node\", model_selection_agent)\n",
    "graph.add_node(\"model_selection_using_tool_node\", model_selection_using_tool_agent)\n",
    "graph.add_node(\"model_output_parser_node\", output_parser_agent)\n",
    "graph.add_node(\"labeling_node\", output_agent)\n",
    "graph.add_node(\"coding_node\", coding_agent)\n",
    "\n",
    "graph.add_edge(START, \"init_node\")\n",
    "graph.add_edge(\"init_node\", \"formulation_node\")\n",
    "graph.add_edge(\"formulation_node\", \"model_selection_node\")\n",
    "graph.add_edge(\"model_selection_node\", \"model_selection_using_tool_node\")\n",
    "graph.add_edge(\"model_selection_using_tool_node\", \"model_output_parser_node\")\n",
    "graph.add_edge(\"model_output_parser_node\", \"labeling_node\")\n",
    "graph.add_edge(\"labeling_node\", \"coding_node\")\n",
    "graph.add_edge(\"coding_node\", END)\n",
    "\n",
    "app = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "5f09819f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5348d0a",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "79568e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STT</th>\n",
       "      <th>Task</th>\n",
       "      <th>Validation data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nNháº­n dáº¡ng chá»¯ viáº¿t ta...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nTrong thá»i Ä‘áº¡i thÃ´ng t...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nÄÃ¢y lÃ  má»™t nhiá»‡m vá»¥ há»“...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nÄiá»u quan trá»ng lÃ  cÃ¡c...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nTrong nhiá»u á»©ng dá»¥ng t...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>ğŸ¾ Story: Aliceâ€™s Magical Pet Photo Box\\r\\nAlic...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STT                                               Task Validation data\n",
       "0    1  \"Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nNháº­n dáº¡ng chá»¯ viáº¿t ta...            Link\n",
       "1    2  Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nTrong thá»i Ä‘áº¡i thÃ´ng t...            Link\n",
       "2    3  Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nÄÃ¢y lÃ  má»™t nhiá»‡m vá»¥ há»“...            Link\n",
       "3    4  Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nÄiá»u quan trá»ng lÃ  cÃ¡c...            Link\n",
       "4    5  Bá»‘i cáº£nh cá»§a váº¥n Ä‘á»:\\r\\nTrong nhiá»u á»©ng dá»¥ng t...            Link\n",
       "5    6  ğŸ¾ Story: Aliceâ€™s Magical Pet Photo Box\\r\\nAlic...            Link"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "dc5039cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_no = 5\n",
    "task = df_task[\"Task\"][task_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5974d577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14401/7332665.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
      "/tmp/ipykernel_14401/7332665.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  url = str(row[2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_6'}`\n",
      "responded: To solve Alice's problem, we need a system that can analyze images and categorize them based on their content: dogs (with breed identification), a single cat (with emotion detection), or unsure (if the image is unclear). Given the models provided:\n",
      "\n",
      "1. **PhÃ¢n loáº¡i hÃ¬nh áº£nh thÃ nh100 lá»›p Ä‘á»‘i tÆ°á»£ng chi tiáº¿t (fine-grained)** â€” https://huggingface.co/zhaospei/Model_6\n",
      "2. **PhÃ¢n loáº¡i hÃ¬nh áº£nh giá»‘ng chÃ³** â€” https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\n",
      "3. **Nháº­n diá»‡n cáº£m xÃºc mÃ¨o** â€” https://huggingface.co/semihdervis/cat-emotion-classifier\n",
      "\n",
      "Let's gather detailed information about each model to determine which ones are suitable for Alice's task.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "ğŸ–¼ï¸ MÃ´ hÃ¬nh PhÃ¢n loáº¡i áº¢nh - ViT Fine-tuned trÃªn CIFAR-100\n",
      "ğŸ“ MÃ´ táº£\n",
      "ÄÃ¢y lÃ  mÃ´ hÃ¬nh Vision Transformer (ViT) Ä‘Æ°á»£c fine-tuned tá»« mÃ´ hÃ¬nh gá»‘c google/vit-base-patch16-224-in21k trÃªn táº­p dá»¯ liá»‡u CIFAR-100. MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ phÃ¢n loáº¡i áº£nh vÃ o 100 lá»›p khÃ¡c nhau, má»—i lá»›p Ä‘áº¡i diá»‡n cho má»™t danh má»¥c Ä‘á»‘i tÆ°á»£ng cá»¥ thá»ƒ.\n",
      "ğŸ“Œ Nhiá»‡m vá»¥\n",
      "Loáº¡i bÃ i toÃ¡n: PhÃ¢n loáº¡i áº£nh (Image Classification)\n",
      "Sá»‘ lá»›p: 100\n",
      "Táº­p dá»¯ liá»‡u: CIFAR-100\n",
      "ğŸ“¥ Äáº§u vÃ o\n",
      "Äá»‹nh dáº¡ng: áº¢nh mÃ u RGB\n",
      "KÃ­ch thÆ°á»›c áº£nh:  224x224\n",
      "ğŸ“¤ Äáº§u ra\n",
      "Äá»‹nh dáº¡ng: XÃ¡c suáº¥t cho má»—i lá»›p (logits)\n",
      "Kiá»ƒu dá»¯ liá»‡u: Tensor cÃ³ kÃ­ch thÆ°á»›c [batch_size, 100]\n",
      "Ã nghÄ©a: XÃ¡c suáº¥t dá»± Ä‘oÃ¡n cho tá»«ng lá»›p trong 100 lá»›p cá»§a CIFAR-100\n",
      "ğŸ›  YÃªu cáº§u thÆ° viá»‡n\n",
      "CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t báº±ng:\n",
      "pip install transformers torch torchvision\n",
      "ğŸ§ª CÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh\n",
      "DÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ vá» cÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘á»ƒ phÃ¢n loáº¡i má»™t áº£nh:\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "# Táº£i áº£nh cáº§n phÃ¢n loáº¡i\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "# Táº£i processor vÃ  mÃ´ hÃ¬nh tá»« Hugging Face\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\n",
      "# Xá»­ lÃ½ Ä‘áº§u vÃ o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "# Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_label = logits.argmax(-1).item()\n",
      "print(f\"NhÃ£n dá»± Ä‘oÃ¡n: {model.config.id2label[predicted_label]}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "Model made by notebook first posted in my Kaggle.\n",
      "Model Motivation\n",
      "Recently, someone asked me if you can classify dog images into their respective dog breeds instead just differentiating from cats vs dogs like my last notebook. I say YES!\n",
      "Due to the complexity of the problem, we will be using the most advanced computer vision architecture released in the 2020 Google paper, the Vision Transformer.\n",
      "The difference between the Vision Transformer and the traditional Convolutional Neural Network (CNN) is how it treats an image. In Vision Transformers, we take the input as a patch of the original image, say 16 x 16, and feed in into the Transformer as a sequence with positional embeddings and self-attention, while in the Convolutional Neural Network (CNN), we use the same patch of original image as an input, but use convolutions and pooling layers as inductive biases. What this means is that Vision Transformer can use it's judgement to attend any particular patch of the image in a global fashion using it's self-attention mechanism without having us to guide the neural network like a CNN with local centering/cropping/bounding box our images to help its convolutions. \n",
      "This allows the Vision Transformer architecture to be more flexible and scalable in nature, allowing us to create foundation models in computer vision, similar to the NLP foundational models like BERT and GPT, with pre-training self-supervised/supervised on massive amount of image data that would generalize to different computer vision tasks such as image classification, recognition, segmentation, etc. This cross-pollination helps us move closer towards the goal of Artificial General Intelligence.\n",
      "One thing about Vision Transformers are it has weaker inductive biases compared to Convolutional Neural Networks that enables it's scalability and flexibility. This feature/bug depending on who you ask will require most well-performing pre-trained models to require more data despite having less parameters compared to it's CNN counterparts.\n",
      "Luckily, in this model, we will use a Vision Transformer from Google hosted at HuggingFace pre-trained on the ImageNet-21k dataset (14 million images, 21k classes) with 16x16 patches, 224x224 resolution to bypass that data limitation. We will be fine-tuning this model to our \"small\" dog breeds dataset of around 20 thousand images from the Stanford Dogs dataset imported by Jessica Li into Kaggle to classify dog images into 120 types of dog breeds!\n",
      "Model Description\n",
      "This model is finetuned using the Google Vision Transformer (vit-base-patch16-224-in21k) on the Stanford Dogs dataset in Kaggle to classify dog images into 120 types of dog breeds.\n",
      "Intended Uses & Limitations\n",
      "You can use this finetuned model to classify images of dogs only and dog breeds that are in the dataset.\n",
      "How to Use\n",
      "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
      "import PIL\n",
      "import requests\n",
      "url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\n",
      "image = PIL.Image.open(requests.get(url, stream=True).raw)\n",
      "image_processor = AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "model = AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
      "outputs = model(**inputs)\n",
      "logits = outputs.logits\n",
      "# model predicts one of the 120 Stanford dog breeds classes\n",
      "predicted_class_idx = logits.argmax(-1).item()\n",
      "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
      "Model Training Metrics\n",
      "Epoch\n",
      "Top-1 Accuracy\n",
      "Top-3 Accuracy\n",
      "Top-5 Accuracy\n",
      "Macro F1\n",
      "1\n",
      "79.8%\n",
      "95.1%\n",
      "97.5%\n",
      "77.2%\n",
      "2\n",
      "83.8%\n",
      "96.7%\n",
      "98.2%\n",
      "81.9%\n",
      "3\n",
      "84.8%\n",
      "96.7%\n",
      "98.3%\n",
      "83.4%\n",
      "Model Evaluation Metrics\n",
      "Top-1 Accuracy\n",
      "Top-3 Accuracy\n",
      "Top-5 Accuracy\n",
      "Macro F1\n",
      "84.0%\n",
      "97.1%\n",
      "98.7%\n",
      "83.0%\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/semihdervis/cat-emotion-classifier'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "vit-base-cat-emotions\n",
      "You can try out the model live here, and check out the GitHub repository for more details.\n",
      "This model is a fine-tuned version of google/vit-base-patch16-224-in21k on the custom dataset dataset.\n",
      "It achieves the following results on the evaluation set:\n",
      "Loss: 1.0160\n",
      "Accuracy: 0.6353\n",
      "Model description\n",
      "More information needed\n",
      "Intended uses & limitations\n",
      "More information needed\n",
      "Training and evaluation data\n",
      "More information needed\n",
      "Training procedure\n",
      "Training hyperparameters\n",
      "The following hyperparameters were used during training:\n",
      "learning_rate: 0.0002\n",
      "train_batch_size: 16\n",
      "eval_batch_size: 8\n",
      "seed: 42\n",
      "optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n",
      "lr_scheduler_type: linear\n",
      "num_epochs: 10\n",
      "mixed_precision_training: Native AMP\n",
      "Training results\n",
      "Training Loss\n",
      "Epoch\n",
      "Step\n",
      "Validation Loss\n",
      "Accuracy\n",
      "0.3361\n",
      "3.125\n",
      "100\n",
      "1.0125\n",
      "0.6548\n",
      "0.0723\n",
      "6.25\n",
      "200\n",
      "0.9043\n",
      "0.7381\n",
      "0.0321\n",
      "9.375\n",
      "300\n",
      "0.9268\n",
      "0.7143\n",
      "Framework versions\n",
      "Transformers 4.44.1\n",
      "Pytorch 2.2.2+cu118\n",
      "Datasets 2.20.0\n",
      "Tokenizers 0.19.1\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_6'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "ğŸ–¼ï¸ MÃ´ hÃ¬nh PhÃ¢n loáº¡i áº¢nh - ViT Fine-tuned trÃªn CIFAR-100\n",
      "ğŸ“ MÃ´ táº£\n",
      "ÄÃ¢y lÃ  mÃ´ hÃ¬nh Vision Transformer (ViT) Ä‘Æ°á»£c fine-tuned tá»« mÃ´ hÃ¬nh gá»‘c google/vit-base-patch16-224-in21k trÃªn táº­p dá»¯ liá»‡u CIFAR-100. MÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ phÃ¢n loáº¡i áº£nh vÃ o 100 lá»›p khÃ¡c nhau, má»—i lá»›p Ä‘áº¡i diá»‡n cho má»™t danh má»¥c Ä‘á»‘i tÆ°á»£ng cá»¥ thá»ƒ.\n",
      "ğŸ“Œ Nhiá»‡m vá»¥\n",
      "Loáº¡i bÃ i toÃ¡n: PhÃ¢n loáº¡i áº£nh (Image Classification)\n",
      "Sá»‘ lá»›p: 100\n",
      "Táº­p dá»¯ liá»‡u: CIFAR-100\n",
      "ğŸ“¥ Äáº§u vÃ o\n",
      "Äá»‹nh dáº¡ng: áº¢nh mÃ u RGB\n",
      "KÃ­ch thÆ°á»›c áº£nh:  224x224\n",
      "ğŸ“¤ Äáº§u ra\n",
      "Äá»‹nh dáº¡ng: XÃ¡c suáº¥t cho má»—i lá»›p (logits)\n",
      "Kiá»ƒu dá»¯ liá»‡u: Tensor cÃ³ kÃ­ch thÆ°á»›c [batch_size, 100]\n",
      "Ã nghÄ©a: XÃ¡c suáº¥t dá»± Ä‘oÃ¡n cho tá»«ng lá»›p trong 100 lá»›p cá»§a CIFAR-100\n",
      "ğŸ›  YÃªu cáº§u thÆ° viá»‡n\n",
      "CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t báº±ng:\n",
      "pip install transformers torch torchvision\n",
      "ğŸ§ª CÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh\n",
      "DÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ vá» cÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘á»ƒ phÃ¢n loáº¡i má»™t áº£nh:\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "# Táº£i áº£nh cáº§n phÃ¢n loáº¡i\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "# Táº£i processor vÃ  mÃ´ hÃ¬nh tá»« Hugging Face\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\n",
      "# Xá»­ lÃ½ Ä‘áº§u vÃ o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "# Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_label = logits.argmax(-1).item()\n",
      "print(f\"NhÃ£n dá»± Ä‘oÃ¡n: {model.config.id2label[predicted_label]}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "Model made by notebook first posted in my Kaggle.\n",
      "Model Motivation\n",
      "Recently, someone asked me if you can classify dog images into their respective dog breeds instead just differentiating from cats vs dogs like my last notebook. I say YES!\n",
      "Due to the complexity of the problem, we will be using the most advanced computer vision architecture released in the 2020 Google paper, the Vision Transformer.\n",
      "The difference between the Vision Transformer and the traditional Convolutional Neural Network (CNN) is how it treats an image. In Vision Transformers, we take the input as a patch of the original image, say 16 x 16, and feed in into the Transformer as a sequence with positional embeddings and self-attention, while in the Convolutional Neural Network (CNN), we use the same patch of original image as an input, but use convolutions and pooling layers as inductive biases. What this means is that Vision Transformer can use it's judgement to attend any particular patch of the image in a global fashion using it's self-attention mechanism without having us to guide the neural network like a CNN with local centering/cropping/bounding box our images to help its convolutions. \n",
      "This allows the Vision Transformer architecture to be more flexible and scalable in nature, allowing us to create foundation models in computer vision, similar to the NLP foundational models like BERT and GPT, with pre-training self-supervised/supervised on massive amount of image data that would generalize to different computer vision tasks such as image classification, recognition, segmentation, etc. This cross-pollination helps us move closer towards the goal of Artificial General Intelligence.\n",
      "One thing about Vision Transformers are it has weaker inductive biases compared to Convolutional Neural Networks that enables it's scalability and flexibility. This feature/bug depending on who you ask will require most well-performing pre-trained models to require more data despite having less parameters compared to it's CNN counterparts.\n",
      "Luckily, in this model, we will use a Vision Transformer from Google hosted at HuggingFace pre-trained on the ImageNet-21k dataset (14 million images, 21k classes) with 16x16 patches, 224x224 resolution to bypass that data limitation. We will be fine-tuning this model to our \"small\" dog breeds dataset of around 20 thousand images from the Stanford Dogs dataset imported by Jessica Li into Kaggle to classify dog images into 120 types of dog breeds!\n",
      "Model Description\n",
      "This model is finetuned using the Google Vision Transformer (vit-base-patch16-224-in21k) on the Stanford Dogs dataset in Kaggle to classify dog images into 120 types of dog breeds.\n",
      "Intended Uses & Limitations\n",
      "You can use this finetuned model to classify images of dogs only and dog breeds that are in the dataset.\n",
      "How to Use\n",
      "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
      "import PIL\n",
      "import requests\n",
      "url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\n",
      "image = PIL.Image.open(requests.get(url, stream=True).raw)\n",
      "image_processor = AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "model = AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
      "outputs = model(**inputs)\n",
      "logits = outputs.logits\n",
      "# model predicts one of the 120 Stanford dog breeds classes\n",
      "predicted_class_idx = logits.argmax(-1).item()\n",
      "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
      "Model Training Metrics\n",
      "Epoch\n",
      "Top-1 Accuracy\n",
      "Top-3 Accuracy\n",
      "Top-5 Accuracy\n",
      "Macro F1\n",
      "1\n",
      "79.8%\n",
      "95.1%\n",
      "97.5%\n",
      "77.2%\n",
      "2\n",
      "83.8%\n",
      "96.7%\n",
      "98.2%\n",
      "81.9%\n",
      "3\n",
      "84.8%\n",
      "96.7%\n",
      "98.3%\n",
      "83.4%\n",
      "Model Evaluation Metrics\n",
      "Top-1 Accuracy\n",
      "Top-3 Accuracy\n",
      "Top-5 Accuracy\n",
      "Macro F1\n",
      "84.0%\n",
      "97.1%\n",
      "98.7%\n",
      "83.0%\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/semihdervis/cat-emotion-classifier'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "vit-base-cat-emotions\n",
      "You can try out the model live here, and check out the GitHub repository for more details.\n",
      "This model is a fine-tuned version of google/vit-base-patch16-224-in21k on the custom dataset dataset.\n",
      "It achieves the following results on the evaluation set:\n",
      "Loss: 1.0160\n",
      "Accuracy: 0.6353\n",
      "Model description\n",
      "More information needed\n",
      "Intended uses & limitations\n",
      "More information needed\n",
      "Training and evaluation data\n",
      "More information needed\n",
      "Training procedure\n",
      "Training hyperparameters\n",
      "The following hyperparameters were used during training:\n",
      "learning_rate: 0.0002\n",
      "train_batch_size: 16\n",
      "eval_batch_size: 8\n",
      "seed: 42\n",
      "optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n",
      "lr_scheduler_type: linear\n",
      "num_epochs: 10\n",
      "mixed_precision_training: Native AMP\n",
      "Training results\n",
      "Training Loss\n",
      "Epoch\n",
      "Step\n",
      "Validation Loss\n",
      "Accuracy\n",
      "0.3361\n",
      "3.125\n",
      "100\n",
      "1.0125\n",
      "0.6548\n",
      "0.0723\n",
      "6.25\n",
      "200\n",
      "0.9043\n",
      "0.7381\n",
      "0.0321\n",
      "9.375\n",
      "300\n",
      "0.9268\n",
      "0.7143\n",
      "Framework versions\n",
      "Transformers 4.44.1\n",
      "Pytorch 2.2.2+cu118\n",
      "Datasets 2.20.0\n",
      "Tokenizers 0.19.1\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mğŸ¤” Based on the models provided:\n",
      "\n",
      "1. **PhÃ¢n loáº¡i hÃ¬nh áº£nh thÃ nh100 lá»›p Ä‘á»‘i tÆ°á»£ng chi tiáº¿t (fine-grained)** â€” https://huggingface.co/zhaospei/Model_6\n",
      "2. **PhÃ¢n loáº¡i hÃ¬nh áº£nh giá»‘ng chÃ³** â€” https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\n",
      "3. **Nháº­n diá»‡n cáº£m xÃºc mÃ¨o** â€” https://huggingface.co/semihdervis/cat-emotion-classifier\n",
      "\n",
      "Let's analyze their capabilities:\n",
      "\n",
      "### Model 1: PhÃ¢n loáº¡i hÃ¬nh áº£nh thÃ nh100 lá»›p Ä‘á»‘i tÆ°á»£ng chi tiáº¿t (fine-grained)\n",
      "- **Description:** Fine-tuned ViT model on CIFAR-100 dataset for 100-class image classification.\n",
      "- **Input:** RGB images, 224x224.\n",
      "- **Output:** Probabilities for each of the 100 classes.\n",
      "- **Library Requirements:** `transformers`, `torch`, `torchvision`.\n",
      "- **Example Code:**\n",
      "  ```python\n",
      "  import torch\n",
      "  from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "  from PIL import Image\n",
      "  \n",
      "  image = Image.open(\"path_to_your_image.jpg\")\n",
      "  processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\n",
      "  model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\n",
      "  inputs = processor(images=image, return_tensors=\"pt\")\n",
      "  with torch.no_grad():\n",
      "      outputs = model(**inputs)\n",
      "      logits = outputs.logits\n",
      "      predicted_label = logits.argmax(-1).item()\n",
      "  print(f\"Predicted class: {model.config.id2label[predicted_label]}\")\n",
      "  ```\n",
      "\n",
      "### Model 2: PhÃ¢n loáº¡i hÃ¬nh áº£nh giá»‘ng chÃ³\n",
      "- **Description:** Fine-tuned ViT model on Stanford Dogs dataset to classify dog images into 120 breeds.\n",
      "- **Input:** Dog images.\n",
      "- **Output:** Predicted dog breed.\n",
      "- **Library Requirements:** `transformers`, `PIL`, `requests`.\n",
      "- **Example Code:**\n",
      "  ```python\n",
      "  from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
      "  import PIL\n",
      "  import requests\n",
      "  \n",
      "  url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\n",
      "  image = PIL.Image.open(requests.get(url, stream=True).raw)\n",
      "  image_processor = AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "  model = AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "  inputs = image_processor(images=image, return_tensors=\"pt\")\n",
      "  outputs = model(**inputs)\n",
      "  logits = outputs.logits\n",
      "  predicted_class_idx = logits.argmax(-1).item()\n",
      "  print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
      "  ```\n",
      "\n",
      "### Model 3: Nháº­n diá»‡n cáº£m xÃºc mÃ¨o\n",
      "- **Description:** Fine-tuned ViT model for cat emotion classification.\n",
      "- **Input:** Cat images.\n",
      "- **Output:** Predicted emotion (e.g., happy, angry, curious, relaxed).\n",
      "- **Library Requirements:** `transformers`, `torch`.\n",
      "- **Example Code:**\n",
      "  ```python\n",
      "  import torch\n",
      "  from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "  from PIL import Image\n",
      "  \n",
      "  image = Image.open(\"path_to_your_image.jpg\")\n",
      "  processor = ViTImageProcessor.from_pretrained(\"semihdervis/cat-emotion-classifier\")\n",
      "  model = ViTForImageClassification.from_pretrained(\"semihdervis/cat-emotion-classifier\")\n",
      "  inputs = processor(images=image, return_tensors=\"pt\")\n",
      "  with torch.no_grad():\n",
      "      outputs = model(**inputs)\n",
      "      logits = outputs.logits\n",
      "      predicted_label = logits.argmax(-1).item()\n",
      "  print(f\"Predicted emotion: {model.config.id2label[predicted_label]}\")\n",
      "  ```\n",
      "\n",
      "### Best-fit Models for Aliceâ€™s Task\n",
      "- **For dog breed classification:**  \n",
      "  `PhÃ¢n loáº¡i hÃ¬nh áº£nh giá»‘ng chÃ³` â€” https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit  \n",
      "  - **Description:** Fine-tuned ViT model to classify dog images into 120 breeds.\n",
      "  - **Input format:** Dog images, RGB, 224x224.\n",
      "  - **Output format:** Predicted dog breed.\n",
      "  - **Library Requirements:** `transformers`, `PIL`, `requests`.\n",
      "  - **Example Code:**\n",
      "    ```python\n",
      "    from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
      "    import PIL\n",
      "    import requests\n",
      "    \n",
      "    url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\n",
      "    image = PIL.Image.open(requests.get(url, stream=True).raw)\n",
      "    image_processor = AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "    model = AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_class_idx = logits.argmax(-1).item()\n",
      "    print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
      "    ```\n",
      "\n",
      "- **For cat emotion classification:**  \n",
      "  `Nháº­n diá»‡n cáº£m xÃºc mÃ¨o` â€” https://huggingface.co/semihdervis/cat-emotion-classifier  \n",
      "  - **Description:** Fine-tuned ViT model for cat emotion classification.\n",
      "  - **Input format:** Cat images, RGB, 224x224.\n",
      "  - **Output format:** Predicted emotion.\n",
      "  - **Library Requirements:** `transformers`, `torch`.\n",
      "  - **Example Code:**\n",
      "    ```python\n",
      "    import torch\n",
      "    from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "    from PIL import Image\n",
      "    \n",
      "    image = Image.open(\"path_to_your_image.jpg\")\n",
      "    processor = ViTImageProcessor.from_pretrained(\"semihdervis/cat-emotion-classifier\")\n",
      "    model = ViTForImageClassification.from_pretrained(\"semihdervis/cat-emotion-classifier\")\n",
      "    inputs = processor(images=image, return_tensors=\"pt\")\n",
      "    with torch.no_grad():\n",
      "        outputs = model(**inputs)\n",
      "        logits = outputs.logits\n",
      "        predicted_label = logits.argmax(-1).item()\n",
      "    print(f\"Predicted emotion: {model.config.id2label[predicted_label]}\")\n",
      "    ```\n",
      "\n",
      "- **For general object classification (to handle unsure cases):**  \n",
      "  `PhÃ¢n loáº¡i hÃ¬nh áº£nh thÃ nh100 lá»›p Ä‘á»‘i tÆ°á»£ng chi tiáº¿t (fine-grained)` â€” https://huggingface.co/zhaospei/Model_6  \n",
      "  - **Description:** Fine-tuned ViT model on CIFAR-100 dataset for 100-class image classification.\n",
      "  - **Input format:** RGB images, 224x224.\n",
      "  - **Output format:** Probabilities for each of the 100 classes.\n",
      "  - **Library Requirements:** `transformers`, `torch`, `torchvision`.\n",
      "  - **Example Code:**\n",
      "    ```python\n",
      "    import torch\n",
      "    from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "    from PIL import Image\n",
      "    \n",
      "    image = Image.open(\"path_to_your_image.jpg\")\n",
      "    processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\n",
      "    model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\n",
      "    inputs = processor(images=image, return_tensors=\"pt\")\n",
      "    with torch.no_grad():\n",
      "        outputs = model(**inputs)\n",
      "        logits = outputs.logits\n",
      "        predicted_label = logits.argmax(-1).item()\n",
      "    print(f\"Predicted class: {model.config.id2label[predicted_label]}\")\n",
      "    ```\n",
      "\n",
      "### Conclusion\n",
      "- Use `PhÃ¢n loáº¡i hÃ¬nh áº£nh giá»‘ng chÃ³` for dog breed classification.\n",
      "- Use `Nháº­n diá»‡n cáº£m xÃºc mÃ¨o` for cat emotion classification.\n",
      "- Use `PhÃ¢n loáº¡i hÃ¬nh áº£nh thÃ nh100 lá»›p Ä‘á»‘i tÆ°á»£ng chi tiáº¿t (fine-grained)` as a fallback for unsure cases.\n",
      "\n",
      "This combination will help Alice organize her pet photo collection effectively.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "state = app.invoke({\"task\": task})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72b342",
   "metadata": {},
   "source": [
    "## Model Selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "75d8cbdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0:\n",
      "'Fine-tuned ViT model to classify dog images into 120 breeds.'\n",
      "'Dog images, RGB, 224x224.'\n",
      "'Predicted dog breed.'\n",
      "'transformers, PIL, requests'\n",
      "('from transformers import AutoImageProcessor, '\n",
      " 'AutoModelForImageClassification\\n'\n",
      " 'import PIL\\n'\n",
      " 'import requests\\n'\n",
      " 'url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\\n'\n",
      " 'image = PIL.Image.open(requests.get(url, stream=True).raw)\\n'\n",
      " 'image_processor = '\n",
      " 'AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\\n'\n",
      " 'model = '\n",
      " 'AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\\n'\n",
      " 'inputs = image_processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'outputs = model(**inputs)\\n'\n",
      " 'logits = outputs.logits\\n'\n",
      " 'predicted_class_idx = logits.argmax(-1).item()\\n'\n",
      " 'print(\"Predicted class:\", model.config.id2label[predicted_class_idx])')\n",
      "Model 1:\n",
      "'Fine-tuned ViT model for cat emotion classification.'\n",
      "'Cat images, RGB, 224x224.'\n",
      "'Predicted emotion (e.g., happy, angry, curious, relaxed).'\n",
      "'transformers, torch'\n",
      "('import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " 'processor = '\n",
      " 'ViTImageProcessor.from_pretrained(\"semihdervis/cat-emotion-classifier\")\\n'\n",
      " 'model = '\n",
      " 'ViTForImageClassification.from_pretrained(\"semihdervis/cat-emotion-classifier\")\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '    outputs = model(**inputs)\\n'\n",
      " '    logits = outputs.logits\\n'\n",
      " '    predicted_label = logits.argmax(-1).item()\\n'\n",
      " '    print(f\"Predicted emotion: {model.config.id2label[predicted_label]}\")')\n",
      "Model 2:\n",
      "'Fine-tuned ViT model on CIFAR-100 dataset for 100-class image classification.'\n",
      "'RGB images, 224x224.'\n",
      "'Probabilities for each of the 100 classes.'\n",
      "'transformers, torch, torchvision'\n",
      "('import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '    outputs = model(**inputs)\\n'\n",
      " '    logits = outputs.logits\\n'\n",
      " '    predicted_label = logits.argmax(-1).item()\\n'\n",
      " '    print(f\"Predicted class: {model.config.id2label[predicted_label]}\")')\n"
     ]
    }
   ],
   "source": [
    "ans = state['model_detailed_list']\n",
    "for i in range(len(ans.models)):\n",
    "    print(f\"Model {i}:\")\n",
    "    pprint.pprint(ans.models[i].model_description)\n",
    "    pprint.pprint(ans.models[i].model_input_format)\n",
    "    pprint.pprint(ans.models[i].model_output_format)\n",
    "    pprint.pprint(ans.models[i].model_requirements)\n",
    "    pprint.pprint(ans.models[i].model_sample_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1831289",
   "metadata": {},
   "source": [
    "## Code Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "eed19012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import os\n",
      "import csv\n",
      "from PIL import Image\n",
      "import torch\n",
      "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Define the models and their corresponding labels\n",
      "models = [\n",
      "    {\"model_name\": \"dog_breed_classifier\", \"model_path\": \"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\", \"labels\": [\"dalmatian\", \"pug\", \"poodle\", \"bulldog\", \"unsure\"]},\n",
      "    {\"model_name\": \"cat_emotion_classifier\", \"model_path\": \"semihdervis/cat-emotion-classifier\", \"labels\": [\"happy\", \"angry\", \"curious\", \"relaxed\", \"unsure\"]},\n",
      "    {\"model_name\": \"image_classifier\", \"model_path\": \"zhaospei/Model_6\", \"labels\": list(range(100))}\n",
      "]\n",
      "\n",
      "# Load the models and their processors\n",
      "model_instances = []\n",
      "for model in models:\n",
      "    processor = AutoImageProcessor.from_pretrained(model[\"model_path\"])\n",
      "    model_instance = AutoModelForImageClassification.from_pretrained(model[\"model_path\"])\n",
      "    model_instances.append({\"processor\": processor, \"model\": model_instance, \"labels\": model[\"labels\"]})\n",
      "\n",
      "# Define the image directory and output file\n",
      "image_dir = \"images/\"\n",
      "output_file = \"output.csv\"\n",
      "\n",
      "# Define the preprocessing function\n",
      "def preprocess_image(image_path):\n",
      "    image = Image.open(image_path)\n",
      "    return image\n",
      "\n",
      "# Define the inference logic function\n",
      "def infer_image(image):\n",
      "    results = []\n",
      "    for model_instance in model_instances:\n",
      "        inputs = model_instance[\"processor\"](images=image, return_tensors=\"pt\")\n",
      "        with torch.no_grad():\n",
      "            outputs = model_instance[\"model\"](**inputs)\n",
      "            logits = outputs.logits\n",
      "            predicted_class_idx = logits.argmax(-1).item()\n",
      "            results.append(model_instance[\"labels\"][predicted_class_idx])\n",
      "    return results\n",
      "\n",
      "# Define the postprocessing function\n",
      "def postprocess_image(image_path, results):\n",
      "    id = os.path.splitext(os.path.basename(image_path))[0]\n",
      "    if results[0] == \"unsure\" or results[1] == \"unsure\":\n",
      "        return [id, \"unsure\"]\n",
      "    elif results[0] in [\"dalmatian\", \"pug\", \"poodle\", \"bulldog\", \"unsure\"]:\n",
      "        return [id, results[0]]\n",
      "    else:\n",
      "        return [id, results[1]]\n",
      "\n",
      "# Process the images\n",
      "with open(output_file, \"w\", newline=\"\") as csvfile:\n",
      "    writer = csv.writer(csvfile)\n",
      "    writer.writerow([\"id\", \"prediction\"])\n",
      "    for image_path in tqdm(os.listdir(image_dir)):\n",
      "        if image_path.endswith(\".jpg\") or image_path.endswith(\".png\"):\n",
      "            image_path = os.path.join(image_dir, image_path)\n",
      "            image = preprocess_image(image_path)\n",
      "            results = infer_image(image)\n",
      "            writer.writerow(postprocess_image(image_path, results))\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(state['code'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
