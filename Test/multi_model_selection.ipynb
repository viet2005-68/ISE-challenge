{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d3e60a",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "535f3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY='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'\n",
    "FIRECRAWL_API_KEY='fc-e36282d62d1c4ee8971ec0649f3862c7'\n",
    "BASE_URL='https://api.groq.com/openai/v1'\n",
    "MODEL=\"meta-llama/llama-4-scout-17b-16e-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42744f82",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91bed7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/mnt/g/Ai agents course/LangGraph/venv/lib/python3.12/site-packages', '..', '.']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('.')\n",
    "print(sys.path)\n",
    "from tools.fetching_description_from_huggingface import fetching_description_from_huggingface\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import Tool\n",
    "from firecrawl import FirecrawlApp\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36830718",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5827b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task = pd.read_csv(\"data/ISE - AutoCode Challenge 2 Public - Public task.csv\")\n",
    "df_model = pd.read_csv(\"data/ISE - AutoCode Challenge 2 Public - Model zoo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22abc01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STT</th>\n",
       "      <th>Task</th>\n",
       "      <th>Validation data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt ta...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong th·ªùi ƒë·∫°i th√¥ng t...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒê√¢y l√† m·ªôt nhi·ªám v·ª• h·ªì...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒêi·ªÅu quan tr·ªçng l√† c√°c...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong nhi·ªÅu ·ª©ng d·ª•ng t...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>üêæ Story: Alice‚Äôs Magical Pet Photo Box\\r\\nAlic...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STT                                               Task Validation data\n",
       "0    1  \"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt ta...            Link\n",
       "1    2  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong th·ªùi ƒë·∫°i th√¥ng t...            Link\n",
       "2    3  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒê√¢y l√† m·ªôt nhi·ªám v·ª• h·ªì...            Link\n",
       "3    4  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒêi·ªÅu quan tr·ªçng l√† c√°c...            Link\n",
       "4    5  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong nhi·ªÅu ·ª©ng d·ª•ng t...            Link\n",
       "5    6  üêæ Story: Alice‚Äôs Magical Pet Photo Box\\r\\nAlic...            Link"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "697add1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = df_task[\"Task\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "953cae02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üêæ Story: Alice‚Äôs Magical Pet Photo Box\\r\\nAlice is an animal lover who adores collecting pictures of pets. In her room, she keeps a magical box filled with hundreds of photos of dogs and cats that she has taken or found from all over the internet.\\r\\n\\r\\nBut there‚Äôs a little problem...\\r\\n\\r\\nOver time, Alice‚Äôs collection has gotten messy. Her box now contains a mix of dog photos, cat photos, and even some blurry or unrecognizable images. She needs a smart system to help her sort through the mess and organize everything again.\\r\\n\\r\\nüß† Your Mission\\r\\nYou‚Äôre building an intelligent system that can analyze each image in Alice‚Äôs magical box and decide what it contains. Each image will fall into one of the following categories:\\r\\n\\r\\nüê∂ If the image contains dogs:\\r\\n\\r\\nThere may be one or more dogs in the photo.\\r\\n\\r\\nYour system should detect all visible dogs, identify their breed, and return the most frequent breed in the image.\\r\\n\\r\\nüê± If the image contains a single cat:\\r\\n\\r\\nLook at the cat‚Äôs facial expression and determine its most likely emotion ‚Äî such as \"happy\", \"angry\", \"curious\", or \"relaxed\".\\r\\n\\r\\n‚ùì If the image is unclear (e.g., no visible pet or the view is too blurry):\\r\\n\\r\\nThe system should simply return \"unsure\" to avoid making a wrong guess.\\r\\n\\r\\nüì¶ The Contents of Alice‚Äôs Box\\r\\nThe images are stored in the following directory:\\r\\n\\r\\nimages/\\r\\n\\r\\nEach image is in .jpg or .png format, typically containing a pet in the foreground.\\r\\n\\r\\nüìù Expected Output\\r\\nAfter processing, your system should save the result to file output.csv with format:\\r\\n\\r\\nid: the image filename without extension (e.g., 001.png ‚Üí id = 001)\\r\\n\\r\\nprediction:\\r\\n\\r\\n    The most frequent dog breed (if dogs are present)\\r\\n\\r\\n    The cat‚Äôs emotion (if a single cat is present)\\r\\n\\r\\n    \"unsure\" (if the content is ambiguous or invalid)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6909b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_list(df):\n",
    "    model_strings = []\n",
    "    for i, row in df.iterrows():\n",
    "        desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        url = str(row[2])\n",
    "        model_strings.append(f\"{i + 1} - {desc} [More info]({url})\")\n",
    "    return \"\\n\".join(model_strings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c40886",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af3bcff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraper(url: str) -> str:\n",
    "    \"\"\"Get more details about the model using the model url\"\"\"\n",
    "    # scraper = FirecrawlApp(api_key=FIRECRAWL_API_KEY)\n",
    "    # scrape_status = scraper.scrape_url(\n",
    "    #     url,\n",
    "    #     formats=['markdown']\n",
    "    # )\n",
    "    # return scrape_status.markdown\n",
    "    result = fetching_description_from_huggingface(url)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77f3a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraperParameters(BaseModel):\n",
    "    url: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6a0e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = web_scraper('https://huggingface.co/thanhtlx/image_classification_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52a9c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"web_scrapper\",\n",
    "        func=web_scraper,\n",
    "        description=\"Use to get more detail about the model\",\n",
    "        args_schema=WebScraperParameters\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c409f8b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eca82430",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI( \n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6511a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tasks(BaseModel):\n",
    "    subtask_one: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 1\"\n",
    "    )\n",
    "    subtask_two: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 2\"\n",
    "    )\n",
    "    subtask_three: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 3\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0de6f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: HumanMessage | None\n",
    "    data: str | None\n",
    "    messages: Annotated[BaseMessage, add_messages]\n",
    "    subtasks: Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef593964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_agent(state: AgentState) -> AgentState:\n",
    "    structured_llm = llm.with_structured_output(Tasks)\n",
    "    system_prompt = \"\"\"\n",
    "                    You are an expert in task formulation. Given a machine learning task, you will\n",
    "                    help divide the task into these subtasks:\n",
    "                    - Subtask1: for classifying the problem type and task description\n",
    "                    - Subtask2: for identifying the input of the problem\n",
    "                    - Subtask3: for identifying the output of the problem\n",
    "                    describe the each subtask as specifically as possible\n",
    "                    \"\"\"\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    messages.append(state['task'])\n",
    "    result = structured_llm.invoke(messages)\n",
    "    state['subtasks'] = result\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14b15c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"task\": HumanMessage(content=task),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78675acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = classify_agent(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab3d7f",
   "metadata": {},
   "source": [
    "# Modeling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f656ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInfo(BaseModel):\n",
    "    model: str = Field(..., description=\"Model name\")\n",
    "    url: str = Field(..., description=\"URL to the model\")\n",
    "\n",
    "class ModelSelection(BaseModel):\n",
    "    models: List[ModelInfo] = Field(..., description=\"List of available models with name and URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18dcc46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_parser = PydanticOutputParser(pydantic_object=ModelSelection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de4ed792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the suitable model for a given task.\n",
    "                    Given:\n",
    "                    - A user description about the task\n",
    "                    - A list of available model (names and links)\n",
    "                    Your job is to choose ALL suitable models provided in the model list for user's specific tasks.\n",
    "                    Return the answer in the format\n",
    "                    {structured_output}\n",
    "                    Here are the list of model:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]).partial(structured_output=model_selection_parser.get_format_instructions())\n",
    "\n",
    "    chain = prompt | llm | model_selection_parser\n",
    "\n",
    "    result = chain.invoke({\"input\": task, \"model_list\": format_model_list(df_model)})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd097a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43628/7332665.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
      "/tmp/ipykernel_43628/7332665.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  url = str(row[2])\n"
     ]
    }
   ],
   "source": [
    "res = model_selection_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03a8819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelInfo(model='Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥', url='https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit'),\n",
      " ModelInfo(model='Nh·∫≠n di·ªán c·∫£m x√∫c m√®o', url='https://huggingface.co/semihdervis/cat-emotion-classifier'),\n",
      " ModelInfo(model='Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)', url='https://huggingface.co/zhaospei/Model_6')]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b0dc638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_using_tool_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the **Best-fit model** for a given task.\n",
    "                    Given:\n",
    "                    - A user task description\n",
    "                    - A list of available models (names and url)\n",
    "                    - A tool that fetches model details from a provided URL (it help gathers model description, input/output format, code sample usage)\n",
    "\n",
    "                    Your job is to:\n",
    "                    1. Use the **provided tool** to retrieve real details about each candidate model:\n",
    "                        - ‚úÖ Model description\n",
    "                        - üì• Input format\n",
    "                        - üì§ Output format\n",
    "                        - üõ†Ô∏è Library requirements\n",
    "                        - üß™ Code sample (usage code snippet)\n",
    "                    2. Select the best model for user given task based on the data that the tool gives you.\n",
    "\n",
    "                    ‚ö†Ô∏è VERY IMPORTANT RULES\n",
    "\n",
    "                    - ‚ùå DO NOT assume or invent any part of the model's description, input/output format, requirements or code\n",
    "                    - ‚ùå DO NOT generate fake code or use your own knowledge about the model\n",
    "                    - ‚úÖ ONLY use the **actual output** returned from the tool\n",
    "                    - ‚úÖ Include tool content in your final answer exactly as returned (especially code)\n",
    "\n",
    "                    ## üß† Output Structure (Final Answer)\n",
    "\n",
    "                    After retrieving tool results, choose best-fit model for the task (You may need to choose more than one model for some tasks), only choose the model \n",
    "                    that strictly relevant to the task and output the model result in the following format:\n",
    "\n",
    "                    **‚úÖ Model name and link**  \n",
    "                    `<model name>` ‚Äî `<link>`\n",
    "\n",
    "                    **üìù Description (from tool):**  \n",
    "                    <model description>\n",
    "\n",
    "                    **üì• Input format (from tool):**  \n",
    "                    <description of expected input>\n",
    "\n",
    "                    **üì§ Output format (from tool):**  \n",
    "                    <description of model output>\n",
    "\n",
    "                    **üõ†Ô∏è Library Requirements (from tool) **\n",
    "                    <requirements to use the model>\n",
    "\n",
    "                    **üß™ Example code (from tool):**  \n",
    "                    ```python\n",
    "                    <exact code snippet from tool>\n",
    "\n",
    "                    Here are the list of models and there corresponding URL:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"ai\", \"{agent_scratchpad}\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "    task_input = state[\"task\"].content if isinstance(state[\"task\"], SystemMessage) else state[\"task\"]\n",
    "\n",
    "    # result = executor.invoke({\"input\": task_input})\n",
    "    result = executor.invoke({\"input\": task_input, \"model_list\": res.models})\n",
    "\n",
    "    return {\"output\": result[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8b0f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit'}`\n",
      "responded: To solve this task, we need to analyze each of the provided models to see which one(s) can help us achieve the goal of sorting through Alice's pet photos, identifying dogs by breed and cats by emotion, or at least contribute towards this goal. Let's examine the models one by one.\n",
      "\n",
      "### Model 1: Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥\n",
      "- **URL:** https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\n",
      "\n",
      "### Model 2: Nh·∫≠n di·ªán c·∫£m x√∫c m√®o\n",
      "- **URL:** https://huggingface.co/semihdervis/cat-emotion-classifier)\n",
      "\n",
      "### Model 3: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh 100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)\n",
      "- **URL:** https://huggingface.co/zhaospei/Model_6\n",
      "\n",
      "Let's start by fetching details about the first model.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "Model made by notebook first posted in my Kaggle.\n",
      "Model Motivation\n",
      "Recently, someone asked me if you can classify dog images into their respective dog breeds instead just differentiating from cats vs dogs like my last notebook. I say YES!\n",
      "Due to the complexity of the problem, we will be using the most advanced computer vision architecture released in the 2020 Google paper, the Vision Transformer.\n",
      "The difference between the Vision Transformer and the traditional Convolutional Neural Network (CNN) is how it treats an image. In Vision Transformers, we take the input as a patch of the original image, say 16 x 16, and feed in into the Transformer as a sequence with positional embeddings and self-attention, while in the Convolutional Neural Network (CNN), we use the same patch of original image as an input, but use convolutions and pooling layers as inductive biases. What this means is that Vision Transformer can use it's judgement to attend any particular patch of the image in a global fashion using it's self-attention mechanism without having us to guide the neural network like a CNN with local centering/cropping/bounding box our images to help its convolutions. \n",
      "This allows the Vision Transformer architecture to be more flexible and scalable in nature, allowing us to create foundation models in computer vision, similar to the NLP foundational models like BERT and GPT, with pre-training self-supervised/supervised on massive amount of image data that would generalize to different computer vision tasks such as image classification, recognition, segmentation, etc. This cross-pollination helps us move closer towards the goal of Artificial General Intelligence.\n",
      "One thing about Vision Transformers are it has weaker inductive biases compared to Convolutional Neural Networks that enables it's scalability and flexibility. This feature/bug depending on who you ask will require most well-performing pre-trained models to require more data despite having less parameters compared to it's CNN counterparts.\n",
      "Luckily, in this model, we will use a Vision Transformer from Google hosted at HuggingFace pre-trained on the ImageNet-21k dataset (14 million images, 21k classes) with 16x16 patches, 224x224 resolution to bypass that data limitation. We will be fine-tuning this model to our \"small\" dog breeds dataset of around 20 thousand images from the Stanford Dogs dataset imported by Jessica Li into Kaggle to classify dog images into 120 types of dog breeds!\n",
      "Model Description\n",
      "This model is finetuned using the Google Vision Transformer (vit-base-patch16-224-in21k) on the Stanford Dogs dataset in Kaggle to classify dog images into 120 types of dog breeds.\n",
      "Intended Uses & Limitations\n",
      "You can use this finetuned model to classify images of dogs only and dog breeds that are in the dataset.\n",
      "How to Use\n",
      "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
      "import PIL\n",
      "import requests\n",
      "url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\n",
      "image = PIL.Image.open(requests.get(url, stream=True).raw)\n",
      "image_processor = AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "model = AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
      "outputs = model(**inputs)\n",
      "logits = outputs.logits\n",
      "# model predicts one of the 120 Stanford dog breeds classes\n",
      "predicted_class_idx = logits.argmax(-1).item()\n",
      "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
      "Model Training Metrics\n",
      "Epoch\n",
      "Top-1 Accuracy\n",
      "Top-3 Accuracy\n",
      "Top-5 Accuracy\n",
      "Macro F1\n",
      "1\n",
      "79.8%\n",
      "95.1%\n",
      "97.5%\n",
      "77.2%\n",
      "2\n",
      "83.8%\n",
      "96.7%\n",
      "98.2%\n",
      "81.9%\n",
      "3\n",
      "84.8%\n",
      "96.7%\n",
      "98.3%\n",
      "83.4%\n",
      "Model Evaluation Metrics\n",
      "Top-1 Accuracy\n",
      "Top-3 Accuracy\n",
      "Top-5 Accuracy\n",
      "Macro F1\n",
      "84.0%\n",
      "97.1%\n",
      "98.7%\n",
      "83.0%\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/semihdervis/cat-emotion-classifier'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "vit-base-cat-emotions\n",
      "You can try out the model live here, and check out the GitHub repository for more details.\n",
      "This model is a fine-tuned version of google/vit-base-patch16-224-in21k on the custom dataset dataset.\n",
      "It achieves the following results on the evaluation set:\n",
      "Loss: 1.0160\n",
      "Accuracy: 0.6353\n",
      "Model description\n",
      "More information needed\n",
      "Intended uses & limitations\n",
      "More information needed\n",
      "Training and evaluation data\n",
      "More information needed\n",
      "Training procedure\n",
      "Training hyperparameters\n",
      "The following hyperparameters were used during training:\n",
      "learning_rate: 0.0002\n",
      "train_batch_size: 16\n",
      "eval_batch_size: 8\n",
      "seed: 42\n",
      "optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n",
      "lr_scheduler_type: linear\n",
      "num_epochs: 10\n",
      "mixed_precision_training: Native AMP\n",
      "Training results\n",
      "Training Loss\n",
      "Epoch\n",
      "Step\n",
      "Validation Loss\n",
      "Accuracy\n",
      "0.3361\n",
      "3.125\n",
      "100\n",
      "1.0125\n",
      "0.6548\n",
      "0.0723\n",
      "6.25\n",
      "200\n",
      "0.9043\n",
      "0.7381\n",
      "0.0321\n",
      "9.375\n",
      "300\n",
      "0.9268\n",
      "0.7143\n",
      "Framework versions\n",
      "Transformers 4.44.1\n",
      "Pytorch 2.2.2+cu118\n",
      "Datasets 2.20.0\n",
      "Tokenizers 0.19.1\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_6'}`\n",
      "responded: ## Model 1: Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥ ‚Äî https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\n",
      "\n",
      "**Description:** This model is fine-tuned using the Google Vision Transformer (vit-base-patch16-224-in21k) on the Stanford Dogs dataset in Kaggle to classify dog images into 120 types of dog breeds.\n",
      "\n",
      "**Input format:** Images of dogs\n",
      "\n",
      "**Output format:** Predicted dog breed\n",
      "\n",
      "**Library Requirements:** transformers, PIL, requests\n",
      "\n",
      "**Example code:**\n",
      "```python\n",
      "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
      "import PIL\n",
      "import requests\n",
      "\n",
      "url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\n",
      "image = PIL.Image.open(requests.get(url, stream=True).raw)\n",
      "image_processor = AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "model = AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
      "outputs = model(**inputs)\n",
      "logits = outputs.logits\n",
      "predicted_class_idx = logits.argmax(-1).item()\n",
      "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
      "```\n",
      "\n",
      "## Model 2: Nh·∫≠n di·ªán c·∫£m x√∫c m√®o ‚Äî https://huggingface.co/semihdervis/cat-emotion-classifier\n",
      "\n",
      "**Description:** This model is a fine-tuned version of google/vit-base-patch16-224-in21k on a custom dataset for cat emotion classification.\n",
      "\n",
      "**Input format:** Images of cats\n",
      "\n",
      "**Output format:** Predicted emotion (e.g. happy, angry, curious, relaxed)\n",
      "\n",
      "**Library Requirements:** transformers, torch\n",
      "\n",
      "**Example code:**\n",
      "```python\n",
      "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
      "import torch\n",
      "\n",
      "# Load image and model\n",
      "image_processor = AutoImageProcessor.from_pretrained(\"semihdervis/cat-emotion-classifier\")\n",
      "model = AutoModelForImageClassification.from_pretrained(\"semihdervis/cat-emotion-classifier\")\n",
      "\n",
      "# Preprocess image\n",
      "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
      "\n",
      "# Forward pass\n",
      "outputs = model(**inputs)\n",
      "logits = outputs.logits\n",
      "predicted_class_idx = logits.argmax(-1).item()\n",
      "print(\"Predicted emotion:\", model.config.id2label[predicted_class_idx])\n",
      "```\n",
      "\n",
      "## Model 3: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained) ‚Äî https://huggingface.co/zhaospei/Model_6\n",
      "\n",
      "To properly evaluate this model, I need to fetch its details using the provided tool.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "üñºÔ∏è M√¥ h√¨nh Ph√¢n lo·∫°i ·∫¢nh - ViT Fine-tuned tr√™n CIFAR-100\n",
      "üìù M√¥ t·∫£\n",
      "ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh g·ªëc google/vit-base-patch16-224-in21k tr√™n t·∫≠p d·ªØ li·ªáu CIFAR-100. M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o 100 l·ªõp kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.\n",
      "üìå Nhi·ªám v·ª•\n",
      "Lo·∫°i b√†i to√°n: Ph√¢n lo·∫°i ·∫£nh (Image Classification)\n",
      "S·ªë l·ªõp: 100\n",
      "T·∫≠p d·ªØ li·ªáu: CIFAR-100\n",
      "üì• ƒê·∫ßu v√†o\n",
      "ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB\n",
      "K√≠ch th∆∞·ªõc ·∫£nh:  224x224\n",
      "üì§ ƒê·∫ßu ra\n",
      "ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits)\n",
      "Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch th∆∞·ªõc [batch_size, 100]\n",
      "√ù nghƒ©a: X√°c su·∫•t d·ª± ƒëo√°n cho t·ª´ng l·ªõp trong 100 l·ªõp c·ªßa CIFAR-100\n",
      "üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt b·∫±ng:\n",
      "pip install transformers torch torchvision\n",
      "üß™ C√°ch s·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "D∆∞·ªõi ƒë√¢y l√† v√≠ d·ª• v·ªÅ c√°ch s·ª≠ d·ª•ng m√¥ h√¨nh ƒë·ªÉ ph√¢n lo·∫°i m·ªôt ·∫£nh:\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "# T·∫£i ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "# T·∫£i processor v√† m√¥ h√¨nh t·ª´ Hugging Face\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\n",
      "# X·ª≠ l√Ω ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_label = logits.argmax(-1).item()\n",
      "print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m## ‚úÖ Model Selection for Pet Photo Classification\n",
      "\n",
      "### Task Description\n",
      "The task involves sorting through Alice's pet photos, identifying dogs by breed and cats by emotion, or returning \"unsure\" if the image is unclear.\n",
      "\n",
      "### Available Models\n",
      "1. **Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥** ‚Äî [https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit](https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit)\n",
      "2. **Nh·∫≠n di·ªán c·∫£m x√∫c m√®o** ‚Äî [https://huggingface.co/semihdervis/cat-emotion-classifier](https://huggingface.co/semihdervis/cat-emotion-classifier)\n",
      "3. **Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)** ‚Äî [https://huggingface.co/zhaospei/Model_6](https://huggingface.co/zhaospei/Model_6)\n",
      "\n",
      "### Model Details and Selection\n",
      "\n",
      "#### Model 1: Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥\n",
      "- **Description:** This model is fine-tuned using the Google Vision Transformer (vit-base-patch16-224-in21k) on the Stanford Dogs dataset in Kaggle to classify dog images into120 types of dog breeds.\n",
      "- **Input format:** Images of dogs\n",
      "- **Output format:** Predicted dog breed\n",
      "- **Library Requirements:** transformers, PIL, requests\n",
      "- **Example code:**\n",
      "```python\n",
      "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
      "import PIL\n",
      "import requests\n",
      "\n",
      "url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\n",
      "image = PIL.Image.open(requests.get(url, stream=True).raw)\n",
      "image_processor = AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "model = AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
      "outputs = model(**inputs)\n",
      "logits = outputs.logits\n",
      "predicted_class_idx = logits.argmax(-1).item()\n",
      "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
      "```\n",
      "\n",
      "#### Model 2: Nh·∫≠n di·ªán c·∫£m x√∫c m√®o\n",
      "- **Description:** This model is a fine-tuned version of google/vit-base-patch16-224-in21k on a custom dataset for cat emotion classification.\n",
      "- **Input format:** Images of cats\n",
      "- **Output format:** Predicted emotion (e.g. happy, angry, curious, relaxed)\n",
      "- **Library Requirements:** transformers, torch\n",
      "- **Example code:**\n",
      "```python\n",
      "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
      "import torch\n",
      "\n",
      "# Load image and model\n",
      "image_processor = AutoImageProcessor.from_pretrained(\"semihdervis/cat-emotion-classifier\")\n",
      "model = AutoModelForImageClassification.from_pretrained(\"semihdervis/cat-emotion-classifier\")\n",
      "\n",
      "# Preprocess image\n",
      "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
      "\n",
      "# Forward pass\n",
      "outputs = model(**inputs)\n",
      "logits = outputs.logits\n",
      "predicted_class_idx = logits.argmax(-1).item()\n",
      "print(\"Predicted emotion:\", model.config.id2label[predicted_class_idx])\n",
      "```\n",
      "\n",
      "#### Model 3: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)\n",
      "- **Description:** This model is a fine-tuned Vision Transformer (ViT) on the CIFAR-100 dataset for classifying images into100 different classes.\n",
      "- **Input format:** RGB images\n",
      "- **Output format:** Probabilities for each class\n",
      "- **Library Requirements:** transformers, torch, torchvision\n",
      "- **Example code:**\n",
      "```python\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\n",
      "\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_label = logits.argmax(-1).item()\n",
      "print(f\"Predicted label: {model.config.id2label[predicted_label]}\")\n",
      "```\n",
      "\n",
      "### Best-fit Model Selection\n",
      "- For dog breed identification: **Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥** is directly applicable.\n",
      "- For cat emotion recognition: **Nh·∫≠n di·ªán c·∫£m x√∫c m√®o** is directly applicable.\n",
      "- For images that are unclear or do not contain a dog or a cat: The system should return \"unsure\".\n",
      "\n",
      "### Conclusion\n",
      "The best approach seems to be using **Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥** for dog breed identification and **Nh·∫≠n di·ªán c·∫£m x√∫c m√®o** for cat emotion recognition. For images that do not fit these categories or are unclear, the system will return \"unsure\". \n",
      "\n",
      "### Recommended Models\n",
      "1. **Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥** ‚Äî [https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit](https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit)\n",
      "2. **Nh·∫≠n di·ªán c·∫£m x√∫c m√®o** ‚Äî [https://huggingface.co/semihdervis/cat-emotion-classifier](https://huggingface.co/semihdervis/cat-emotion-classifier)\n",
      "\n",
      "These models can be integrated into a larger system that checks for the presence of a dog or a cat in an image and then uses the appropriate model for classification. If neither a dog nor a cat is clearly identified, or if the image is too blurry, the system will classify it as \"unsure\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res2 = model_selection_using_tool_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb681a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('## ‚úÖ Model Selection for Pet Photo Classification\\n'\n",
      " '\\n'\n",
      " '### Task Description\\n'\n",
      " \"The task involves sorting through Alice's pet photos, identifying dogs by \"\n",
      " 'breed and cats by emotion, or returning \"unsure\" if the image is unclear.\\n'\n",
      " '\\n'\n",
      " '### Available Models\\n'\n",
      " '1. **Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥** ‚Äî '\n",
      " '[https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit](https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit)\\n'\n",
      " '2. **Nh·∫≠n di·ªán c·∫£m x√∫c m√®o** ‚Äî '\n",
      " '[https://huggingface.co/semihdervis/cat-emotion-classifier](https://huggingface.co/semihdervis/cat-emotion-classifier)\\n'\n",
      " '3. **Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)** ‚Äî '\n",
      " '[https://huggingface.co/zhaospei/Model_6](https://huggingface.co/zhaospei/Model_6)\\n'\n",
      " '\\n'\n",
      " '### Model Details and Selection\\n'\n",
      " '\\n'\n",
      " '#### Model 1: Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥\\n'\n",
      " '- **Description:** This model is fine-tuned using the Google Vision '\n",
      " 'Transformer (vit-base-patch16-224-in21k) on the Stanford Dogs dataset in '\n",
      " 'Kaggle to classify dog images into120 types of dog breeds.\\n'\n",
      " '- **Input format:** Images of dogs\\n'\n",
      " '- **Output format:** Predicted dog breed\\n'\n",
      " '- **Library Requirements:** transformers, PIL, requests\\n'\n",
      " '- **Example code:**\\n'\n",
      " '```python\\n'\n",
      " 'from transformers import AutoImageProcessor, '\n",
      " 'AutoModelForImageClassification\\n'\n",
      " 'import PIL\\n'\n",
      " 'import requests\\n'\n",
      " '\\n'\n",
      " 'url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\\n'\n",
      " 'image = PIL.Image.open(requests.get(url, stream=True).raw)\\n'\n",
      " 'image_processor = '\n",
      " 'AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\\n'\n",
      " 'model = '\n",
      " 'AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\\n'\n",
      " 'inputs = image_processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'outputs = model(**inputs)\\n'\n",
      " 'logits = outputs.logits\\n'\n",
      " 'predicted_class_idx = logits.argmax(-1).item()\\n'\n",
      " 'print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '#### Model 2: Nh·∫≠n di·ªán c·∫£m x√∫c m√®o\\n'\n",
      " '- **Description:** This model is a fine-tuned version of '\n",
      " 'google/vit-base-patch16-224-in21k on a custom dataset for cat emotion '\n",
      " 'classification.\\n'\n",
      " '- **Input format:** Images of cats\\n'\n",
      " '- **Output format:** Predicted emotion (e.g. happy, angry, curious, '\n",
      " 'relaxed)\\n'\n",
      " '- **Library Requirements:** transformers, torch\\n'\n",
      " '- **Example code:**\\n'\n",
      " '```python\\n'\n",
      " 'from transformers import AutoImageProcessor, '\n",
      " 'AutoModelForImageClassification\\n'\n",
      " 'import torch\\n'\n",
      " '\\n'\n",
      " '# Load image and model\\n'\n",
      " 'image_processor = '\n",
      " 'AutoImageProcessor.from_pretrained(\"semihdervis/cat-emotion-classifier\")\\n'\n",
      " 'model = '\n",
      " 'AutoModelForImageClassification.from_pretrained(\"semihdervis/cat-emotion-classifier\")\\n'\n",
      " '\\n'\n",
      " '# Preprocess image\\n'\n",
      " 'inputs = image_processor(images=image, return_tensors=\"pt\")\\n'\n",
      " '\\n'\n",
      " '# Forward pass\\n'\n",
      " 'outputs = model(**inputs)\\n'\n",
      " 'logits = outputs.logits\\n'\n",
      " 'predicted_class_idx = logits.argmax(-1).item()\\n'\n",
      " 'print(\"Predicted emotion:\", model.config.id2label[predicted_class_idx])\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '#### Model 3: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt '\n",
      " '(fine-grained)\\n'\n",
      " '- **Description:** This model is a fine-tuned Vision Transformer (ViT) on '\n",
      " 'the CIFAR-100 dataset for classifying images into100 different classes.\\n'\n",
      " '- **Input format:** RGB images\\n'\n",
      " '- **Output format:** Probabilities for each class\\n'\n",
      " '- **Library Requirements:** transformers, torch, torchvision\\n'\n",
      " '- **Example code:**\\n'\n",
      " '```python\\n'\n",
      " 'import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " '\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '    outputs = model(**inputs)\\n'\n",
      " '    logits = outputs.logits\\n'\n",
      " '    predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Predicted label: {model.config.id2label[predicted_label]}\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '### Best-fit Model Selection\\n'\n",
      " '- For dog breed identification: **Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥** is directly '\n",
      " 'applicable.\\n'\n",
      " '- For cat emotion recognition: **Nh·∫≠n di·ªán c·∫£m x√∫c m√®o** is directly '\n",
      " 'applicable.\\n'\n",
      " '- For images that are unclear or do not contain a dog or a cat: The system '\n",
      " 'should return \"unsure\".\\n'\n",
      " '\\n'\n",
      " '### Conclusion\\n'\n",
      " 'The best approach seems to be using **Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥** for dog '\n",
      " 'breed identification and **Nh·∫≠n di·ªán c·∫£m x√∫c m√®o** for cat emotion '\n",
      " 'recognition. For images that do not fit these categories or are unclear, the '\n",
      " 'system will return \"unsure\". \\n'\n",
      " '\\n'\n",
      " '### Recommended Models\\n'\n",
      " '1. **Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥** ‚Äî '\n",
      " '[https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit](https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit)\\n'\n",
      " '2. **Nh·∫≠n di·ªán c·∫£m x√∫c m√®o** ‚Äî '\n",
      " '[https://huggingface.co/semihdervis/cat-emotion-classifier](https://huggingface.co/semihdervis/cat-emotion-classifier)\\n'\n",
      " '\\n'\n",
      " 'These models can be integrated into a larger system that checks for the '\n",
      " 'presence of a dog or a cat in an image and then uses the appropriate model '\n",
      " 'for classification. If neither a dog nor a cat is clearly identified, or if '\n",
      " 'the image is too blurry, the system will classify it as \"unsure\".')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res2['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874a1c7",
   "metadata": {},
   "source": [
    "## Output Parser Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3658cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelingOutput(BaseModel):\n",
    "    model_description: str = Field(\n",
    "        ...,\n",
    "        description=\"The detailed description of the model.\"\n",
    "    )\n",
    "    model_input_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed input format\"\n",
    "    )\n",
    "    model_output_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed output format\"\n",
    "    )\n",
    "    model_requirements: str = Field(\n",
    "        ...,\n",
    "        description=\"Requirement libraries need to install to use the model.\"\n",
    "    )\n",
    "    model_sample_code: str = Field(\n",
    "        ...,\n",
    "        description=\"Example code to use the model\"\n",
    "    )\n",
    "\n",
    "class ModelList(BaseModel):\n",
    "    models: List[ModelingOutput] = Field(\n",
    "        ...,\n",
    "        description=\"List of suitable model for given task\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b610e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=ModelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "707428ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser_llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a1f51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_parser_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a smart AI tasked with extracting structured technical details about a machine learning model from a reasoning result.\n",
    "                    You are given a detailed text description about a model (or a list of model).\n",
    "                    Your goal is to fill the following fields based on the text:\n",
    "\n",
    "                    - `model_description`: A detailed explanation of what the model is and what it does.\n",
    "                    - `model_input_format`: A detailed description of the model's input format, including dimensions, data types, and expected preprocessing if mentioned.\n",
    "                    - `model_output_format`: A detailed description of the output format including dimensions, data types, label name and its meaning.\n",
    "                    - `model_requirements`: A detailed description about the requirements needed to be sastified in order to use the model\n",
    "                    - `model_sample_code`: A sample code on how to use the model.\n",
    "\n",
    "                    You MUSTN'T create any data on your own, only using the data provided in the text.\n",
    "\n",
    "                    Return the data as a JSON object matching the following structure:\n",
    "                    {formatted_output}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        ('system', system_prompt),\n",
    "        ('human', \"{input}\")\n",
    "    ]).partial(formatted_output=parser.get_format_instructions())\n",
    "    chain = prompt | llm | parser\n",
    "    response = chain.invoke({\"input\": res2['output']})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d82f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = output_parser_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "583a7bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('## ‚úÖ Model Selection for Pet Photo Classification\\n'\n",
      " '\\n'\n",
      " '### Task Description\\n'\n",
      " \"The task involves sorting through Alice's pet photos, identifying dogs by \"\n",
      " 'breed and cats by emotion, or returning \"unsure\" if the image is unclear.\\n'\n",
      " '\\n'\n",
      " '### Available Models\\n'\n",
      " '1. **Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥** ‚Äî '\n",
      " '[https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit](https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit)\\n'\n",
      " '2. **Nh·∫≠n di·ªán c·∫£m x√∫c m√®o** ‚Äî '\n",
      " '[https://huggingface.co/semihdervis/cat-emotion-classifier](https://huggingface.co/semihdervis/cat-emotion-classifier)\\n'\n",
      " '3. **Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)** ‚Äî '\n",
      " '[https://huggingface.co/zhaospei/Model_6](https://huggingface.co/zhaospei/Model_6)\\n'\n",
      " '\\n'\n",
      " '### Model Details and Selection\\n'\n",
      " '\\n'\n",
      " '#### Model 1: Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥\\n'\n",
      " '- **Description:** This model is fine-tuned using the Google Vision '\n",
      " 'Transformer (vit-base-patch16-224-in21k) on the Stanford Dogs dataset in '\n",
      " 'Kaggle to classify dog images into120 types of dog breeds.\\n'\n",
      " '- **Input format:** Images of dogs\\n'\n",
      " '- **Output format:** Predicted dog breed\\n'\n",
      " '- **Library Requirements:** transformers, PIL, requests\\n'\n",
      " '- **Example code:**\\n'\n",
      " '```python\\n'\n",
      " 'from transformers import AutoImageProcessor, '\n",
      " 'AutoModelForImageClassification\\n'\n",
      " 'import PIL\\n'\n",
      " 'import requests\\n'\n",
      " '\\n'\n",
      " 'url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\\n'\n",
      " 'image = PIL.Image.open(requests.get(url, stream=True).raw)\\n'\n",
      " 'image_processor = '\n",
      " 'AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\\n'\n",
      " 'model = '\n",
      " 'AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\\n'\n",
      " 'inputs = image_processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'outputs = model(**inputs)\\n'\n",
      " 'logits = outputs.logits\\n'\n",
      " 'predicted_class_idx = logits.argmax(-1).item()\\n'\n",
      " 'print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '#### Model 2: Nh·∫≠n di·ªán c·∫£m x√∫c m√®o\\n'\n",
      " '- **Description:** This model is a fine-tuned version of '\n",
      " 'google/vit-base-patch16-224-in21k on a custom dataset for cat emotion '\n",
      " 'classification.\\n'\n",
      " '- **Input format:** Images of cats\\n'\n",
      " '- **Output format:** Predicted emotion (e.g. happy, angry, curious, '\n",
      " 'relaxed)\\n'\n",
      " '- **Library Requirements:** transformers, torch\\n'\n",
      " '- **Example code:**\\n'\n",
      " '```python\\n'\n",
      " 'from transformers import AutoImageProcessor, '\n",
      " 'AutoModelForImageClassification\\n'\n",
      " 'import torch\\n'\n",
      " '\\n'\n",
      " '# Load image and model\\n'\n",
      " 'image_processor = '\n",
      " 'AutoImageProcessor.from_pretrained(\"semihdervis/cat-emotion-classifier\")\\n'\n",
      " 'model = '\n",
      " 'AutoModelForImageClassification.from_pretrained(\"semihdervis/cat-emotion-classifier\")\\n'\n",
      " '\\n'\n",
      " '# Preprocess image\\n'\n",
      " 'inputs = image_processor(images=image, return_tensors=\"pt\")\\n'\n",
      " '\\n'\n",
      " '# Forward pass\\n'\n",
      " 'outputs = model(**inputs)\\n'\n",
      " 'logits = outputs.logits\\n'\n",
      " 'predicted_class_idx = logits.argmax(-1).item()\\n'\n",
      " 'print(\"Predicted emotion:\", model.config.id2label[predicted_class_idx])\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '#### Model 3: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt '\n",
      " '(fine-grained)\\n'\n",
      " '- **Description:** This model is a fine-tuned Vision Transformer (ViT) on '\n",
      " 'the CIFAR-100 dataset for classifying images into100 different classes.\\n'\n",
      " '- **Input format:** RGB images\\n'\n",
      " '- **Output format:** Probabilities for each class\\n'\n",
      " '- **Library Requirements:** transformers, torch, torchvision\\n'\n",
      " '- **Example code:**\\n'\n",
      " '```python\\n'\n",
      " 'import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " '\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '    outputs = model(**inputs)\\n'\n",
      " '    logits = outputs.logits\\n'\n",
      " '    predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Predicted label: {model.config.id2label[predicted_label]}\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '### Best-fit Model Selection\\n'\n",
      " '- For dog breed identification: **Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥** is directly '\n",
      " 'applicable.\\n'\n",
      " '- For cat emotion recognition: **Nh·∫≠n di·ªán c·∫£m x√∫c m√®o** is directly '\n",
      " 'applicable.\\n'\n",
      " '- For images that are unclear or do not contain a dog or a cat: The system '\n",
      " 'should return \"unsure\".\\n'\n",
      " '\\n'\n",
      " '### Conclusion\\n'\n",
      " 'The best approach seems to be using **Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥** for dog '\n",
      " 'breed identification and **Nh·∫≠n di·ªán c·∫£m x√∫c m√®o** for cat emotion '\n",
      " 'recognition. For images that do not fit these categories or are unclear, the '\n",
      " 'system will return \"unsure\". \\n'\n",
      " '\\n'\n",
      " '### Recommended Models\\n'\n",
      " '1. **Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥** ‚Äî '\n",
      " '[https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit](https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit)\\n'\n",
      " '2. **Nh·∫≠n di·ªán c·∫£m x√∫c m√®o** ‚Äî '\n",
      " '[https://huggingface.co/semihdervis/cat-emotion-classifier](https://huggingface.co/semihdervis/cat-emotion-classifier)\\n'\n",
      " '\\n'\n",
      " 'These models can be integrated into a larger system that checks for the '\n",
      " 'presence of a dog or a cat in an image and then uses the appropriate model '\n",
      " 'for classification. If neither a dog nor a cat is clearly identified, or if '\n",
      " 'the image is too blurry, the system will classify it as \"unsure\".')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res2['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c497d868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0:\n",
      "('This model is fine-tuned using the Google Vision Transformer '\n",
      " '(vit-base-patch16-224-in21k) on the Stanford Dogs dataset in Kaggle to '\n",
      " 'classify dog images into 120 types of dog breeds.')\n",
      "'Images of dogs'\n",
      "'Predicted dog breed'\n",
      "'transformers, PIL, requests'\n",
      "('from transformers import AutoImageProcessor, '\n",
      " 'AutoModelForImageClassification\\n'\n",
      " 'import PIL\\n'\n",
      " 'import requests\\n'\n",
      " '\\n'\n",
      " 'url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\\n'\n",
      " 'image = PIL.Image.open(requests.get(url, stream=True).raw)\\n'\n",
      " 'image_processor = '\n",
      " 'AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\\n'\n",
      " 'model = '\n",
      " 'AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\\n'\n",
      " 'inputs = image_processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'outputs = model(**inputs)\\n'\n",
      " 'logits = outputs.logits\\n'\n",
      " 'predicted_class_idx = logits.argmax(-1).item()\\n'\n",
      " 'print(\"Predicted class:\", model.config.id2label[predicted_class_idx])')\n",
      "Model 1:\n",
      "('This model is a fine-tuned version of google/vit-base-patch16-224-in21k on a '\n",
      " 'custom dataset for cat emotion classification.')\n",
      "'Images of cats'\n",
      "'Predicted emotion (e.g. happy, angry, curious, relaxed)'\n",
      "'transformers, torch'\n",
      "('from transformers import AutoImageProcessor, '\n",
      " 'AutoModelForImageClassification\\n'\n",
      " 'import torch\\n'\n",
      " '\\n'\n",
      " '# Load image and model\\n'\n",
      " 'image_processor = '\n",
      " 'AutoImageProcessor.from_pretrained(\"semihdervis/cat-emotion-classifier\")\\n'\n",
      " 'model = '\n",
      " 'AutoModelForImageClassification.from_pretrained(\"semihdervis/cat-emotion-classifier\")\\n'\n",
      " '\\n'\n",
      " '# Preprocess image\\n'\n",
      " 'inputs = image_processor(images=image, return_tensors=\"pt\")\\n'\n",
      " '\\n'\n",
      " '# Forward pass\\n'\n",
      " 'outputs = model(**inputs)\\n'\n",
      " 'logits = outputs.logits\\n'\n",
      " 'predicted_class_idx = logits.argmax(-1).item()\\n'\n",
      " 'print(\"Predicted emotion:\", model.config.id2label[predicted_class_idx])')\n",
      "Model 2:\n",
      "('This model is a fine-tuned Vision Transformer (ViT) on the CIFAR-100 dataset '\n",
      " 'for classifying images into 100 different classes.')\n",
      "'RGB images'\n",
      "'Probabilities for each class'\n",
      "'transformers, torch, torchvision'\n",
      "('import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " '\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " ' outputs = model(**inputs)\\n'\n",
      " ' logits = outputs.logits\\n'\n",
      " ' predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Predicted label: {model.config.id2label[predicted_label]}\")')\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ans.models)):\n",
    "    print(f\"Model {i}:\")\n",
    "    pprint.pprint(ans.models[i].model_description)\n",
    "    pprint.pprint(ans.models[i].model_input_format)\n",
    "    pprint.pprint(ans.models[i].model_output_format)\n",
    "    pprint.pprint(ans.models[i].model_requirements)\n",
    "    pprint.pprint(ans.models[i].model_sample_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
