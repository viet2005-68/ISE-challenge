{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d3e60a",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "535f3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY='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'\n",
    "FIRECRAWL_API_KEY='fc-e36282d62d1c4ee8971ec0649f3862c7'\n",
    "BASE_URL='https://api.groq.com/openai/v1'\n",
    "MODEL=\"meta-llama/llama-4-scout-17b-16e-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42744f82",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91bed7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/mnt/g/Ai agents course/LangGraph/venv/lib/python3.12/site-packages', '..', '.', '..', '.', '..', '.']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "sys.path.append('.')\n",
    "print(sys.path)\n",
    "from tools.fetching_description_from_huggingface import fetching_description_from_huggingface\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import Tool\n",
    "from firecrawl import FirecrawlApp\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36830718",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5827b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task = pd.read_csv(\"data/ISE - AutoCode Challenge 2 Public - Public task.csv\")\n",
    "df_model = pd.read_csv(\"data/ISE - AutoCode Challenge 2 Public - Model zoo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22abc01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STT</th>\n",
       "      <th>Task</th>\n",
       "      <th>Validation data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt ta...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong th·ªùi ƒë·∫°i th√¥ng t...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒê√¢y l√† m·ªôt nhi·ªám v·ª• h·ªì...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒêi·ªÅu quan tr·ªçng l√† c√°c...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong nhi·ªÅu ·ª©ng d·ª•ng t...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>üêæ Story: Alice‚Äôs Magical Pet Photo Box\\r\\nAlic...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STT                                               Task Validation data\n",
       "0    1  \"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt ta...            Link\n",
       "1    2  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong th·ªùi ƒë·∫°i th√¥ng t...            Link\n",
       "2    3  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒê√¢y l√† m·ªôt nhi·ªám v·ª• h·ªì...            Link\n",
       "3    4  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒêi·ªÅu quan tr·ªçng l√† c√°c...            Link\n",
       "4    5  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong nhi·ªÅu ·ª©ng d·ª•ng t...            Link\n",
       "5    6  üêæ Story: Alice‚Äôs Magical Pet Photo Box\\r\\nAlic...            Link"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "697add1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = df_task[\"Task\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "953cae02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'üêæ Story: Alice‚Äôs Magical Pet Photo Box\\r\\nAlice is an animal lover who adores collecting pictures of pets. In her room, she keeps a magical box filled with hundreds of photos of dogs and cats that she has taken or found from all over the internet.\\r\\n\\r\\nBut there‚Äôs a little problem...\\r\\n\\r\\nOver time, Alice‚Äôs collection has gotten messy. Her box now contains a mix of dog photos, cat photos, and even some blurry or unrecognizable images. She needs a smart system to help her sort through the mess and organize everything again.\\r\\n\\r\\nüß† Your Mission\\r\\nYou‚Äôre building an intelligent system that can analyze each image in Alice‚Äôs magical box and decide what it contains. Each image will fall into one of the following categories:\\r\\n\\r\\nüê∂ If the image contains dogs:\\r\\n\\r\\nThere may be one or more dogs in the photo.\\r\\n\\r\\nYour system should detect all visible dogs, identify their breed, and return the most frequent breed in the image.\\r\\n\\r\\nüê± If the image contains a single cat:\\r\\n\\r\\nLook at the cat‚Äôs facial expression and determine its most likely emotion ‚Äî such as \"happy\", \"angry\", \"curious\", or \"relaxed\".\\r\\n\\r\\n‚ùì If the image is unclear (e.g., no visible pet or the view is too blurry):\\r\\n\\r\\nThe system should simply return \"unsure\" to avoid making a wrong guess.\\r\\n\\r\\nüì¶ The Contents of Alice‚Äôs Box\\r\\nThe images are stored in the following directory:\\r\\n\\r\\nimages/\\r\\n\\r\\nEach image is in .jpg or .png format, typically containing a pet in the foreground.\\r\\n\\r\\nüìù Expected Output\\r\\nAfter processing, your system should save the result to file output.csv with format:\\r\\n\\r\\nid: the image filename without extension (e.g., 001.png ‚Üí id = 001)\\r\\n\\r\\nprediction:\\r\\n\\r\\n    The most frequent dog breed (if dogs are present)\\r\\n\\r\\n    The cat‚Äôs emotion (if a single cat is present)\\r\\n\\r\\n    \"unsure\" (if the content is ambiguous or invalid)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6909b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_list(df):\n",
    "    model_strings = []\n",
    "    for i, row in df.iterrows():\n",
    "        desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        url = str(row[2])\n",
    "        model_strings.append(f\"{i + 1} - {desc} [More info]({url})\")\n",
    "    return \"\\n\".join(model_strings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c40886",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af3bcff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraper(url: str) -> str:\n",
    "    \"\"\"Get more details about the model using the model url\"\"\"\n",
    "    # scraper = FirecrawlApp(api_key=FIRECRAWL_API_KEY)\n",
    "    # scrape_status = scraper.scrape_url(\n",
    "    #     url,\n",
    "    #     formats=['markdown']\n",
    "    # )\n",
    "    # return scrape_status.markdown\n",
    "    result = fetching_description_from_huggingface(url)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77f3a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraperParameters(BaseModel):\n",
    "    url: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6a0e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = web_scraper('https://huggingface.co/thanhtlx/image_classification_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52a9c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"web_scrapper\",\n",
    "        func=web_scraper,\n",
    "        description=\"Use to get more detail about the model\",\n",
    "        args_schema=WebScraperParameters\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c409f8b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eca82430",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI( \n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6511a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tasks(BaseModel):\n",
    "    subtask_one: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 1\"\n",
    "    )\n",
    "    subtask_two: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 2\"\n",
    "    )\n",
    "    subtask_three: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 3\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0de6f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: HumanMessage | None\n",
    "    data: str | None\n",
    "    messages: Annotated[BaseMessage, add_messages]\n",
    "    subtasks: Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef593964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_agent(state: AgentState) -> AgentState:\n",
    "    structured_llm = llm.with_structured_output(Tasks)\n",
    "    system_prompt = \"\"\"\n",
    "                    You are an expert in task formulation. Given a machine learning task, you will\n",
    "                    help divide the task into these subtasks:\n",
    "                    - Subtask1: for classifying the problem type and task description\n",
    "                    - Subtask2: for identifying the input of the problem\n",
    "                    - Subtask3: for identifying the output of the problem\n",
    "                    describe the each subtask as specifically as possible\n",
    "                    \"\"\"\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    messages.append(state['task'])\n",
    "    result = structured_llm.invoke(messages)\n",
    "    state['subtasks'] = result\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14b15c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"task\": HumanMessage(content=task),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78675acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = classify_agent(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab3d7f",
   "metadata": {},
   "source": [
    "# Modeling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f656ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInfo(BaseModel):\n",
    "    model: str = Field(..., description=\"Model name\")\n",
    "    url: str = Field(..., description=\"URL to the model\")\n",
    "\n",
    "class ModelSelection(BaseModel):\n",
    "    models: List[ModelInfo] = Field(..., description=\"List of available models with name and URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18dcc46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_parser = PydanticOutputParser(pydantic_object=ModelSelection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de4ed792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the suitable model for a given task.\n",
    "                    Given:\n",
    "                    - A user description about the task\n",
    "                    - A list of available model (names and links)\n",
    "                    Your job is to choose ALL suitable models provided in the model list for user's specific tasks.\n",
    "                    Return the answer in the format\n",
    "                    {structured_output}\n",
    "                    Here are the list of model:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]).partial(structured_output=model_selection_parser.get_format_instructions())\n",
    "\n",
    "    chain = prompt | llm | model_selection_parser\n",
    "\n",
    "    result = chain.invoke({\"input\": task, \"model_list\": format_model_list(df_model)})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd097a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10912/7332665.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
      "/tmp/ipykernel_10912/7332665.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  url = str(row[2])\n"
     ]
    }
   ],
   "source": [
    "res = model_selection_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03a8819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelInfo(model='Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥', url='https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit'),\n",
      " ModelInfo(model='Nh·∫≠n di·ªán c·∫£m x√∫c m√®o', url='https://huggingface.co/semihdervis/cat-emotion-classifier'),\n",
      " ModelInfo(model='Ph√¢n lo·∫°i h√¨nh ·∫£nh', url='https://huggingface.co/zhaospei/Model_7')]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b0dc638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_using_tool_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the **Best-fit model** for a given task.\n",
    "                    Given:\n",
    "                    - A user task description\n",
    "                    - A list of available models (names and url)\n",
    "                    - A tool that fetches model details from a provided URL (it help gathers model description, input/output format, code sample usage)\n",
    "\n",
    "                    Your job is to:\n",
    "                    1. Use the **provided tool** to retrieve real details about each candidate model:\n",
    "                        - ‚úÖ Model description\n",
    "                        - üì• Input format\n",
    "                        - üì§ Output format\n",
    "                        - üõ†Ô∏è Library requirements\n",
    "                        - üß™ Code sample (usage code snippet)\n",
    "                    2. Select the best model for user given task based on the data that the tool gives you.\n",
    "\n",
    "                    ‚ö†Ô∏è VERY IMPORTANT RULES\n",
    "\n",
    "                    - ‚ùå DO NOT assume or invent any part of the model's description, input/output format, requirements or code\n",
    "                    - ‚ùå DO NOT generate fake code or use your own knowledge about the model\n",
    "                    - ‚úÖ ONLY use the **actual output** returned from the tool\n",
    "                    - ‚úÖ Include tool content in your final answer exactly as returned (especially code)\n",
    "\n",
    "                    ## üß† Output Structure (Final Answer)\n",
    "\n",
    "                    After retrieving tool results, choose best-fit model for the task (You may need to choose more than one model for some tasks), only choose the model \n",
    "                    that strictly relevant to the task and output the model result in the following format:\n",
    "\n",
    "                    **‚úÖ Model name and link**  \n",
    "                    `<model name>` ‚Äî `<link>`\n",
    "\n",
    "                    **üìù Description (from tool):**  \n",
    "                    <model description>\n",
    "\n",
    "                    **üì• Input format (from tool):**  \n",
    "                    <description of expected input>\n",
    "\n",
    "                    **üì§ Output format (from tool):**  \n",
    "                    <description of model output>\n",
    "\n",
    "                    **üõ†Ô∏è Library Requirements (from tool) **\n",
    "                    <requirements to use the model>\n",
    "\n",
    "                    **üß™ Example code (from tool):**  \n",
    "                    ```python\n",
    "                    <exact code snippet from tool>\n",
    "\n",
    "                    Here are the list of models and there corresponding URL:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"ai\", \"{agent_scratchpad}\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "    task_input = state[\"task\"].content if isinstance(state[\"task\"], SystemMessage) else state[\"task\"]\n",
    "\n",
    "    # result = executor.invoke({\"input\": task_input})\n",
    "    result = executor.invoke({\"input\": task_input, \"model_list\": res.models})\n",
    "\n",
    "    return {\"output\": result[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8b0f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit'}`\n",
      "responded: To solve this task, I need to analyze the requirements and select the best-fit model for image classification and emotion detection. \n",
      "\n",
      "The task requires two main functionalities: \n",
      "1. Detecting dogs in an image, identifying their breed, and returning the most frequent breed.\n",
      "2. Detecting cats in an image and determining their emotion.\n",
      "\n",
      "Given the models:\n",
      "- 'Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥' (https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit)\n",
      "- 'Nh·∫≠n di·ªán c·∫£m x√∫c m√®o' (https://huggingface.co/semihdervis/cat-emotion-classifier)\n",
      "- 'Ph√¢n lo·∫°i h√¨nh ·∫£nh' (https://huggingface.co/zhaospei/Model_7)\n",
      "\n",
      "I will first gather detailed information about the 'Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥' model.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "Model made by notebook first posted in my Kaggle.\n",
      "Model Motivation\n",
      "Recently, someone asked me if you can classify dog images into their respective dog breeds instead just differentiating from cats vs dogs like my last notebook. I say YES!\n",
      "Due to the complexity of the problem, we will be using the most advanced computer vision architecture released in the 2020 Google paper, the Vision Transformer.\n",
      "The difference between the Vision Transformer and the traditional Convolutional Neural Network (CNN) is how it treats an image. In Vision Transformers, we take the input as a patch of the original image, say 16 x 16, and feed in into the Transformer as a sequence with positional embeddings and self-attention, while in the Convolutional Neural Network (CNN), we use the same patch of original image as an input, but use convolutions and pooling layers as inductive biases. What this means is that Vision Transformer can use it's judgement to attend any particular patch of the image in a global fashion using it's self-attention mechanism without having us to guide the neural network like a CNN with local centering/cropping/bounding box our images to help its convolutions. \n",
      "This allows the Vision Transformer architecture to be more flexible and scalable in nature, allowing us to create foundation models in computer vision, similar to the NLP foundational models like BERT and GPT, with pre-training self-supervised/supervised on massive amount of image data that would generalize to different computer vision tasks such as image classification, recognition, segmentation, etc. This cross-pollination helps us move closer towards the goal of Artificial General Intelligence.\n",
      "One thing about Vision Transformers are it has weaker inductive biases compared to Convolutional Neural Networks that enables it's scalability and flexibility. This feature/bug depending on who you ask will require most well-performing pre-trained models to require more data despite having less parameters compared to it's CNN counterparts.\n",
      "Luckily, in this model, we will use a Vision Transformer from Google hosted at HuggingFace pre-trained on the ImageNet-21k dataset (14 million images, 21k classes) with 16x16 patches, 224x224 resolution to bypass that data limitation. We will be fine-tuning this model to our \"small\" dog breeds dataset of around 20 thousand images from the Stanford Dogs dataset imported by Jessica Li into Kaggle to classify dog images into 120 types of dog breeds!\n",
      "Model Description\n",
      "This model is finetuned using the Google Vision Transformer (vit-base-patch16-224-in21k) on the Stanford Dogs dataset in Kaggle to classify dog images into 120 types of dog breeds.\n",
      "Intended Uses & Limitations\n",
      "You can use this finetuned model to classify images of dogs only and dog breeds that are in the dataset.\n",
      "How to Use\n",
      "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
      "import PIL\n",
      "import requests\n",
      "url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\n",
      "image = PIL.Image.open(requests.get(url, stream=True).raw)\n",
      "image_processor = AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "model = AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
      "outputs = model(**inputs)\n",
      "logits = outputs.logits\n",
      "# model predicts one of the 120 Stanford dog breeds classes\n",
      "predicted_class_idx = logits.argmax(-1).item()\n",
      "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
      "Model Training Metrics\n",
      "Epoch\n",
      "Top-1 Accuracy\n",
      "Top-3 Accuracy\n",
      "Top-5 Accuracy\n",
      "Macro F1\n",
      "1\n",
      "79.8%\n",
      "95.1%\n",
      "97.5%\n",
      "77.2%\n",
      "2\n",
      "83.8%\n",
      "96.7%\n",
      "98.2%\n",
      "81.9%\n",
      "3\n",
      "84.8%\n",
      "96.7%\n",
      "98.3%\n",
      "83.4%\n",
      "Model Evaluation Metrics\n",
      "Top-1 Accuracy\n",
      "Top-3 Accuracy\n",
      "Top-5 Accuracy\n",
      "Macro F1\n",
      "84.0%\n",
      "97.1%\n",
      "98.7%\n",
      "83.0%\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/semihdervis/cat-emotion-classifier'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "vit-base-cat-emotions\n",
      "You can try out the model live here, and check out the GitHub repository for more details.\n",
      "This model is a fine-tuned version of google/vit-base-patch16-224-in21k on the custom dataset dataset.\n",
      "It achieves the following results on the evaluation set:\n",
      "Loss: 1.0160\n",
      "Accuracy: 0.6353\n",
      "Model description\n",
      "More information needed\n",
      "Intended uses & limitations\n",
      "More information needed\n",
      "Training and evaluation data\n",
      "More information needed\n",
      "Training procedure\n",
      "Training hyperparameters\n",
      "The following hyperparameters were used during training:\n",
      "learning_rate: 0.0002\n",
      "train_batch_size: 16\n",
      "eval_batch_size: 8\n",
      "seed: 42\n",
      "optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n",
      "lr_scheduler_type: linear\n",
      "num_epochs: 10\n",
      "mixed_precision_training: Native AMP\n",
      "Training results\n",
      "Training Loss\n",
      "Epoch\n",
      "Step\n",
      "Validation Loss\n",
      "Accuracy\n",
      "0.3361\n",
      "3.125\n",
      "100\n",
      "1.0125\n",
      "0.6548\n",
      "0.0723\n",
      "6.25\n",
      "200\n",
      "0.9043\n",
      "0.7381\n",
      "0.0321\n",
      "9.375\n",
      "300\n",
      "0.9268\n",
      "0.7143\n",
      "Framework versions\n",
      "Transformers 4.44.1\n",
      "Pytorch 2.2.2+cu118\n",
      "Datasets 2.20.0\n",
      "Tokenizers 0.19.1\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_7'}`\n",
      "responded: ## Model 1: Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥ ‚Äî https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\n",
      "\n",
      "**üìù Description (from tool):**  \n",
      "This model is finetuned using the Google Vision Transformer (vit-base-patch16-224-in21k) on the Stanford Dogs dataset in Kaggle to classify dog images into 120 types of dog breeds.\n",
      "\n",
      "**üì• Input format (from tool):**  \n",
      "The model expects an image input.\n",
      "\n",
      "**üì§ Output format (from tool):**  \n",
      "The model predicts one of the 120 Stanford dog breeds classes.\n",
      "\n",
      "**üõ†Ô∏è Library Requirements (from tool):**  \n",
      "- Transformers\n",
      "- Pytorch\n",
      "- PIL\n",
      "\n",
      "**üß™ Example code (from tool):**  \n",
      "```python\n",
      "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
      "import PIL\n",
      "import requests\n",
      "\n",
      "url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\n",
      "image = PIL.Image.open(requests.get(url, stream=True).raw)\n",
      "image_processor = AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "model = AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "\n",
      "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
      "outputs = model(**inputs)\n",
      "logits = outputs.logits\n",
      "predicted_class_idx = logits.argmax(-1).item()\n",
      "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
      "```\n",
      "\n",
      "## Model 2: Nh·∫≠n di·ªán c·∫£m x√∫c m√®o ‚Äî https://huggingface.co/semihdervis/cat-emotion-classifier\n",
      "\n",
      "**üìù Description (from tool):**  \n",
      "This model is a fine-tuned version of google/vit-base-patch16-224-in21k on a custom dataset.\n",
      "\n",
      "**üì• Input format (from tool):**  \n",
      "The model expects an image input.\n",
      "\n",
      "**üì§ Output format (from tool):**  \n",
      "The model predicts the emotion of a cat.\n",
      "\n",
      "**üõ†Ô∏è Library Requirements (from tool):**  \n",
      "- Transformers\n",
      "- Pytorch\n",
      "- PIL\n",
      "\n",
      "**üß™ Example code (from tool):**  \n",
      "No code snippet provided.\n",
      "\n",
      "## Model 3: Ph√¢n lo·∫°i h√¨nh ·∫£nh ‚Äî https://huggingface.co/zhaospei/Model_7\n",
      "\n",
      "I will now gather detailed information about this model.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "üñºÔ∏è M√¥ h√¨nh Ph√¢n lo·∫°i ·∫¢nh - ViT Fine-tuned tr√™n CIFAR-10\n",
      "üìù M√¥ t·∫£\n",
      "ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh kh√°c tr√™n t·∫≠p d·ªØ li·ªáu CIFAR-10. M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o 10 l·ªõp kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.\n",
      "üìå Nhi·ªám v·ª•\n",
      "Lo·∫°i b√†i to√°n: Ph√¢n lo·∫°i ·∫£nh (Image Classification)\n",
      "S·ªë l·ªõp: 10 (T∆∞∆°ng ·ª©ng nh√£n) CIFAR-10\n",
      "üì• ƒê·∫ßu v√†o\n",
      "ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB\n",
      "K√≠ch th∆∞·ªõc ·∫£nh: 224x224 pixels\n",
      "üì§ ƒê·∫ßu ra\n",
      "ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits)\n",
      "Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch th∆∞·ªõc [batch_size, 10]\n",
      "√ù nghƒ©a: X√°c su·∫•t d·ª± ƒëo√°n cho t·ª´ng l·ªõp trong 10 l·ªõp c·ªßa CIFAR-10\n",
      "üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt b·∫±ng:\n",
      "pip install transformers torch torchvision\n",
      "üß™ C√°ch s·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "D∆∞·ªõi ƒë√¢y l√† v√≠ d·ª• v·ªÅ c√°ch s·ª≠ d·ª•ng m√¥ h√¨nh ƒë·ªÉ ph√¢n lo·∫°i m·ªôt ·∫£nh:\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "# T·∫£i ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "# T·∫£i processor v√† m√¥ h√¨nh t·ª´ Hugging Face\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_7\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_7\")\n",
      "# X·ª≠ l√Ω ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_label = logits.argmax(-1).item()\n",
      "print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m## ‚úÖ Model1: Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥ ‚Äî <https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit>\n",
      "\n",
      "**üìù Description (from tool):**  \n",
      "This model is finetuned using the Google Vision Transformer (vit-base-patch16-224-in21k) on the Stanford Dogs dataset in Kaggle to classify dog images into120 types of dog breeds.\n",
      "\n",
      "**üì• Input format (from tool):**  \n",
      "The model expects an image input.\n",
      "\n",
      "**üì§ Output format (from tool):**  \n",
      "The model predicts one of the120 Stanford dog breeds classes.\n",
      "\n",
      "**üõ†Ô∏è Library Requirements (from tool) **  \n",
      "- Transformers\n",
      "- Pytorch\n",
      "- PIL\n",
      "\n",
      "**üß™ Example code (from tool):**  \n",
      "```python\n",
      "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
      "import PIL\n",
      "import requests\n",
      "\n",
      "url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\n",
      "image = PIL.Image.open(requests.get(url, stream=True).raw)\n",
      "image_processor = AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "model = AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "\n",
      "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
      "outputs = model(**inputs)\n",
      "logits = outputs.logits\n",
      "predicted_class_idx = logits.argmax(-1).item()\n",
      "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
      "```\n",
      "\n",
      "## ‚úÖ Model2: Nh·∫≠n di·ªán c·∫£m x√∫c m√®o ‚Äî <https://huggingface.co/semihdervis/cat-emotion-classifier>\n",
      "\n",
      "**üìù Description (from tool):**  \n",
      "This model is a fine-tuned version of google/vit-base-patch16-224-in21k on a custom dataset.\n",
      "\n",
      "**üì• Input format (from tool):**  \n",
      "The model expects an image input.\n",
      "\n",
      "**üì§ Output format (from tool):**  \n",
      "The model predicts the emotion of a cat.\n",
      "\n",
      "**üõ†Ô∏è Library Requirements (from tool) **  \n",
      "- Transformers\n",
      "- Pytorch\n",
      "- PIL\n",
      "\n",
      "**üß™ Example code (from tool):**  \n",
      "No code snippet provided.\n",
      "\n",
      "## ‚úÖ Model3: Ph√¢n lo·∫°i h√¨nh ·∫£nh ‚Äî <https://huggingface.co/zhaospei/Model_7>\n",
      "\n",
      "**üìù Description (from tool):**  \n",
      "ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh kh√°c tr√™n t·∫≠p d·ªØ li·ªáu CIFAR-10. M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o10 l·ªõp kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.\n",
      "\n",
      "**üì• Input format (from tool):**  \n",
      "ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB\n",
      "K√≠ch th∆∞·ªõc ·∫£nh:224x224 pixels\n",
      "\n",
      "**üì§ Output format (from tool):**  \n",
      "ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits)\n",
      "Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch th∆∞·ªõc [batch_size,10]\n",
      "√ù nghƒ©a: X√°c su·∫•t d·ª± ƒëo√°n cho t·ª´ng l·ªõp trong10 l·ªõp c·ªßa CIFAR-10\n",
      "\n",
      "**üõ†Ô∏è Library Requirements (from tool) **  \n",
      "- transformers\n",
      "- torch\n",
      "- torchvision\n",
      "\n",
      "**üß™ Example code (from tool):**  \n",
      "```python\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_7\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_7\")\n",
      "\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_label = logits.argmax(-1).item()\n",
      "print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\n",
      "```\n",
      "\n",
      "## üìù Selecting the Best-Fit Models\n",
      "\n",
      "Based on the task description, we need two models:\n",
      "1. A model to detect dogs and identify their breed.\n",
      "2. A model to detect cats and determine their emotion.\n",
      "\n",
      "### Model for Dog Breed Classification\n",
      "- **Model:** Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥ ‚Äî <https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit>\n",
      "- **Reason:** This model is specifically fine-tuned for classifying dog images into 120 types of dog breeds, which aligns with the task requirement of identifying dog breeds.\n",
      "\n",
      "### Model for Cat Emotion Classification\n",
      "- **Model:** Nh·∫≠n di·ªán c·∫£m x√∫c m√®o ‚Äî <https://huggingface.co/semihdervis/cat-emotion-classifier>\n",
      "- **Reason:** This model is designed to classify the emotion of cats, which matches the task requirement of determining a cat's emotion.\n",
      "\n",
      "### Model for Image Classification (if needed)\n",
      "- **Model:** Ph√¢n lo·∫°i h√¨nh ·∫£nh ‚Äî <https://huggingface.co/zhaospei/Model_7>\n",
      "- **Reason:** This model can be used for general image classification tasks, but it is not specifically tailored for dog breed or cat emotion classification. It can be used for unsure or unclear images.\n",
      "\n",
      "## üìù Conclusion\n",
      "\n",
      "The best-fit models for Alice's Magical Pet Photo Box are:\n",
      "1. **Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥** ‚Äî <https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit> for dog breed classification.\n",
      "2. **Nh·∫≠n di·ªán c·∫£m x√∫c m√®o** ‚Äî <https://huggingface.co/semihdervis/cat-emotion-classifier> for cat emotion classification.\n",
      "\n",
      "These two models can effectively handle the requirements of sorting and organizing Alice's pet photos.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res2 = model_selection_using_tool_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cb681a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('## ‚úÖ Model1: Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥ ‚Äî '\n",
      " '<https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit>\\n'\n",
      " '\\n'\n",
      " '**üìù Description (from tool):**  \\n'\n",
      " 'This model is finetuned using the Google Vision Transformer '\n",
      " '(vit-base-patch16-224-in21k) on the Stanford Dogs dataset in Kaggle to '\n",
      " 'classify dog images into120 types of dog breeds.\\n'\n",
      " '\\n'\n",
      " '**üì• Input format (from tool):**  \\n'\n",
      " 'The model expects an image input.\\n'\n",
      " '\\n'\n",
      " '**üì§ Output format (from tool):**  \\n'\n",
      " 'The model predicts one of the120 Stanford dog breeds classes.\\n'\n",
      " '\\n'\n",
      " '**üõ†Ô∏è Library Requirements (from tool) **  \\n'\n",
      " '- Transformers\\n'\n",
      " '- Pytorch\\n'\n",
      " '- PIL\\n'\n",
      " '\\n'\n",
      " '**üß™ Example code (from tool):**  \\n'\n",
      " '```python\\n'\n",
      " 'from transformers import AutoImageProcessor, '\n",
      " 'AutoModelForImageClassification\\n'\n",
      " 'import PIL\\n'\n",
      " 'import requests\\n'\n",
      " '\\n'\n",
      " 'url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\\n'\n",
      " 'image = PIL.Image.open(requests.get(url, stream=True).raw)\\n'\n",
      " 'image_processor = '\n",
      " 'AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\\n'\n",
      " 'model = '\n",
      " 'AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\\n'\n",
      " '\\n'\n",
      " 'inputs = image_processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'outputs = model(**inputs)\\n'\n",
      " 'logits = outputs.logits\\n'\n",
      " 'predicted_class_idx = logits.argmax(-1).item()\\n'\n",
      " 'print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '## ‚úÖ Model2: Nh·∫≠n di·ªán c·∫£m x√∫c m√®o ‚Äî '\n",
      " '<https://huggingface.co/semihdervis/cat-emotion-classifier>\\n'\n",
      " '\\n'\n",
      " '**üìù Description (from tool):**  \\n'\n",
      " 'This model is a fine-tuned version of google/vit-base-patch16-224-in21k on a '\n",
      " 'custom dataset.\\n'\n",
      " '\\n'\n",
      " '**üì• Input format (from tool):**  \\n'\n",
      " 'The model expects an image input.\\n'\n",
      " '\\n'\n",
      " '**üì§ Output format (from tool):**  \\n'\n",
      " 'The model predicts the emotion of a cat.\\n'\n",
      " '\\n'\n",
      " '**üõ†Ô∏è Library Requirements (from tool) **  \\n'\n",
      " '- Transformers\\n'\n",
      " '- Pytorch\\n'\n",
      " '- PIL\\n'\n",
      " '\\n'\n",
      " '**üß™ Example code (from tool):**  \\n'\n",
      " 'No code snippet provided.\\n'\n",
      " '\\n'\n",
      " '## ‚úÖ Model3: Ph√¢n lo·∫°i h√¨nh ·∫£nh ‚Äî <https://huggingface.co/zhaospei/Model_7>\\n'\n",
      " '\\n'\n",
      " '**üìù Description (from tool):**  \\n'\n",
      " 'ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh kh√°c tr√™n '\n",
      " 't·∫≠p d·ªØ li·ªáu CIFAR-10. M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o10 l·ªõp '\n",
      " 'kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.\\n'\n",
      " '\\n'\n",
      " '**üì• Input format (from tool):**  \\n'\n",
      " 'ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB\\n'\n",
      " 'K√≠ch th∆∞·ªõc ·∫£nh:224x224 pixels\\n'\n",
      " '\\n'\n",
      " '**üì§ Output format (from tool):**  \\n'\n",
      " 'ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits)\\n'\n",
      " 'Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch th∆∞·ªõc [batch_size,10]\\n'\n",
      " '√ù nghƒ©a: X√°c su·∫•t d·ª± ƒëo√°n cho t·ª´ng l·ªõp trong10 l·ªõp c·ªßa CIFAR-10\\n'\n",
      " '\\n'\n",
      " '**üõ†Ô∏è Library Requirements (from tool) **  \\n'\n",
      " '- transformers\\n'\n",
      " '- torch\\n'\n",
      " '- torchvision\\n'\n",
      " '\\n'\n",
      " '**üß™ Example code (from tool):**  \\n'\n",
      " '```python\\n'\n",
      " 'import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_7\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_7\")\\n'\n",
      " '\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '    outputs = model(**inputs)\\n'\n",
      " '    logits = outputs.logits\\n'\n",
      " '    predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '## üìù Selecting the Best-Fit Models\\n'\n",
      " '\\n'\n",
      " 'Based on the task description, we need two models:\\n'\n",
      " '1. A model to detect dogs and identify their breed.\\n'\n",
      " '2. A model to detect cats and determine their emotion.\\n'\n",
      " '\\n'\n",
      " '### Model for Dog Breed Classification\\n'\n",
      " '- **Model:** Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥ ‚Äî '\n",
      " '<https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit>\\n'\n",
      " '- **Reason:** This model is specifically fine-tuned for classifying dog '\n",
      " 'images into 120 types of dog breeds, which aligns with the task requirement '\n",
      " 'of identifying dog breeds.\\n'\n",
      " '\\n'\n",
      " '### Model for Cat Emotion Classification\\n'\n",
      " '- **Model:** Nh·∫≠n di·ªán c·∫£m x√∫c m√®o ‚Äî '\n",
      " '<https://huggingface.co/semihdervis/cat-emotion-classifier>\\n'\n",
      " '- **Reason:** This model is designed to classify the emotion of cats, which '\n",
      " \"matches the task requirement of determining a cat's emotion.\\n\"\n",
      " '\\n'\n",
      " '### Model for Image Classification (if needed)\\n'\n",
      " '- **Model:** Ph√¢n lo·∫°i h√¨nh ·∫£nh ‚Äî <https://huggingface.co/zhaospei/Model_7>\\n'\n",
      " '- **Reason:** This model can be used for general image classification tasks, '\n",
      " 'but it is not specifically tailored for dog breed or cat emotion '\n",
      " 'classification. It can be used for unsure or unclear images.\\n'\n",
      " '\\n'\n",
      " '## üìù Conclusion\\n'\n",
      " '\\n'\n",
      " \"The best-fit models for Alice's Magical Pet Photo Box are:\\n\"\n",
      " '1. **Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥** ‚Äî '\n",
      " '<https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit> '\n",
      " 'for dog breed classification.\\n'\n",
      " '2. **Nh·∫≠n di·ªán c·∫£m x√∫c m√®o** ‚Äî '\n",
      " '<https://huggingface.co/semihdervis/cat-emotion-classifier> for cat emotion '\n",
      " 'classification.\\n'\n",
      " '\\n'\n",
      " 'These two models can effectively handle the requirements of sorting and '\n",
      " \"organizing Alice's pet photos.\")\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res2['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874a1c7",
   "metadata": {},
   "source": [
    "## Output Parser Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3658cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelingOutput(BaseModel):\n",
    "    model_description: str = Field(\n",
    "        ...,\n",
    "        description=\"The detailed description of the model.\"\n",
    "    )\n",
    "    model_input_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed input format\"\n",
    "    )\n",
    "    model_output_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed output format\"\n",
    "    )\n",
    "    model_requirements: str = Field(\n",
    "        ...,\n",
    "        description=\"Requirement libraries need to install to use the model.\"\n",
    "    )\n",
    "    model_sample_code: str = Field(\n",
    "        ...,\n",
    "        description=\"Example code to use the model\"\n",
    "    )\n",
    "\n",
    "class ModelList(BaseModel):\n",
    "    models: List[ModelingOutput] = Field(\n",
    "        ...,\n",
    "        description=\"List of suitable model for given task\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b610e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=ModelList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "707428ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser_llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a1f51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_parser_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a smart AI tasked with extracting structured technical details about a machine learning model from a reasoning result.\n",
    "                    You are given a detailed text description about a model (or a list of model).\n",
    "                    Your goal is to fill the following fields based on the text:\n",
    "\n",
    "                    - `model_description`: A detailed explanation of what the model is and what it does.\n",
    "                    - `model_input_format`: A detailed description of the model's input format, including dimensions, data types, and expected preprocessing if mentioned.\n",
    "                    - `model_output_format`: A detailed description of the output format including dimensions, data types, label name and its meaning.\n",
    "                    - `model_requirements`: A detailed description about the requirements needed to be sastified in order to use the model\n",
    "                    - `model_sample_code`: A sample code on how to use the model.\n",
    "\n",
    "                    You MUSTN'T create any data on your own, only using the data provided in the text.\n",
    "\n",
    "                    Return the data as a JSON object matching the following structure:\n",
    "                    {formatted_output}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        ('system', system_prompt),\n",
    "        ('human', \"{input}\")\n",
    "    ]).partial(formatted_output=parser.get_format_instructions())\n",
    "    chain = prompt | llm | parser\n",
    "    response = chain.invoke({\"input\": res2['output']})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d82f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = output_parser_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "583a7bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('## ‚úÖ Model1: Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥ ‚Äî '\n",
      " '<https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit>\\n'\n",
      " '\\n'\n",
      " '**üìù Description (from tool):**  \\n'\n",
      " 'This model is finetuned using the Google Vision Transformer '\n",
      " '(vit-base-patch16-224-in21k) on the Stanford Dogs dataset in Kaggle to '\n",
      " 'classify dog images into120 types of dog breeds.\\n'\n",
      " '\\n'\n",
      " '**üì• Input format (from tool):**  \\n'\n",
      " 'The model expects an image input.\\n'\n",
      " '\\n'\n",
      " '**üì§ Output format (from tool):**  \\n'\n",
      " 'The model predicts one of the120 Stanford dog breeds classes.\\n'\n",
      " '\\n'\n",
      " '**üõ†Ô∏è Library Requirements (from tool) **  \\n'\n",
      " '- Transformers\\n'\n",
      " '- Pytorch\\n'\n",
      " '- PIL\\n'\n",
      " '\\n'\n",
      " '**üß™ Example code (from tool):**  \\n'\n",
      " '```python\\n'\n",
      " 'from transformers import AutoImageProcessor, '\n",
      " 'AutoModelForImageClassification\\n'\n",
      " 'import PIL\\n'\n",
      " 'import requests\\n'\n",
      " '\\n'\n",
      " 'url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\\n'\n",
      " 'image = PIL.Image.open(requests.get(url, stream=True).raw)\\n'\n",
      " 'image_processor = '\n",
      " 'AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\\n'\n",
      " 'model = '\n",
      " 'AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\\n'\n",
      " '\\n'\n",
      " 'inputs = image_processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'outputs = model(**inputs)\\n'\n",
      " 'logits = outputs.logits\\n'\n",
      " 'predicted_class_idx = logits.argmax(-1).item()\\n'\n",
      " 'print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '## ‚úÖ Model2: Nh·∫≠n di·ªán c·∫£m x√∫c m√®o ‚Äî '\n",
      " '<https://huggingface.co/semihdervis/cat-emotion-classifier>\\n'\n",
      " '\\n'\n",
      " '**üìù Description (from tool):**  \\n'\n",
      " 'This model is a fine-tuned version of google/vit-base-patch16-224-in21k on a '\n",
      " 'custom dataset.\\n'\n",
      " '\\n'\n",
      " '**üì• Input format (from tool):**  \\n'\n",
      " 'The model expects an image input.\\n'\n",
      " '\\n'\n",
      " '**üì§ Output format (from tool):**  \\n'\n",
      " 'The model predicts the emotion of a cat.\\n'\n",
      " '\\n'\n",
      " '**üõ†Ô∏è Library Requirements (from tool) **  \\n'\n",
      " '- Transformers\\n'\n",
      " '- Pytorch\\n'\n",
      " '- PIL\\n'\n",
      " '\\n'\n",
      " '**üß™ Example code (from tool):**  \\n'\n",
      " 'No code snippet provided.\\n'\n",
      " '\\n'\n",
      " '## ‚úÖ Model3: Ph√¢n lo·∫°i h√¨nh ·∫£nh ‚Äî <https://huggingface.co/zhaospei/Model_7>\\n'\n",
      " '\\n'\n",
      " '**üìù Description (from tool):**  \\n'\n",
      " 'ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh kh√°c tr√™n '\n",
      " 't·∫≠p d·ªØ li·ªáu CIFAR-10. M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o10 l·ªõp '\n",
      " 'kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.\\n'\n",
      " '\\n'\n",
      " '**üì• Input format (from tool):**  \\n'\n",
      " 'ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB\\n'\n",
      " 'K√≠ch th∆∞·ªõc ·∫£nh:224x224 pixels\\n'\n",
      " '\\n'\n",
      " '**üì§ Output format (from tool):**  \\n'\n",
      " 'ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits)\\n'\n",
      " 'Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch th∆∞·ªõc [batch_size,10]\\n'\n",
      " '√ù nghƒ©a: X√°c su·∫•t d·ª± ƒëo√°n cho t·ª´ng l·ªõp trong10 l·ªõp c·ªßa CIFAR-10\\n'\n",
      " '\\n'\n",
      " '**üõ†Ô∏è Library Requirements (from tool) **  \\n'\n",
      " '- transformers\\n'\n",
      " '- torch\\n'\n",
      " '- torchvision\\n'\n",
      " '\\n'\n",
      " '**üß™ Example code (from tool):**  \\n'\n",
      " '```python\\n'\n",
      " 'import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_7\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_7\")\\n'\n",
      " '\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '    outputs = model(**inputs)\\n'\n",
      " '    logits = outputs.logits\\n'\n",
      " '    predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '## üìù Selecting the Best-Fit Models\\n'\n",
      " '\\n'\n",
      " 'Based on the task description, we need two models:\\n'\n",
      " '1. A model to detect dogs and identify their breed.\\n'\n",
      " '2. A model to detect cats and determine their emotion.\\n'\n",
      " '\\n'\n",
      " '### Model for Dog Breed Classification\\n'\n",
      " '- **Model:** Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥ ‚Äî '\n",
      " '<https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit>\\n'\n",
      " '- **Reason:** This model is specifically fine-tuned for classifying dog '\n",
      " 'images into 120 types of dog breeds, which aligns with the task requirement '\n",
      " 'of identifying dog breeds.\\n'\n",
      " '\\n'\n",
      " '### Model for Cat Emotion Classification\\n'\n",
      " '- **Model:** Nh·∫≠n di·ªán c·∫£m x√∫c m√®o ‚Äî '\n",
      " '<https://huggingface.co/semihdervis/cat-emotion-classifier>\\n'\n",
      " '- **Reason:** This model is designed to classify the emotion of cats, which '\n",
      " \"matches the task requirement of determining a cat's emotion.\\n\"\n",
      " '\\n'\n",
      " '### Model for Image Classification (if needed)\\n'\n",
      " '- **Model:** Ph√¢n lo·∫°i h√¨nh ·∫£nh ‚Äî <https://huggingface.co/zhaospei/Model_7>\\n'\n",
      " '- **Reason:** This model can be used for general image classification tasks, '\n",
      " 'but it is not specifically tailored for dog breed or cat emotion '\n",
      " 'classification. It can be used for unsure or unclear images.\\n'\n",
      " '\\n'\n",
      " '## üìù Conclusion\\n'\n",
      " '\\n'\n",
      " \"The best-fit models for Alice's Magical Pet Photo Box are:\\n\"\n",
      " '1. **Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥** ‚Äî '\n",
      " '<https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit> '\n",
      " 'for dog breed classification.\\n'\n",
      " '2. **Nh·∫≠n di·ªán c·∫£m x√∫c m√®o** ‚Äî '\n",
      " '<https://huggingface.co/semihdervis/cat-emotion-classifier> for cat emotion '\n",
      " 'classification.\\n'\n",
      " '\\n'\n",
      " 'These two models can effectively handle the requirements of sorting and '\n",
      " \"organizing Alice's pet photos.\")\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res2['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c497d868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0:\n",
      "('This model is finetuned using the Google Vision Transformer '\n",
      " '(vit-base-patch16-224-in21k) on the Stanford Dogs dataset in Kaggle to '\n",
      " 'classify dog images into 120 types of dog breeds.')\n",
      "'The model expects an image input.'\n",
      "'The model predicts one of the 120 Stanford dog breeds classes.'\n",
      "'Transformers, Pytorch, PIL'\n",
      "('from transformers import AutoImageProcessor, '\n",
      " 'AutoModelForImageClassification\\n'\n",
      " 'import PIL\\n'\n",
      " 'import requests\\n'\n",
      " '\\n'\n",
      " 'url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\\n'\n",
      " 'image = PIL.Image.open(requests.get(url, stream=True).raw)\\n'\n",
      " 'image_processor = '\n",
      " 'AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\\n'\n",
      " 'model = '\n",
      " 'AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\\n'\n",
      " '\\n'\n",
      " 'inputs = image_processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'outputs = model(**inputs)\\n'\n",
      " 'logits = outputs.logits\\n'\n",
      " 'predicted_class_idx = logits.argmax(-1).item()\\n'\n",
      " 'print(\"Predicted class:\", model.config.id2label[predicted_class_idx])')\n",
      "Model 1:\n",
      "('This model is a fine-tuned version of google/vit-base-patch16-224-in21k on a '\n",
      " 'custom dataset.')\n",
      "'The model expects an image input.'\n",
      "'The model predicts the emotion of a cat.'\n",
      "'Transformers, Pytorch, PIL'\n",
      "'No code snippet provided.'\n",
      "Model 2:\n",
      "('ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh kh√°c tr√™n '\n",
      " 't·∫≠p d·ªØ li·ªáu CIFAR-10. M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o 10 l·ªõp '\n",
      " 'kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.')\n",
      "'ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB\\nK√≠ch th∆∞·ªõc ·∫£nh: 224x224 pixels'\n",
      "('ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits)\\n'\n",
      " 'Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch th∆∞·ªõc [batch_size,10]\\n'\n",
      " '√ù nghƒ©a: X√°c su·∫•t d·ª± ƒëo√°n cho t·ª´ng l·ªõp trong 10 l·ªõp c·ªßa CIFAR-10')\n",
      "'transformers, torch, torchvision'\n",
      "('import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_7\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_7\")\\n'\n",
      " '\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " ' outputs = model(**inputs)\\n'\n",
      " ' logits = outputs.logits\\n'\n",
      " ' predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")')\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(ans.models)):\n",
    "    print(f\"Model {i}:\")\n",
    "    pprint.pprint(ans.models[i].model_description)\n",
    "    pprint.pprint(ans.models[i].model_input_format)\n",
    "    pprint.pprint(ans.models[i].model_output_format)\n",
    "    pprint.pprint(ans.models[i].model_requirements)\n",
    "    pprint.pprint(ans.models[i].model_sample_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
