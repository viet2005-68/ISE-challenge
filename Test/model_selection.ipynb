{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d3e60a",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "535f3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY='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'\n",
    "FIRECRAWL_API_KEY='fc-e36282d62d1c4ee8971ec0649f3862c7'\n",
    "BASE_URL='https://api.groq.com/openai/v1'\n",
    "MODEL=\"meta-llama/llama-4-scout-17b-16e-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42744f82",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "91bed7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c:\\\\Users\\\\viett\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\python313.zip', 'c:\\\\Users\\\\viett\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\DLLs', 'c:\\\\Users\\\\viett\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib', 'c:\\\\Users\\\\viett\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313', '', 'c:\\\\Users\\\\viett\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages', 'C:\\\\Users\\\\viett\\\\label-studio', 'c:\\\\Users\\\\viett\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\win32', 'c:\\\\Users\\\\viett\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\win32\\\\lib', 'c:\\\\Users\\\\viett\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python313\\\\Lib\\\\site-packages\\\\Pythonwin', '..', '..']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "print(sys.path)\n",
    "from tools.fetching_description_from_huggingface import fetching_description_from_huggingface\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import Tool\n",
    "from firecrawl import FirecrawlApp\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36830718",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "5827b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task = pd.read_csv(\"/mnt/g/Ai agents course/LangGraph/Challenge2/data/ISE - AutoCode Challenge 2 Public - Public task.csv\")\n",
    "df_model = pd.read_csv(\"/mnt/g/Ai agents course/LangGraph/Challenge2/data/ISE - AutoCode Challenge 2 Public - Model zoo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "22abc01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STT</th>\n",
       "      <th>Task</th>\n",
       "      <th>Validation data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Bối cảnh của vấn đề:\\r\\nNhận dạng chữ viết ta...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Bối cảnh của vấn đề:\\r\\nTrong thời đại thông t...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Bối cảnh của vấn đề:\\r\\nĐây là một nhiệm vụ hồ...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Bối cảnh của vấn đề:\\r\\nĐiều quan trọng là các...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Bối cảnh của vấn đề:\\r\\nTrong nhiều ứng dụng t...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STT                                               Task Validation data\n",
       "0    1  \"Bối cảnh của vấn đề:\\r\\nNhận dạng chữ viết ta...            Link\n",
       "1    2  Bối cảnh của vấn đề:\\r\\nTrong thời đại thông t...            Link\n",
       "2    3  Bối cảnh của vấn đề:\\r\\nĐây là một nhiệm vụ hồ...            Link\n",
       "3    4  Bối cảnh của vấn đề:\\r\\nĐiều quan trọng là các...            Link\n",
       "4    5  Bối cảnh của vấn đề:\\r\\nTrong nhiều ứng dụng t...            Link"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "697add1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = df_task[\"Task\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "953cae02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bối cảnh của vấn đề:\\r\\nTrong nhiều ứng dụng thực tế như tổng hợp nội dung hình ảnh, phân loại album, hay trích xuất đặc trưng nội dung cho ảnh tổ hợp, cần xác định chủ đề chính của một ảnh ghép từ nhiều ảnh con. Bài toán này đòi hỏi ứng dụng không chỉ nhận diện được nội dung của từng ảnh con, mà còn cần xác định chủ đề chiếm ưu thế trong toàn bộ ảnh tổng thể.\\r\\n\\r\\nYêu cầu cụ thể cần đạt được:\\r\\nMỗi đầu vào là một ảnh tổ hợp, ghép từ 9 ảnh màu nhỏ (3x3).\\r\\nNhiệm vụ của ứng dụng là:\\r\\nXác định chủ đề chiếm số lượng nhiều nhất trong 9 ảnh con.\\r\\nTrường hợp có nhiều chủ đề xuất hiện với tần suất bằng nhau, có thể chọn một chủ đề bất kỳ trong số đó làm kết quả.\\r\\n\\r\\nĐịnh dạng dữ liệu đầu vào cho bài toán tổng thể:\\r\\nMột thư mục có tên là \"images\" chứa các ảnh màu RGB (288x288), được ghép từ 9 ảnh RGB con (32x32) (3 hàng × 3 cột).\\r\\n\\r\\nĐịnh dạng kết quả đầu ra mong muốn cho bài toán tổng thể:\\r\\nFile output.csv mỗi hàng là kết quả dự đoán mỗi ảnh\\r\\ncó các cột:\\r\\nfile_name: tên file ảnh\\r\\nprediction: nhãn của ảnh. \\r\\nnhãn của ảnh là một nhãn duy nhất đại diện cho chủ đề xuất hiện nhiều nhất trong 9 ảnh con.\\r\\nCác chủ đề sau nằm trong danh sách sau:\\r\\n       [\\'forest\\', \\'orchid\\', \\'spider\\', \\'fox\\', \\'caterpillar\\', \\'crocodile\\',\\r\\n       \\'whale\\', \\'road\\', \\'motorcycle\\', \\'tiger\\', \\'woman\\', \\'orange\\',\\r\\n       \\'elephant\\', \\'otter\\', \\'flatfish\\', \\'cup\\', \\'sea\\', \\'mountain\\', \\'cloud\\',\\r\\n       \\'willow_tree\\', \\'rabbit\\', \\'couch\\', \\'maple_tree\\', \\'wardrobe\\',\\r\\n       \\'mushroom\\', \\'plain\\', \\'telephone\\', \\'shrew\\', \\'bowl\\', \\'tractor\\',\\r\\n       \\'camel\\', \\'bus\\', \\'cattle\\', \\'house\\', \\'cockroach\\', \\'lawn_mower\\',\\r\\n       \\'oak_tree\\', \\'apple\\', \\'bicycle\\', \\'beetle\\', \\'palm_tree\\', \\'sunflower\\',\\r\\n       \\'poppy\\', \\'keyboard\\', \\'porcupine\\', \\'snake\\', \\'can\\', \\'pear\\', \\'shark\\',\\r\\n       \\'chimpanzee\\', \\'hamster\\', \\'train\\', \\'cra\\', \\'bear\\', \\'sweet_pepper\\',\\r\\n       \\'pine_tree\\', \\'bed\\', \\'tank\\', \\'beaver\\', \\'bridge\\', \\'girl\\',\\r\\n       \\'skyscraper\\', \\'wolf\\', \\'raccoon\\', \\'rose\\', \\'worm\\', \\'table\\',\\r\\n       \\'pickup_truck\\', \\'turtle\\', \\'lizard\\', \\'leopard\\', \\'streetcar\\',\\r\\n       \\'rocket\\', \\'man\\', \\'tulip\\', \\'lamp\\', \\'seal\\', \\'lion\\', \\'butterfly\\',\\r\\n       \\'kangaroo\\', \\'television\\', \\'ray\\', \\'chair\\', \\'skunk\\', \\'trout\\',\\r\\n       \\'possum\\', \\'mouse\\']'"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "b6909b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_list(df):\n",
    "    model_strings = []\n",
    "    for i, row in df.iterrows():\n",
    "        desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        url = str(row[2])\n",
    "        model_strings.append(f\"{i + 1} - {desc} [More info]({url})\")\n",
    "    return \"\\n\".join(model_strings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c40886",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "af3bcff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraper(url: str) -> str:\n",
    "    \"\"\"Get more details about the model using the model url\"\"\"\n",
    "    # scraper = FirecrawlApp(api_key=FIRECRAWL_API_KEY)\n",
    "    # scrape_status = scraper.scrape_url(\n",
    "    #     url,\n",
    "    #     formats=['markdown']\n",
    "    # )\n",
    "    # return scrape_status.markdown\n",
    "    result = fetching_description_from_huggingface(url)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "77f3a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraperParameters(BaseModel):\n",
    "    url: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "b6a0e635",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Failed to fetch from https://huggingface.co/thanhtlx/image_classification_01: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /thanhtlx/image_classification_01 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000024682F32350>: Failed to resolve 'huggingface.co' ([Errno 11001] getaddrinfo failed)\"))",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mgaierror\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\util\\connection.py:60\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocationParseError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, label empty or too long\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     61\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py:977\u001b[39m, in \u001b[36mgetaddrinfo\u001b[39m\u001b[34m(host, port, family, type, proto, flags)\u001b[39m\n\u001b[32m    976\u001b[39m addrlist = []\n\u001b[32m--> \u001b[39m\u001b[32m977\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    978\u001b[39m     af, socktype, proto, canonname, sa = res\n",
      "\u001b[31mgaierror\u001b[39m: [Errno 11001] getaddrinfo failed",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mNameResolutionError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:488\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    487\u001b[39m         new_e = _wrap_proxy_error(new_e, conn.proxy.scheme)\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m new_e\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# conn.request() calls http.client.*.request, not the method in\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[38;5;66;03m# urllib3.request. It also calls makefile (recv) on the socket.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    463\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connection.py:704\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    703\u001b[39m sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m704\u001b[39m \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connection.py:205\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mNameResolutionError\u001b[39m: <urllib3.connection.HTTPSConnection object at 0x0000024682F32350>: Failed to resolve 'huggingface.co' ([Errno 11001] getaddrinfo failed)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMaxRetryError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\connectionpool.py:841\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    839\u001b[39m     new_e = ProtocolError(\u001b[33m\"\u001b[39m\u001b[33mConnection aborted.\u001b[39m\u001b[33m\"\u001b[39m, new_e)\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m retries = \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m retries.sleep()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\urllib3\\util\\retry.py:519\u001b[39m, in \u001b[36mRetry.increment\u001b[39m\u001b[34m(self, method, url, response, error, _pool, _stacktrace)\u001b[39m\n\u001b[32m    518\u001b[39m     reason = error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    521\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mIncremented Retry for (url=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[33m\"\u001b[39m, url, new_retry)\n",
      "\u001b[31mMaxRetryError\u001b[39m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /thanhtlx/image_classification_01 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000024682F32350>: Failed to resolve 'huggingface.co' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\ISE-challenge\\Test\\..\\tools\\fetching_description_from_huggingface.py:16\u001b[39m, in \u001b[36mfetching_description_from_huggingface\u001b[39m\u001b[34m(url, timeout)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m   response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m   response.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\requests\\adapters.py:700\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    698\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SSLError(e, request=request)\n\u001b[32m--> \u001b[39m\u001b[32m700\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(e, request=request)\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ClosedPoolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mConnectionError\u001b[39m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /thanhtlx/image_classification_01 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000024682F32350>: Failed to resolve 'huggingface.co' ([Errno 11001] getaddrinfo failed)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[409]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res = \u001b[43mweb_scraper\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps://huggingface.co/thanhtlx/image_classification_01\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[407]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mweb_scraper\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get more details about the model using the model url\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# scraper = FirecrawlApp(api_key=FIRECRAWL_API_KEY)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# scrape_status = scraper.scrape_url(\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#     url,\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#     formats=['markdown']\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# return scrape_status.markdown\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m result = \u001b[43mfetching_description_from_huggingface\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\viett\\ISE-challenge\\Test\\..\\tools\\fetching_description_from_huggingface.py:29\u001b[39m, in \u001b[36mfetching_description_from_huggingface\u001b[39m\u001b[34m(url, timeout)\u001b[39m\n\u001b[32m     27\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRequest to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m timed out after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m requests.exceptions.RequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to fetch from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mConnectionError\u001b[39m: Failed to fetch from https://huggingface.co/thanhtlx/image_classification_01: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /thanhtlx/image_classification_01 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000024682F32350>: Failed to resolve 'huggingface.co' ([Errno 11001] getaddrinfo failed)\"))"
     ]
    }
   ],
   "source": [
    "res = web_scraper('https://huggingface.co/thanhtlx/image_classification_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "52a9c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"web_scrapper\",\n",
    "        func=web_scraper,\n",
    "        description=\"Use to get more detail about the model\",\n",
    "        args_schema=WebScraperParameters\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c409f8b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "eca82430",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI( \n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "6511a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tasks(BaseModel):\n",
    "    subtask_one: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 1\"\n",
    "    )\n",
    "    subtask_two: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 2\"\n",
    "    )\n",
    "    subtask_three: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 3\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "0de6f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: HumanMessage | None\n",
    "    data: str | None\n",
    "    messages: Annotated[BaseMessage, add_messages]\n",
    "    subtasks: Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "ef593964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_agent(state: AgentState) -> AgentState:\n",
    "    structured_llm = llm.with_structured_output(Tasks)\n",
    "    system_prompt = \"\"\"\n",
    "                    You are an expert in task formulation. Given a machine learning task, you will\n",
    "                    help divide the task into these subtasks:\n",
    "                    - Subtask1: for classifying the problem type and task description\n",
    "                    - Subtask2: for identifying the input of the problem\n",
    "                    - Subtask3: for identifying the output of the problem\n",
    "                    describe the each subtask as specifically as possible\n",
    "                    \"\"\"\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    messages.append(state['task'])\n",
    "    result = structured_llm.invoke(messages)\n",
    "    state['subtasks'] = result\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "14b15c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"task\": HumanMessage(content=task),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "78675acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = classify_agent(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab3d7f",
   "metadata": {},
   "source": [
    "# Modeling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "f656ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInfo(BaseModel):\n",
    "    model: str = Field(..., description=\"Model name\")\n",
    "    url: str = Field(..., description=\"URL to the model\")\n",
    "\n",
    "class ModelSelection(BaseModel):\n",
    "    models: List[ModelInfo] = Field(..., description=\"List of available models with name and URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "18dcc46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_parser = PydanticOutputParser(pydantic_object=ModelSelection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "de4ed792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the suitable model for a given task.\n",
    "                    Given:\n",
    "                    - A user description about the task\n",
    "                    - A list of available model (names and links)\n",
    "                    Your job is to choose ALL suitable models provided in the list for user's specific tasks.\n",
    "                    Return the answer in the format\n",
    "                    {structured_output}\n",
    "                    Here are the list of model:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]).partial(structured_output=model_selection_parser.get_format_instructions())\n",
    "\n",
    "    chain = prompt | llm | model_selection_parser\n",
    "\n",
    "    result = chain.invoke({\"input\": task, \"model_list\": format_model_list(df_model)})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "fd097a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\viett\\AppData\\Local\\Temp\\ipykernel_48012\\7332665.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
      "C:\\Users\\viett\\AppData\\Local\\Temp\\ipykernel_48012\\7332665.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  url = str(row[2])\n"
     ]
    }
   ],
   "source": [
    "res = model_selection_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "03a8819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelInfo(model='Phân loại hình ảnh', url='https://huggingface.co/zhaospei/Model_7'),\n",
      " ModelInfo(model='Phân loại hình ảnh thành 100 lớp đối tượng chi tiết', url='https://huggingface.co/zhaospei/Model_6')]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "3b0dc638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_using_tool_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the **Best-fit model** for a given task.\n",
    "                    Given:\n",
    "                    - A user task description\n",
    "                    - A list of available models (names and url)\n",
    "                    - A tool that fetches model details from a provided URL (it help gathers model description, input/output format, code sample usage)\n",
    "\n",
    "                    Your job is to:\n",
    "                    1. Use the **provided tool** to retrieve real details about each candidate model:\n",
    "                        - ✅ Model description\n",
    "                        - 📥 Input format\n",
    "                        - 📤 Output format\n",
    "                        - 🛠️ Library requirements\n",
    "                        - 🧪 Code sample (usage code snippet)\n",
    "                    2. Select the best model for user given task based on the data that the tool gives you.\n",
    "\n",
    "                    ⚠️ VERY IMPORTANT RULES\n",
    "\n",
    "                    - ❌ DO NOT assume or invent any part of the model's description, input/output format, or code\n",
    "                    - ❌ DO NOT generate fake code or use your own knowledge about the model\n",
    "                    - ✅ ONLY use the **actual output** returned from the tool\n",
    "                    - ✅ Include tool content in your final answer exactly as returned (especially code)\n",
    "\n",
    "                    ## 🧠 Output Structure (Final Answer)\n",
    "\n",
    "                    After retrieving tool results, choose ONE best-fit model and output the result in the following format:\n",
    "\n",
    "                    **✅ Model name and link**  \n",
    "                    `<model name>` — `<link>`\n",
    "\n",
    "                    **📝 Description (from tool):**  \n",
    "                    <model description>\n",
    "\n",
    "                    **📥 Input format (from tool):**  \n",
    "                    <description of expected input>\n",
    "\n",
    "                    **📤 Output format (from tool):**  \n",
    "                    <description of model output>\n",
    "\n",
    "                    **🛠️ Library Requirements (from tool) **\n",
    "                    <requirements to use the model>\n",
    "\n",
    "                    **🧪 Example code (from tool):**  \n",
    "                    ```python\n",
    "                    <exact code snippet from tool>\n",
    "\n",
    "                    Here are the list of models and there corresponding URL:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"ai\", \"{agent_scratchpad}\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "    task_input = state[\"task\"].content if isinstance(state[\"task\"], SystemMessage) else state[\"task\"]\n",
    "\n",
    "    # result = executor.invoke({\"input\": task_input})\n",
    "    result = executor.invoke({\"input\": task_input, \"model_list\": res.models})\n",
    "\n",
    "    return {\"output\": result[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "e8b0f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_7'}`\n",
      "responded: To solve this task, I will first use the `web_scrapper` function to gather information about each of the available models. This will help me understand their descriptions, input and output formats, library requirements, and code samples.\n",
      "\n",
      "I have two models to investigate:\n",
      "1. Phân loại hình ảnh — https://huggingface.co/zhaospei/Model_7\n",
      "2. Phân loại hình ảnh thành100 lớp đối tượng chi tiết — https://huggingface.co/zhaospei/Model_6\n",
      "\n",
      "Let's start by gathering information about the first model.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "🖼️ Mô hình Phân loại Ảnh - ViT Fine-tuned trên CIFAR-10\n",
      "📝 Mô tả\n",
      "Đây là mô hình Vision Transformer (ViT) được fine-tuned từ mô hình khác trên tập dữ liệu CIFAR-10. Mô hình được huấn luyện để phân loại ảnh vào 10 lớp khác nhau, mỗi lớp đại diện cho một danh mục đối tượng cụ thể.\n",
      "📌 Nhiệm vụ\n",
      "Loại bài toán: Phân loại ảnh (Image Classification)\n",
      "Số lớp: 10 (Tương ứng nhãn) CIFAR-10\n",
      "📥 Đầu vào\n",
      "Định dạng: Ảnh màu RGB\n",
      "Kích thước ảnh: 224x224 pixels\n",
      "📤 Đầu ra\n",
      "Định dạng: Xác suất cho mỗi lớp (logits)\n",
      "Kiểu dữ liệu: Tensor có kích thước [batch_size, 10]\n",
      "Ý nghĩa: Xác suất dự đoán cho từng lớp trong 10 lớp của CIFAR-10\n",
      "🛠 Yêu cầu thư viện\n",
      "Cài đặt các thư viện cần thiết bằng:\n",
      "pip install transformers torch torchvision\n",
      "🧪 Cách sử dụng mô hình\n",
      "Dưới đây là ví dụ về cách sử dụng mô hình để phân loại một ảnh:\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "# Tải ảnh cần phân loại\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "# Tải processor và mô hình từ Hugging Face\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_7\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_7\")\n",
      "# Xử lý đầu vào\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "# Dự đoán với mô hình\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_label = logits.argmax(-1).item()\n",
      "print(f\"Nhãn dự đoán: {model.config.id2label[predicted_label]}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_6'}`\n",
      "responded: To solve this task, I will now gather information about the second model.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "🖼️ Mô hình Phân loại Ảnh - ViT Fine-tuned trên CIFAR-100\n",
      "📝 Mô tả\n",
      "Đây là mô hình Vision Transformer (ViT) được fine-tuned từ mô hình gốc google/vit-base-patch16-224-in21k trên tập dữ liệu CIFAR-100. Mô hình được huấn luyện để phân loại ảnh vào 100 lớp khác nhau, mỗi lớp đại diện cho một danh mục đối tượng cụ thể.\n",
      "📌 Nhiệm vụ\n",
      "Loại bài toán: Phân loại ảnh (Image Classification)\n",
      "Số lớp: 100\n",
      "Tập dữ liệu: CIFAR-100\n",
      "📥 Đầu vào\n",
      "Định dạng: Ảnh màu RGB\n",
      "Kích thước ảnh:  224x224\n",
      "📤 Đầu ra\n",
      "Định dạng: Xác suất cho mỗi lớp (logits)\n",
      "Kiểu dữ liệu: Tensor có kích thước [batch_size, 100]\n",
      "Ý nghĩa: Xác suất dự đoán cho từng lớp trong 100 lớp của CIFAR-100\n",
      "🛠 Yêu cầu thư viện\n",
      "Cài đặt các thư viện cần thiết bằng:\n",
      "pip install transformers torch torchvision\n",
      "🧪 Cách sử dụng mô hình\n",
      "Dưới đây là ví dụ về cách sử dụng mô hình để phân loại một ảnh:\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "# Tải ảnh cần phân loại\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "# Tải processor và mô hình từ Hugging Face\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\n",
      "# Xử lý đầu vào\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "# Dự đoán với mô hình\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_label = logits.argmax(-1).item()\n",
      "print(f\"Nhãn dự đoán: {model.config.id2label[predicted_label]}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mTo solve this task, I need to select the best-fit model for the given task based on the information retrieved about each model. The task involves determining the dominant theme in a composite image made of 9 smaller images.\n",
      "\n",
      "Here are the details of the two models:\n",
      "1. **Phân loại hình ảnh** — https://huggingface.co/zhaospei/Model_7\n",
      "2. **Phân loại hình ảnh thành100 lớp đối tượng chi tiết** — https://huggingface.co/zhaospei/Model_6\n",
      "\n",
      "Given the task requirements:\n",
      "- Each input is a composite image made of 9 smaller images (3x3).\n",
      "- The task is to identify the most dominant theme among the 9 smaller images.\n",
      "\n",
      "Let's analyze the models:\n",
      "\n",
      "### Model 1: Phân loại hình ảnh — https://huggingface.co/zhaospei/Model_7\n",
      "- **Description:** A Vision Transformer (ViT) model fine-tuned on CIFAR-10 dataset for image classification into 10 classes.\n",
      "- **Input Format:** RGB images of size 224x224 pixels.\n",
      "- **Output Format:** Probabilities for each class (logits) in a tensor of size [batch_size, 10].\n",
      "- **Library Requirements:** `transformers`, `torch`, `torchvision`.\n",
      "- **Example Code:**\n",
      "```python\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_7\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_7\")\n",
      "\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_label = logits.argmax(-1).item()\n",
      "print(f\"Predicted label: {model.config.id2label[predicted_label]}\")\n",
      "```\n",
      "\n",
      "### Model 2: Phân loại hình ảnh thành100 lớp đối tượng chi tiết — https://huggingface.co/zhaospei/Model_6\n",
      "- **Description:** A Vision Transformer (ViT) model fine-tuned on CIFAR-100 dataset for image classification into 100 classes.\n",
      "- **Input Format:** RGB images of size 224x224 pixels.\n",
      "- **Output Format:** Probabilities for each class (logits) in a tensor of size [batch_size, 100].\n",
      "- **Library Requirements:** `transformers`, `torch`, `torchvision`.\n",
      "- **Example Code:**\n",
      "```python\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\n",
      "\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_label = logits.argmax(-1).item()\n",
      "print(f\"Predicted label: {model.config.id2label[predicted_label]}\")\n",
      "```\n",
      "\n",
      "### Task Requirements:\n",
      "- **Input:** A composite image made of 9 smaller images (3x3 grid).\n",
      "- **Output:** The dominant theme (label) among the 9 smaller images.\n",
      "\n",
      "### Analysis:\n",
      "- The task requires analyzing a composite image and determining the dominant theme among its 9 constituent images.\n",
      "- Model 1 is trained on CIFAR-10, which has 10 classes.\n",
      "- Model 2 is trained on CIFAR-100, which has 100 classes.\n",
      "\n",
      "### Choosing the Best Model:\n",
      "- Since the task involves identifying the dominant theme among 9 images and the list of possible labels includes 100 different classes, **Model 2: Phân loại hình ảnh thành100 lớp đối tượng chi tiết** seems more suitable. This is because it can handle a larger number of classes (100) compared to Model 1 (10 classes).\n",
      "\n",
      "### Final Answer:\n",
      "**✅ Model name and link**  \n",
      "`Phân loại hình ảnh thành100 lớp đối tượng chi tiết` — `https://huggingface.co/zhaospei/Model_6`\n",
      "\n",
      "**📝 Description (from tool):**  \n",
      "`Đây là mô hình Vision Transformer (ViT) được fine-tuned từ mô hình gốc google/vit-base-patch16-224-in21k trên tập dữ liệu CIFAR-100. Mô hình được huấn luyện để phân loại ảnh vào100 lớp khác nhau, mỗi lớp đại diện cho một danh mục đối tượng cụ thể.`\n",
      "\n",
      "**📥 Input format (from tool):**  \n",
      "`Định dạng: Ảnh màu RGB\n",
      "Kích thước ảnh:224x224`\n",
      "\n",
      "**📤 Output format (from tool):**  \n",
      "`Định dạng: Xác suất cho mỗi lớp (logits)\n",
      "Kiểu dữ liệu: Tensor có kích thước [batch_size,100]\n",
      "Ý nghĩa: Xác suất dự đoán cho từng lớp trong100 lớp của CIFAR-100`\n",
      "\n",
      "**🛠️ Library Requirements (from tool) **  \n",
      "`Cài đặt các thư viện cần thiết bằng:\n",
      "pip install transformers torch torchvision`\n",
      "\n",
      "**🧪 Example code (from tool):**  \n",
      "```python\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\n",
      "\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_label = logits.argmax(-1).item()\n",
      "print(f\"Nhãn dự đoán: {model.config.id2label[predicted_label]}\")\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res2 = model_selection_using_tool_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "cb681a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('To solve this task, I need to select the best-fit model for the given task '\n",
      " 'based on the information retrieved about each model. The task involves '\n",
      " 'determining the dominant theme in a composite image made of 9 smaller '\n",
      " 'images.\\n'\n",
      " '\\n'\n",
      " 'Here are the details of the two models:\\n'\n",
      " '1. **Phân loại hình ảnh** — https://huggingface.co/zhaospei/Model_7\\n'\n",
      " '2. **Phân loại hình ảnh thành100 lớp đối tượng chi tiết** — '\n",
      " 'https://huggingface.co/zhaospei/Model_6\\n'\n",
      " '\\n'\n",
      " 'Given the task requirements:\\n'\n",
      " '- Each input is a composite image made of 9 smaller images (3x3).\\n'\n",
      " '- The task is to identify the most dominant theme among the 9 smaller '\n",
      " 'images.\\n'\n",
      " '\\n'\n",
      " \"Let's analyze the models:\\n\"\n",
      " '\\n'\n",
      " '### Model 1: Phân loại hình ảnh — https://huggingface.co/zhaospei/Model_7\\n'\n",
      " '- **Description:** A Vision Transformer (ViT) model fine-tuned on CIFAR-10 '\n",
      " 'dataset for image classification into 10 classes.\\n'\n",
      " '- **Input Format:** RGB images of size 224x224 pixels.\\n'\n",
      " '- **Output Format:** Probabilities for each class (logits) in a tensor of '\n",
      " 'size [batch_size, 10].\\n'\n",
      " '- **Library Requirements:** `transformers`, `torch`, `torchvision`.\\n'\n",
      " '- **Example Code:**\\n'\n",
      " '```python\\n'\n",
      " 'import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_7\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_7\")\\n'\n",
      " '\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '    outputs = model(**inputs)\\n'\n",
      " '    logits = outputs.logits\\n'\n",
      " '    predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Predicted label: {model.config.id2label[predicted_label]}\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '### Model 2: Phân loại hình ảnh thành100 lớp đối tượng chi tiết — '\n",
      " 'https://huggingface.co/zhaospei/Model_6\\n'\n",
      " '- **Description:** A Vision Transformer (ViT) model fine-tuned on CIFAR-100 '\n",
      " 'dataset for image classification into 100 classes.\\n'\n",
      " '- **Input Format:** RGB images of size 224x224 pixels.\\n'\n",
      " '- **Output Format:** Probabilities for each class (logits) in a tensor of '\n",
      " 'size [batch_size, 100].\\n'\n",
      " '- **Library Requirements:** `transformers`, `torch`, `torchvision`.\\n'\n",
      " '- **Example Code:**\\n'\n",
      " '```python\\n'\n",
      " 'import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " '\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '    outputs = model(**inputs)\\n'\n",
      " '    logits = outputs.logits\\n'\n",
      " '    predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Predicted label: {model.config.id2label[predicted_label]}\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '### Task Requirements:\\n'\n",
      " '- **Input:** A composite image made of 9 smaller images (3x3 grid).\\n'\n",
      " '- **Output:** The dominant theme (label) among the 9 smaller images.\\n'\n",
      " '\\n'\n",
      " '### Analysis:\\n'\n",
      " '- The task requires analyzing a composite image and determining the dominant '\n",
      " 'theme among its 9 constituent images.\\n'\n",
      " '- Model 1 is trained on CIFAR-10, which has 10 classes.\\n'\n",
      " '- Model 2 is trained on CIFAR-100, which has 100 classes.\\n'\n",
      " '\\n'\n",
      " '### Choosing the Best Model:\\n'\n",
      " '- Since the task involves identifying the dominant theme among 9 images and '\n",
      " 'the list of possible labels includes 100 different classes, **Model 2: Phân '\n",
      " 'loại hình ảnh thành100 lớp đối tượng chi tiết** seems more suitable. This is '\n",
      " 'because it can handle a larger number of classes (100) compared to Model 1 '\n",
      " '(10 classes).\\n'\n",
      " '\\n'\n",
      " '### Final Answer:\\n'\n",
      " '**✅ Model name and link**  \\n'\n",
      " '`Phân loại hình ảnh thành100 lớp đối tượng chi tiết` — '\n",
      " '`https://huggingface.co/zhaospei/Model_6`\\n'\n",
      " '\\n'\n",
      " '**📝 Description (from tool):**  \\n'\n",
      " '`Đây là mô hình Vision Transformer (ViT) được fine-tuned từ mô hình gốc '\n",
      " 'google/vit-base-patch16-224-in21k trên tập dữ liệu CIFAR-100. Mô hình được '\n",
      " 'huấn luyện để phân loại ảnh vào100 lớp khác nhau, mỗi lớp đại diện cho một '\n",
      " 'danh mục đối tượng cụ thể.`\\n'\n",
      " '\\n'\n",
      " '**📥 Input format (from tool):**  \\n'\n",
      " '`Định dạng: Ảnh màu RGB\\n'\n",
      " 'Kích thước ảnh:224x224`\\n'\n",
      " '\\n'\n",
      " '**📤 Output format (from tool):**  \\n'\n",
      " '`Định dạng: Xác suất cho mỗi lớp (logits)\\n'\n",
      " 'Kiểu dữ liệu: Tensor có kích thước [batch_size,100]\\n'\n",
      " 'Ý nghĩa: Xác suất dự đoán cho từng lớp trong100 lớp của CIFAR-100`\\n'\n",
      " '\\n'\n",
      " '**🛠️ Library Requirements (from tool) **  \\n'\n",
      " '`Cài đặt các thư viện cần thiết bằng:\\n'\n",
      " 'pip install transformers torch torchvision`\\n'\n",
      " '\\n'\n",
      " '**🧪 Example code (from tool):**  \\n'\n",
      " '```python\\n'\n",
      " 'import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " '\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '    outputs = model(**inputs)\\n'\n",
      " '    logits = outputs.logits\\n'\n",
      " '    predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Nhãn dự đoán: {model.config.id2label[predicted_label]}\")\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res2['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874a1c7",
   "metadata": {},
   "source": [
    "## Output Parser Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "3658cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelingOutput(BaseModel):\n",
    "    model_description: str = Field(\n",
    "        ...,\n",
    "        description=\"The detailed description of the model.\"\n",
    "    )\n",
    "    model_input_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed input format\"\n",
    "    )\n",
    "    model_output_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed output format\"\n",
    "    )\n",
    "    model_sample_code: str = Field(\n",
    "        ...,\n",
    "        description=\"Example code to use the model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "0b610e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=ModelingOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "707428ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser_llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "5a1f51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_parser_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a smart AI tasked with extracting structured technical details about a machine learning model from a reasoning result.\n",
    "                    You are given a detailed text description about a model.\n",
    "                    Your goal is to fill the following fields based on the text:\n",
    "\n",
    "                    - `model_description`: A detailed explanation of what the model is and what it does.\n",
    "                    - `model_input_format`: A detailed description of the model's input format, including dimensions, data types, and expected preprocessing if mentioned.\n",
    "                    - `model_output_format`: A detailed description of the output format including dimensions, data types, label name and its meaning.\n",
    "                    - `model_sample_code`: A sample code on how to use the model.\n",
    "\n",
    "                    You MUSTN'T create any data on your own, only using the data provided in the text.\n",
    "\n",
    "                    Return the data as a JSON object matching the following structure:\n",
    "                    {formatted_output}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        ('system', system_prompt),\n",
    "        ('human', \"{input}\")\n",
    "    ]).partial(formatted_output=parser.get_format_instructions())\n",
    "    chain = prompt | llm | parser\n",
    "    response = chain.invoke({\"input\": res2['output']})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "0d82f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = output_parser_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "583a7bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('To solve this task, I need to select the best-fit model for the given task '\n",
      " 'based on the information retrieved about each model. The task involves '\n",
      " 'determining the dominant theme in a composite image made of 9 smaller '\n",
      " 'images.\\n'\n",
      " '\\n'\n",
      " 'Here are the details of the two models:\\n'\n",
      " '1. **Phân loại hình ảnh** — https://huggingface.co/zhaospei/Model_7\\n'\n",
      " '2. **Phân loại hình ảnh thành100 lớp đối tượng chi tiết** — '\n",
      " 'https://huggingface.co/zhaospei/Model_6\\n'\n",
      " '\\n'\n",
      " 'Given the task requirements:\\n'\n",
      " '- Each input is a composite image made of 9 smaller images (3x3).\\n'\n",
      " '- The task is to identify the most dominant theme among the 9 smaller '\n",
      " 'images.\\n'\n",
      " '\\n'\n",
      " \"Let's analyze the models:\\n\"\n",
      " '\\n'\n",
      " '### Model 1: Phân loại hình ảnh — https://huggingface.co/zhaospei/Model_7\\n'\n",
      " '- **Description:** A Vision Transformer (ViT) model fine-tuned on CIFAR-10 '\n",
      " 'dataset for image classification into 10 classes.\\n'\n",
      " '- **Input Format:** RGB images of size 224x224 pixels.\\n'\n",
      " '- **Output Format:** Probabilities for each class (logits) in a tensor of '\n",
      " 'size [batch_size, 10].\\n'\n",
      " '- **Library Requirements:** `transformers`, `torch`, `torchvision`.\\n'\n",
      " '- **Example Code:**\\n'\n",
      " '```python\\n'\n",
      " 'import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_7\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_7\")\\n'\n",
      " '\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '    outputs = model(**inputs)\\n'\n",
      " '    logits = outputs.logits\\n'\n",
      " '    predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Predicted label: {model.config.id2label[predicted_label]}\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '### Model 2: Phân loại hình ảnh thành100 lớp đối tượng chi tiết — '\n",
      " 'https://huggingface.co/zhaospei/Model_6\\n'\n",
      " '- **Description:** A Vision Transformer (ViT) model fine-tuned on CIFAR-100 '\n",
      " 'dataset for image classification into 100 classes.\\n'\n",
      " '- **Input Format:** RGB images of size 224x224 pixels.\\n'\n",
      " '- **Output Format:** Probabilities for each class (logits) in a tensor of '\n",
      " 'size [batch_size, 100].\\n'\n",
      " '- **Library Requirements:** `transformers`, `torch`, `torchvision`.\\n'\n",
      " '- **Example Code:**\\n'\n",
      " '```python\\n'\n",
      " 'import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " '\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '    outputs = model(**inputs)\\n'\n",
      " '    logits = outputs.logits\\n'\n",
      " '    predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Predicted label: {model.config.id2label[predicted_label]}\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " '### Task Requirements:\\n'\n",
      " '- **Input:** A composite image made of 9 smaller images (3x3 grid).\\n'\n",
      " '- **Output:** The dominant theme (label) among the 9 smaller images.\\n'\n",
      " '\\n'\n",
      " '### Analysis:\\n'\n",
      " '- The task requires analyzing a composite image and determining the dominant '\n",
      " 'theme among its 9 constituent images.\\n'\n",
      " '- Model 1 is trained on CIFAR-10, which has 10 classes.\\n'\n",
      " '- Model 2 is trained on CIFAR-100, which has 100 classes.\\n'\n",
      " '\\n'\n",
      " '### Choosing the Best Model:\\n'\n",
      " '- Since the task involves identifying the dominant theme among 9 images and '\n",
      " 'the list of possible labels includes 100 different classes, **Model 2: Phân '\n",
      " 'loại hình ảnh thành100 lớp đối tượng chi tiết** seems more suitable. This is '\n",
      " 'because it can handle a larger number of classes (100) compared to Model 1 '\n",
      " '(10 classes).\\n'\n",
      " '\\n'\n",
      " '### Final Answer:\\n'\n",
      " '**✅ Model name and link**  \\n'\n",
      " '`Phân loại hình ảnh thành100 lớp đối tượng chi tiết` — '\n",
      " '`https://huggingface.co/zhaospei/Model_6`\\n'\n",
      " '\\n'\n",
      " '**📝 Description (from tool):**  \\n'\n",
      " '`Đây là mô hình Vision Transformer (ViT) được fine-tuned từ mô hình gốc '\n",
      " 'google/vit-base-patch16-224-in21k trên tập dữ liệu CIFAR-100. Mô hình được '\n",
      " 'huấn luyện để phân loại ảnh vào100 lớp khác nhau, mỗi lớp đại diện cho một '\n",
      " 'danh mục đối tượng cụ thể.`\\n'\n",
      " '\\n'\n",
      " '**📥 Input format (from tool):**  \\n'\n",
      " '`Định dạng: Ảnh màu RGB\\n'\n",
      " 'Kích thước ảnh:224x224`\\n'\n",
      " '\\n'\n",
      " '**📤 Output format (from tool):**  \\n'\n",
      " '`Định dạng: Xác suất cho mỗi lớp (logits)\\n'\n",
      " 'Kiểu dữ liệu: Tensor có kích thước [batch_size,100]\\n'\n",
      " 'Ý nghĩa: Xác suất dự đoán cho từng lớp trong100 lớp của CIFAR-100`\\n'\n",
      " '\\n'\n",
      " '**🛠️ Library Requirements (from tool) **  \\n'\n",
      " '`Cài đặt các thư viện cần thiết bằng:\\n'\n",
      " 'pip install transformers torch torchvision`\\n'\n",
      " '\\n'\n",
      " '**🧪 Example code (from tool):**  \\n'\n",
      " '```python\\n'\n",
      " 'import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " '\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '    outputs = model(**inputs)\\n'\n",
      " '    logits = outputs.logits\\n'\n",
      " '    predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Nhãn dự đoán: {model.config.id2label[predicted_label]}\")\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res2['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "530f5c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This is a ViT model fine-tuned on CIFAR-100 dataset for image classification '\n",
      " 'into 100 classes. The model is expected to classify images into one of 100 '\n",
      " 'different themes, which could potentially align with the themes present in a '\n",
      " '3x3 grid of images.')\n",
      "('RGB images of size 224x224. However, the task requires a 288x288 image '\n",
      " 'composed of 9 smaller 32x32 images. The input image would need to be '\n",
      " \"preprocessed to fit the model's expected input size.\")\n",
      "('Probabilities for each class. The output is a tensor of size [batch_size, '\n",
      " '100] representing the logits for each class. The predicted label is '\n",
      " 'determined by taking the argmax of the logits.')\n",
      "('import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " '# Load image\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " '\\n'\n",
      " '# Load processor and model from Hugging Face\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " '\\n'\n",
      " '# Process input\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " '\\n'\n",
      " '# Predict with model\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '    outputs = model(**inputs)\\n'\n",
      " '    logits = outputs.logits\\n'\n",
      " '    predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Predicted label: {model.config.id2label[predicted_label]}\")')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(ans.model_description)\n",
    "pprint.pprint(ans.model_input_format)\n",
    "pprint.pprint(ans.model_output_format)\n",
    "pprint.pprint(ans.model_sample_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
