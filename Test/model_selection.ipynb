{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d3e60a",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "535f3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY='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'\n",
    "FIRECRAWL_API_KEY='fc-e36282d62d1c4ee8971ec0649f3862c7'\n",
    "BASE_URL='https://api.groq.com/openai/v1'\n",
    "MODEL=\"meta-llama/llama-4-scout-17b-16e-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42744f82",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "91bed7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/codespace/.python/current/lib/python312.zip', '/home/codespace/.python/current/lib/python3.12', '/home/codespace/.python/current/lib/python3.12/lib-dynload', '', '/home/codespace/.local/lib/python3.12/site-packages', '/home/codespace/.python/current/lib/python3.12/site-packages', '..', '..', '..']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "print(sys.path)\n",
    "from tools.fetching_description_from_huggingface import fetching_description_from_huggingface\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import Tool\n",
    "from firecrawl import FirecrawlApp\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36830718",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5827b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task = pd.read_csv(\"data/ISE - AutoCode Challenge 2 Public - Public task.csv\")\n",
    "df_model = pd.read_csv(\"data/ISE - AutoCode Challenge 2 Public - Model zoo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "22abc01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STT</th>\n",
       "      <th>Task</th>\n",
       "      <th>Validation data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt ta...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong th·ªùi ƒë·∫°i th√¥ng t...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒê√¢y l√† m·ªôt nhi·ªám v·ª• h·ªì...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒêi·ªÅu quan tr·ªçng l√† c√°c...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong nhi·ªÅu ·ª©ng d·ª•ng t...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>üêæ Story: Alice‚Äôs Magical Pet Photo Box\\r\\nAlic...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STT                                               Task Validation data\n",
       "0    1  \"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt ta...            Link\n",
       "1    2  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong th·ªùi ƒë·∫°i th√¥ng t...            Link\n",
       "2    3  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒê√¢y l√† m·ªôt nhi·ªám v·ª• h·ªì...            Link\n",
       "3    4  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒêi·ªÅu quan tr·ªçng l√† c√°c...            Link\n",
       "4    5  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong nhi·ªÅu ·ª©ng d·ª•ng t...            Link\n",
       "5    6  üêæ Story: Alice‚Äôs Magical Pet Photo Box\\r\\nAlic...            Link"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "697add1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = df_task[\"Task\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "953cae02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong nhi·ªÅu ·ª©ng d·ª•ng th·ª±c t·∫ø nh∆∞ t·ªïng h·ª£p n·ªôi dung h√¨nh ·∫£nh, ph√¢n lo·∫°i album, hay tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng n·ªôi dung cho ·∫£nh t·ªï h·ª£p, c·∫ßn x√°c ƒë·ªãnh ch·ªß ƒë·ªÅ ch√≠nh c·ªßa m·ªôt ·∫£nh gh√©p t·ª´ nhi·ªÅu ·∫£nh con. B√†i to√°n n√†y ƒë√≤i h·ªèi ·ª©ng d·ª•ng kh√¥ng ch·ªâ nh·∫≠n di·ªán ƒë∆∞·ª£c n·ªôi dung c·ªßa t·ª´ng ·∫£nh con, m√† c√≤n c·∫ßn x√°c ƒë·ªãnh ch·ªß ƒë·ªÅ chi·∫øm ∆∞u th·∫ø trong to√†n b·ªô ·∫£nh t·ªïng th·ªÉ.\\r\\n\\r\\nY√™u c·∫ßu c·ª• th·ªÉ c·∫ßn ƒë·∫°t ƒë∆∞·ª£c:\\r\\nM·ªói ƒë·∫ßu v√†o l√† m·ªôt ·∫£nh t·ªï h·ª£p, gh√©p t·ª´ 9 ·∫£nh m√†u nh·ªè (3x3).\\r\\nNhi·ªám v·ª• c·ªßa ·ª©ng d·ª•ng l√†:\\r\\nX√°c ƒë·ªãnh ch·ªß ƒë·ªÅ chi·∫øm s·ªë l∆∞·ª£ng nhi·ªÅu nh·∫•t trong 9 ·∫£nh con.\\r\\nTr∆∞·ªùng h·ª£p c√≥ nhi·ªÅu ch·ªß ƒë·ªÅ xu·∫•t hi·ªán v·ªõi t·∫ßn su·∫•t b·∫±ng nhau, c√≥ th·ªÉ ch·ªçn m·ªôt ch·ªß ƒë·ªÅ b·∫•t k·ª≥ trong s·ªë ƒë√≥ l√†m k·∫øt qu·∫£.\\r\\n\\r\\nƒê·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫ßu v√†o cho b√†i to√°n t·ªïng th·ªÉ:\\r\\nM·ªôt th∆∞ m·ª•c c√≥ t√™n l√† \"images\" ch·ª©a c√°c ·∫£nh m√†u RGB (288x288), ƒë∆∞·ª£c gh√©p t·ª´ 9 ·∫£nh RGB con (32x32) (3 h√†ng √ó 3 c·ªôt).\\r\\n\\r\\nƒê·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra mong mu·ªën cho b√†i to√°n t·ªïng th·ªÉ:\\r\\nFile output.csv m·ªói h√†ng l√† k·∫øt qu·∫£ d·ª± ƒëo√°n m·ªói ·∫£nh\\r\\nc√≥ c√°c c·ªôt:\\r\\nfile_name: t√™n file ·∫£nh\\r\\nprediction: nh√£n c·ªßa ·∫£nh. \\r\\nnh√£n c·ªßa ·∫£nh l√† m·ªôt nh√£n duy nh·∫•t ƒë·∫°i di·ªán cho ch·ªß ƒë·ªÅ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t trong 9 ·∫£nh con.\\r\\nC√°c ch·ªß ƒë·ªÅ sau n·∫±m trong danh s√°ch sau:\\r\\n       [\\'forest\\', \\'orchid\\', \\'spider\\', \\'fox\\', \\'caterpillar\\', \\'crocodile\\',\\r\\n       \\'whale\\', \\'road\\', \\'motorcycle\\', \\'tiger\\', \\'woman\\', \\'orange\\',\\r\\n       \\'elephant\\', \\'otter\\', \\'flatfish\\', \\'cup\\', \\'sea\\', \\'mountain\\', \\'cloud\\',\\r\\n       \\'willow_tree\\', \\'rabbit\\', \\'couch\\', \\'maple_tree\\', \\'wardrobe\\',\\r\\n       \\'mushroom\\', \\'plain\\', \\'telephone\\', \\'shrew\\', \\'bowl\\', \\'tractor\\',\\r\\n       \\'camel\\', \\'bus\\', \\'cattle\\', \\'house\\', \\'cockroach\\', \\'lawn_mower\\',\\r\\n       \\'oak_tree\\', \\'apple\\', \\'bicycle\\', \\'beetle\\', \\'palm_tree\\', \\'sunflower\\',\\r\\n       \\'poppy\\', \\'keyboard\\', \\'porcupine\\', \\'snake\\', \\'can\\', \\'pear\\', \\'shark\\',\\r\\n       \\'chimpanzee\\', \\'hamster\\', \\'train\\', \\'cra\\', \\'bear\\', \\'sweet_pepper\\',\\r\\n       \\'pine_tree\\', \\'bed\\', \\'tank\\', \\'beaver\\', \\'bridge\\', \\'girl\\',\\r\\n       \\'skyscraper\\', \\'wolf\\', \\'raccoon\\', \\'rose\\', \\'worm\\', \\'table\\',\\r\\n       \\'pickup_truck\\', \\'turtle\\', \\'lizard\\', \\'leopard\\', \\'streetcar\\',\\r\\n       \\'rocket\\', \\'man\\', \\'tulip\\', \\'lamp\\', \\'seal\\', \\'lion\\', \\'butterfly\\',\\r\\n       \\'kangaroo\\', \\'television\\', \\'ray\\', \\'chair\\', \\'skunk\\', \\'trout\\',\\r\\n       \\'possum\\', \\'mouse\\']'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b6909b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_list(df):\n",
    "    model_strings = []\n",
    "    for i, row in df.iterrows():\n",
    "        desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        url = str(row[2])\n",
    "        model_strings.append(f\"{i + 1} - {desc} [More info]({url})\")\n",
    "    return \"\\n\".join(model_strings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c40886",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "af3bcff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraper(url: str) -> str:\n",
    "    \"\"\"Get more details about the model using the model url\"\"\"\n",
    "    # scraper = FirecrawlApp(api_key=FIRECRAWL_API_KEY)\n",
    "    # scrape_status = scraper.scrape_url(\n",
    "    #     url,\n",
    "    #     formats=['markdown']\n",
    "    # )\n",
    "    # return scrape_status.markdown\n",
    "    result = fetching_description_from_huggingface(url)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "77f3a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraperParameters(BaseModel):\n",
    "    url: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b6a0e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = web_scraper('https://huggingface.co/thanhtlx/image_classification_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "52a9c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"web_scrapper\",\n",
    "        func=web_scraper,\n",
    "        description=\"Use to get more detail about the model\",\n",
    "        args_schema=WebScraperParameters\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c409f8b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "eca82430",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI( \n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6511a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tasks(BaseModel):\n",
    "    subtask_one: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 1\"\n",
    "    )\n",
    "    subtask_two: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 2\"\n",
    "    )\n",
    "    subtask_three: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 3\"\n",
    "    )\n",
    "    subtask_four: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 4\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0de6f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: HumanMessage | None\n",
    "    data: str | None\n",
    "    messages: Annotated[BaseMessage, add_messages]\n",
    "    subtasks: Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ef593964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_agent(state: AgentState) -> AgentState:\n",
    "    structured_llm = llm.with_structured_output(Tasks)\n",
    "    system_prompt = \"\"\"\n",
    "        You are an expert in task formulation. Given a machine learning task, you will\n",
    "        help divide the task into these subtasks:\n",
    "        - Subtask1: for identifying task description and classify the problem type.\n",
    "        - Subtask2: for identifying the problem requirements as described in the \"Y√™u c·∫ßu c·ª• th·ªÉ c·∫ßn ƒë·∫°t ƒë∆∞·ª£c\" (or other equivalent) section, including: problem requirements, expected input and expected output.\n",
    "        - Subtask3: for identifying the input format of the problem, as being described in the \"ƒê·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫ßu v√†o cho b√†i to√°n t·ªïng th·ªÉ\" (or other equivalent) section.\n",
    "        - Subtask4: for identifying the output format of the problem with the given classes, as being described in the \"ƒê·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra mong mu·ªën cho b√†i to√°n t·ªïng th·ªÉ\" (or other equivalent) section.\n",
    "\n",
    "        You **MUST** follow these guidelines:\n",
    "        - Do not mistake the output classes with the one described in the problem requirements. The output classes **MUST** contain the classes described in the \"ƒê·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra mong mu·ªën cho b√†i to√°n t·ªïng th·ªÉ\" section.\n",
    "        - Describe the subtasks as specific as possible.\n",
    "        \"\"\"\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    messages.append(state['task'])\n",
    "    result = structured_llm.invoke(messages)\n",
    "    state['subtasks'] = result\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39d4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "14b15c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"task\": HumanMessage(content=task),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "78675acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = classify_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bf9d721c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X√°c ƒë·ªãnh b√†i to√°n v√† ph√¢n lo·∫°i lo·∫°i b√†i to√°n. B√†i to√°n n√†y l√† m·ªôt b√†i to√°n '\n",
      " 'ph√¢n lo·∫°i ·∫£nh, c·ª• th·ªÉ l√† x√°c ƒë·ªãnh ch·ªß ƒë·ªÅ chi·∫øm ∆∞u th·∫ø trong m·ªôt ·∫£nh gh√©p t·ª´ '\n",
      " 'nhi·ªÅu ·∫£nh con.')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(state['subtasks'].subtask_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c0e51e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X√°c ƒë·ªãnh y√™u c·∫ßu c·ªßa b√†i to√°n. Y√™u c·∫ßu c·ªßa b√†i to√°n l√† x√°c ƒë·ªãnh ch·ªß ƒë·ªÅ chi·∫øm '\n",
      " 's·ªë l∆∞·ª£ng nhi·ªÅu nh·∫•t trong 9 ·∫£nh con c·ªßa m·ªôt ·∫£nh t·ªï h·ª£p. N·∫øu c√≥ nhi·ªÅu ch·ªß ƒë·ªÅ '\n",
      " 'xu·∫•t hi·ªán v·ªõi t·∫ßn su·∫•t b·∫±ng nhau, c√≥ th·ªÉ ch·ªçn m·ªôt ch·ªß ƒë·ªÅ b·∫•t k·ª≥ trong s·ªë ƒë√≥ '\n",
      " 'l√†m k·∫øt qu·∫£. ƒê·∫ßu v√†o l√† m·ªôt ·∫£nh t·ªï h·ª£p gh√©p t·ª´ 9 ·∫£nh m√†u nh·ªè (3x3). ƒê·∫ßu ra '\n",
      " 'l√† m·ªôt nh√£n duy nh·∫•t ƒë·∫°i di·ªán cho ch·ªß ƒë·ªÅ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t trong 9 ·∫£nh '\n",
      " 'con.')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(state['subtasks'].subtask_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "e38609cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X√°c ƒë·ªãnh ƒë·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫ßu v√†o. D·ªØ li·ªáu ƒë·∫ßu v√†o l√† m·ªôt th∆∞ m·ª•c c√≥ t√™n l√† '\n",
      " \"'images' ch·ª©a c√°c ·∫£nh m√†u RGB (288x288), ƒë∆∞·ª£c gh√©p t·ª´ 9 ·∫£nh RGB con (32x32) \"\n",
      " '(3 h√†ng √ó3 c·ªôt).')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(state['subtasks'].subtask_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9b1ddbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"X√°c ƒë·ªãnh ƒë·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra. K·∫øt qu·∫£ ƒë·∫ßu ra l√† m·ªôt file 'output.csv' \"\n",
      " \"v·ªõi m·ªói h√†ng l√† k·∫øt qu·∫£ d·ª± ƒëo√°n m·ªói ·∫£nh, c√≥ c√°c c·ªôt: 'file_name' (t√™n file \"\n",
      " \"·∫£nh) v√† 'prediction' (nh√£n c·ªßa ·∫£nh). Nh√£n c·ªßa ·∫£nh l√† m·ªôt nh√£n duy nh·∫•t ƒë·∫°i \"\n",
      " 'di·ªán cho ch·ªß ƒë·ªÅ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t trong 9 ·∫£nh con. C√°c ch·ªß ƒë·ªÅ c√≥ th·ªÉ l√† '\n",
      " \"m·ªôt trong c√°c nh√£n sau: ['forest', 'orchid', 'spider', 'fox', 'caterpillar', \"\n",
      " \"'crocodile', 'whale', 'road', 'motorcycle', 'tiger', 'woman', 'orange', \"\n",
      " \"'elephant', 'otter', 'flatfish', 'cup', 'sea', 'mountain', 'cloud', \"\n",
      " \"'willow_tree', 'rabbit', 'couch', 'maple_tree', 'wardrobe', 'mushroom', \"\n",
      " \"'plain', 'telephone', 'shrew', 'bowl', 'tractor', 'camel', 'bus', 'cattle', \"\n",
      " \"'house', 'cockroach', 'lawn_mower', 'oak_tree', 'apple', 'bicycle', \"\n",
      " \"'beetle', 'palm_tree', 'sunflower', 'poppy', 'keyboard', 'porcupine', \"\n",
      " \"'snake', 'can', 'pear', 'shark', 'chimpanzee', 'hamster', 'train', 'cra', \"\n",
      " \"'bear', 'sweet_pepper', 'pine_tree', 'bed', 'tank', 'beaver', 'bridge', \"\n",
      " \"'girl', 'skyscraper', 'wolf', 'raccoon', 'rose', 'worm', 'table', \"\n",
      " \"'pickup_truck', 'turtle', 'lizard', 'leopard', 'streetcar', 'rocket', 'man', \"\n",
      " \"'tulip', 'lamp', 'seal', 'lion', 'butterfly', 'kangaroo', 'television', \"\n",
      " \"'ray', 'chair', 'skunk', 'trout', 'possum', 'mouse']\")\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(state['subtasks'].subtask_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab3d7f",
   "metadata": {},
   "source": [
    "# Modeling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f656ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInfo(BaseModel):\n",
    "    model: str = Field(..., description=\"Model name\")\n",
    "    url: str = Field(..., description=\"URL to the model\")\n",
    "\n",
    "class ModelSelection(BaseModel):\n",
    "    models: List[ModelInfo] = Field(..., description=\"List of available models with name and URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "18dcc46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_parser = PydanticOutputParser(pydantic_object=ModelSelection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "de4ed792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the suitable model for a given task.\n",
    "                    Given:\n",
    "                    - A user description about the task\n",
    "                    - A list of available model (names and links)\n",
    "                    Your job is to choose ALL suitable models provided in the list for user's specific tasks.\n",
    "                    Return the answer in the format\n",
    "                    {structured_output}\n",
    "                    Here are the list of model:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]).partial(structured_output=model_selection_parser.get_format_instructions())\n",
    "\n",
    "    chain = prompt | llm | model_selection_parser\n",
    "\n",
    "    result = chain.invoke({\"input\": task, \"model_list\": format_model_list(df_model)})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fd097a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30944/7332665.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
      "/tmp/ipykernel_30944/7332665.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  url = str(row[2])\n"
     ]
    }
   ],
   "source": [
    "res = model_selection_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "03a8819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelInfo(model='Ph√¢n lo·∫°i h√¨nh ·∫£nh', url='https://huggingface.co/zhaospei/Model_7'),\n",
      " ModelInfo(model='Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh 100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)', url='https://huggingface.co/zhaospei/Model_6'),\n",
      " ModelInfo(model='Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥', url='https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit')]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3b0dc638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_using_tool_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the **Best-fit model** for a given task.\n",
    "                    Given:\n",
    "                    - A user task description\n",
    "                    - A list of available models (names and url)\n",
    "                    - A tool that fetches model details from a provided URL (it help gathers model description, input/output format, code sample usage)\n",
    "\n",
    "                    Your job is to:\n",
    "                    1. Use the **provided tool** to retrieve real details about each candidate model:\n",
    "                        - ‚úÖ Model description\n",
    "                        - üì• Input format\n",
    "                        - üì§ Output format\n",
    "                        - üõ†Ô∏è Library requirements\n",
    "                        - üß™ Code sample (usage code snippet)\n",
    "                    2. Select the best model for user given task based on the data that the tool gives you.\n",
    "\n",
    "                    ‚ö†Ô∏è VERY IMPORTANT RULES\n",
    "\n",
    "                    - ‚ùå DO NOT assume or invent any part of the model's description, input/output format, or code\n",
    "                    - ‚ùå DO NOT generate fake code or use your own knowledge about the model\n",
    "                    - ‚úÖ ONLY use the **actual output** returned from the tool\n",
    "                    - ‚úÖ Include tool content in your final answer exactly as returned (especially code)\n",
    "\n",
    "                    ## üß† Output Structure (Final Answer)\n",
    "\n",
    "                    After retrieving tool results, choose ONE best-fit model and output the result in the following format:\n",
    "\n",
    "                    **‚úÖ Model name and link**  \n",
    "                    `<model name>` ‚Äî `<link>`\n",
    "\n",
    "                    **üìù Description (from tool):**  \n",
    "                    <model description>\n",
    "\n",
    "                    **üì• Input format (from tool):**  \n",
    "                    <description of expected input>\n",
    "\n",
    "                    **üì§ Output format (from tool):**  \n",
    "                    <description of model output>\n",
    "\n",
    "                    **üõ†Ô∏è Library Requirements (from tool) **\n",
    "                    <requirements to use the model>\n",
    "\n",
    "                    **üß™ Example code (from tool):**  \n",
    "                    ```python\n",
    "                    <exact code snippet from tool>\n",
    "\n",
    "                    Here are the list of models and there corresponding URL:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"ai\", \"{agent_scratchpad}\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "    task_input = state[\"task\"].content if isinstance(state[\"task\"], SystemMessage) else state[\"task\"]\n",
    "\n",
    "    # result = executor.invoke({\"input\": task_input})\n",
    "    result = executor.invoke({\"input\": task_input, \"model_list\": res.models})\n",
    "\n",
    "    return {\"output\": result[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e8b0f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_7'}`\n",
      "responded: To solve this task, I need to analyze the provided models and select the best one that fits the given task description. The task involves determining the dominant theme in a collage image made of 9 smaller images.\n",
      "\n",
      "First, I will use the `web_scrapper` function to gather detailed information about each model. This will help in understanding their capabilities and determining which one is best suited for the task.\n",
      "\n",
      "I will start by scraping information about the first model:\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "üñºÔ∏è M√¥ h√¨nh Ph√¢n lo·∫°i ·∫¢nh - ViT Fine-tuned tr√™n CIFAR-10\n",
      "üìù M√¥ t·∫£\n",
      "ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh kh√°c tr√™n t·∫≠p d·ªØ li·ªáu CIFAR-10. M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o 10 l·ªõp kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.\n",
      "üìå Nhi·ªám v·ª•\n",
      "Lo·∫°i b√†i to√°n: Ph√¢n lo·∫°i ·∫£nh (Image Classification)\n",
      "S·ªë l·ªõp: 10 (T∆∞∆°ng ·ª©ng nh√£n) CIFAR-10\n",
      "üì• ƒê·∫ßu v√†o\n",
      "ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB\n",
      "K√≠ch th∆∞·ªõc ·∫£nh: 224x224 pixels\n",
      "üì§ ƒê·∫ßu ra\n",
      "ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits)\n",
      "Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch th∆∞·ªõc [batch_size, 10]\n",
      "√ù nghƒ©a: X√°c su·∫•t d·ª± ƒëo√°n cho t·ª´ng l·ªõp trong 10 l·ªõp c·ªßa CIFAR-10\n",
      "üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt b·∫±ng:\n",
      "pip install transformers torch torchvision\n",
      "üß™ C√°ch s·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "D∆∞·ªõi ƒë√¢y l√† v√≠ d·ª• v·ªÅ c√°ch s·ª≠ d·ª•ng m√¥ h√¨nh ƒë·ªÉ ph√¢n lo·∫°i m·ªôt ·∫£nh:\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "# T·∫£i ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "# T·∫£i processor v√† m√¥ h√¨nh t·ª´ Hugging Face\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_7\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_7\")\n",
      "# X·ª≠ l√Ω ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_label = logits.argmax(-1).item()\n",
      "print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_6'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "üñºÔ∏è M√¥ h√¨nh Ph√¢n lo·∫°i ·∫¢nh - ViT Fine-tuned tr√™n CIFAR-100\n",
      "üìù M√¥ t·∫£\n",
      "ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh g·ªëc google/vit-base-patch16-224-in21k tr√™n t·∫≠p d·ªØ li·ªáu CIFAR-100. M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o 100 l·ªõp kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.\n",
      "üìå Nhi·ªám v·ª•\n",
      "Lo·∫°i b√†i to√°n: Ph√¢n lo·∫°i ·∫£nh (Image Classification)\n",
      "S·ªë l·ªõp: 100\n",
      "T·∫≠p d·ªØ li·ªáu: CIFAR-100\n",
      "üì• ƒê·∫ßu v√†o\n",
      "ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB\n",
      "K√≠ch th∆∞·ªõc ·∫£nh:  224x224\n",
      "üì§ ƒê·∫ßu ra\n",
      "ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits)\n",
      "Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch th∆∞·ªõc [batch_size, 100]\n",
      "√ù nghƒ©a: X√°c su·∫•t d·ª± ƒëo√°n cho t·ª´ng l·ªõp trong 100 l·ªõp c·ªßa CIFAR-100\n",
      "üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt b·∫±ng:\n",
      "pip install transformers torch torchvision\n",
      "üß™ C√°ch s·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "D∆∞·ªõi ƒë√¢y l√† v√≠ d·ª• v·ªÅ c√°ch s·ª≠ d·ª•ng m√¥ h√¨nh ƒë·ªÉ ph√¢n lo·∫°i m·ªôt ·∫£nh:\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "# T·∫£i ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "# T·∫£i processor v√† m√¥ h√¨nh t·ª´ Hugging Face\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\n",
      "# X·ª≠ l√Ω ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_label = logits.argmax(-1).item()\n",
      "print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit'}`\n",
      "responded: To solve this task, I need to analyze the provided models and select the best one that fits the given task description. The task involves determining the dominant theme in a collage image made of 9 smaller images.\n",
      "\n",
      "I have already gathered detailed information about the first two models:\n",
      "\n",
      "**Model 1: Ph√¢n lo·∫°i h√¨nh ·∫£nh ‚Äî https://huggingface.co/zhaospei/Model_7**\n",
      "- Description: Vision Transformer (ViT) fine-tuned on CIFAR-10 dataset for image classification into 10 classes.\n",
      "- Input format: RGB images of size 224x224 pixels.\n",
      "- Output format: Probabilities for each class (logits) in a tensor of size [batch_size, 10].\n",
      "- Library requirements: transformers, torch, torchvision.\n",
      "- Example code: Provided in the tool output.\n",
      "\n",
      "**Model 2: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained) ‚Äî https://huggingface.co/zhaospei/Model_6**\n",
      "- Description: Vision Transformer (ViT) fine-tuned on CIFAR-100 dataset for image classification into 100 classes.\n",
      "- Input format: RGB images of size 224x224 pixels.\n",
      "- Output format: Probabilities for each class (logits) in a tensor of size [batch_size, 100].\n",
      "- Library requirements: transformers, torch, torchvision.\n",
      "- Example code: Provided in the tool output.\n",
      "\n",
      "Now, I will gather information about the third model:\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "Model made by notebook first posted in my Kaggle.\n",
      "Model Motivation\n",
      "Recently, someone asked me if you can classify dog images into their respective dog breeds instead just differentiating from cats vs dogs like my last notebook. I say YES!\n",
      "Due to the complexity of the problem, we will be using the most advanced computer vision architecture released in the 2020 Google paper, the Vision Transformer.\n",
      "The difference between the Vision Transformer and the traditional Convolutional Neural Network (CNN) is how it treats an image. In Vision Transformers, we take the input as a patch of the original image, say 16 x 16, and feed in into the Transformer as a sequence with positional embeddings and self-attention, while in the Convolutional Neural Network (CNN), we use the same patch of original image as an input, but use convolutions and pooling layers as inductive biases. What this means is that Vision Transformer can use it's judgement to attend any particular patch of the image in a global fashion using it's self-attention mechanism without having us to guide the neural network like a CNN with local centering/cropping/bounding box our images to help its convolutions. \n",
      "This allows the Vision Transformer architecture to be more flexible and scalable in nature, allowing us to create foundation models in computer vision, similar to the NLP foundational models like BERT and GPT, with pre-training self-supervised/supervised on massive amount of image data that would generalize to different computer vision tasks such as image classification, recognition, segmentation, etc. This cross-pollination helps us move closer towards the goal of Artificial General Intelligence.\n",
      "One thing about Vision Transformers are it has weaker inductive biases compared to Convolutional Neural Networks that enables it's scalability and flexibility. This feature/bug depending on who you ask will require most well-performing pre-trained models to require more data despite having less parameters compared to it's CNN counterparts.\n",
      "Luckily, in this model, we will use a Vision Transformer from Google hosted at HuggingFace pre-trained on the ImageNet-21k dataset (14 million images, 21k classes) with 16x16 patches, 224x224 resolution to bypass that data limitation. We will be fine-tuning this model to our \"small\" dog breeds dataset of around 20 thousand images from the Stanford Dogs dataset imported by Jessica Li into Kaggle to classify dog images into 120 types of dog breeds!\n",
      "Model Description\n",
      "This model is finetuned using the Google Vision Transformer (vit-base-patch16-224-in21k) on the Stanford Dogs dataset in Kaggle to classify dog images into 120 types of dog breeds.\n",
      "Intended Uses & Limitations\n",
      "You can use this finetuned model to classify images of dogs only and dog breeds that are in the dataset.\n",
      "How to Use\n",
      "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
      "import PIL\n",
      "import requests\n",
      "url = \"https://upload.wikimedia.org/wikipedia/commons/5/55/Beagle_600.jpg\"\n",
      "image = PIL.Image.open(requests.get(url, stream=True).raw)\n",
      "image_processor = AutoImageProcessor.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "model = AutoModelForImageClassification.from_pretrained(\"wesleyacheng/dog-breeds-multiclass-image-classification-with-vit\")\n",
      "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
      "outputs = model(**inputs)\n",
      "logits = outputs.logits\n",
      "# model predicts one of the 120 Stanford dog breeds classes\n",
      "predicted_class_idx = logits.argmax(-1).item()\n",
      "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
      "Model Training Metrics\n",
      "Epoch\n",
      "Top-1 Accuracy\n",
      "Top-3 Accuracy\n",
      "Top-5 Accuracy\n",
      "Macro F1\n",
      "1\n",
      "79.8%\n",
      "95.1%\n",
      "97.5%\n",
      "77.2%\n",
      "2\n",
      "83.8%\n",
      "96.7%\n",
      "98.2%\n",
      "81.9%\n",
      "3\n",
      "84.8%\n",
      "96.7%\n",
      "98.3%\n",
      "83.4%\n",
      "Model Evaluation Metrics\n",
      "Top-1 Accuracy\n",
      "Top-3 Accuracy\n",
      "Top-5 Accuracy\n",
      "Macro F1\n",
      "84.0%\n",
      "97.1%\n",
      "98.7%\n",
      "83.0%\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mTo solve this task, I need to analyze the provided models and select the best one that fits the given task description. The task involves determining the dominant theme in a collage image made of 9 smaller images.\n",
      "\n",
      "I have already gathered detailed information about the three models:\n",
      "\n",
      "**Model 1: Ph√¢n lo·∫°i h√¨nh ·∫£nh ‚Äî https://huggingface.co/zhaospei/Model_7**\n",
      "- Description: Vision Transformer (ViT) fine-tuned on CIFAR-10 dataset for image classification into 10 classes.\n",
      "- Input format: RGB images of size 224x224 pixels.\n",
      "- Output format: Probabilities for each class (logits) in a tensor of size [batch_size, 10].\n",
      "- Library requirements: transformers, torch, torchvision.\n",
      "- Example code: Provided in the tool output.\n",
      "\n",
      "**Model 2: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained) ‚Äî https://huggingface.co/zhaospei/Model_6**\n",
      "- Description: Vision Transformer (ViT) fine-tuned on CIFAR-100 dataset for image classification into 100 classes.\n",
      "- Input format: RGB images of size 224x224 pixels.\n",
      "- Output format: Probabilities for each class (logits) in a tensor of size [batch_size, 100].\n",
      "- Library requirements: transformers, torch, torchvision.\n",
      "- Example code: Provided in the tool output.\n",
      "\n",
      "**Model 3: Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥ ‚Äî https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit**\n",
      "- Description: Vision Transformer (ViT) fine-tuned on Stanford Dogs dataset to classify dog images into 120 types of dog breeds.\n",
      "- Input format: RGB images.\n",
      "- Output format: Probabilities for each class (logits).\n",
      "- Library requirements: transformers, torch.\n",
      "- Example code: Provided in the tool output.\n",
      "\n",
      "The task requires classifying a collage image made of 9 smaller images into one of the given themes. The models provided are primarily for image classification tasks but differ in their specific applications and class numbers.\n",
      "\n",
      "Given the task description, **Model 2: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained) ‚Äî https://huggingface.co/zhaospei/Model_6** seems to be the most suitable. This is because:\n",
      "- It can classify images into 100 classes, which is closer to the number of themes provided (around 100).\n",
      "- The input format matches the requirement (RGB images of size 224x224 pixels).\n",
      "\n",
      "Here is the final answer:\n",
      "\n",
      "**‚úÖ Model name and link**  \n",
      "`Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)` ‚Äî `https://huggingface.co/zhaospei/Model_6`\n",
      "\n",
      "**üìù Description (from tool):**  \n",
      "`ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh g·ªëc google/vit-base-patch16-224-in21k tr√™n t·∫≠p d·ªØ li·ªáu CIFAR-100. M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o100 l·ªõp kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.`\n",
      "\n",
      "**üì• Input format (from tool):**  \n",
      "`ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB, K√≠ch th∆∞·ªõc ·∫£nh:224x224`\n",
      "\n",
      "**üì§ Output format (from tool):**  \n",
      "`ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits), Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch th∆∞·ªõc [batch_size,100]`\n",
      "\n",
      "**üõ†Ô∏è Library Requirements (from tool) **  \n",
      "`C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt b·∫±ng: pip install transformers torch torchvision`\n",
      "\n",
      "**üß™ Example code (from tool):**  \n",
      "```python\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "\n",
      "# T·∫£i ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "\n",
      "# T·∫£i processor v√† m√¥ h√¨nh t·ª´ Hugging Face\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\n",
      "\n",
      "# X·ª≠ l√Ω ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      " outputs = model(**inputs)\n",
      " logits = outputs.logits\n",
      " predicted_label = logits.argmax(-1).item()\n",
      "print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res2 = model_selection_using_tool_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cb681a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('To solve this task, I need to analyze the provided models and select the '\n",
      " 'best one that fits the given task description. The task involves determining '\n",
      " 'the dominant theme in a collage image made of 9 smaller images.\\n'\n",
      " '\\n'\n",
      " 'I have already gathered detailed information about the three models:\\n'\n",
      " '\\n'\n",
      " '**Model 1: Ph√¢n lo·∫°i h√¨nh ·∫£nh ‚Äî https://huggingface.co/zhaospei/Model_7**\\n'\n",
      " '- Description: Vision Transformer (ViT) fine-tuned on CIFAR-10 dataset for '\n",
      " 'image classification into 10 classes.\\n'\n",
      " '- Input format: RGB images of size 224x224 pixels.\\n'\n",
      " '- Output format: Probabilities for each class (logits) in a tensor of size '\n",
      " '[batch_size, 10].\\n'\n",
      " '- Library requirements: transformers, torch, torchvision.\\n'\n",
      " '- Example code: Provided in the tool output.\\n'\n",
      " '\\n'\n",
      " '**Model 2: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained) '\n",
      " '‚Äî https://huggingface.co/zhaospei/Model_6**\\n'\n",
      " '- Description: Vision Transformer (ViT) fine-tuned on CIFAR-100 dataset for '\n",
      " 'image classification into 100 classes.\\n'\n",
      " '- Input format: RGB images of size 224x224 pixels.\\n'\n",
      " '- Output format: Probabilities for each class (logits) in a tensor of size '\n",
      " '[batch_size, 100].\\n'\n",
      " '- Library requirements: transformers, torch, torchvision.\\n'\n",
      " '- Example code: Provided in the tool output.\\n'\n",
      " '\\n'\n",
      " '**Model 3: Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥ ‚Äî '\n",
      " 'https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit**\\n'\n",
      " '- Description: Vision Transformer (ViT) fine-tuned on Stanford Dogs dataset '\n",
      " 'to classify dog images into 120 types of dog breeds.\\n'\n",
      " '- Input format: RGB images.\\n'\n",
      " '- Output format: Probabilities for each class (logits).\\n'\n",
      " '- Library requirements: transformers, torch.\\n'\n",
      " '- Example code: Provided in the tool output.\\n'\n",
      " '\\n'\n",
      " 'The task requires classifying a collage image made of 9 smaller images into '\n",
      " 'one of the given themes. The models provided are primarily for image '\n",
      " 'classification tasks but differ in their specific applications and class '\n",
      " 'numbers.\\n'\n",
      " '\\n'\n",
      " 'Given the task description, **Model 2: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi '\n",
      " 't∆∞·ª£ng chi ti·∫øt (fine-grained) ‚Äî https://huggingface.co/zhaospei/Model_6** '\n",
      " 'seems to be the most suitable. This is because:\\n'\n",
      " '- It can classify images into 100 classes, which is closer to the number of '\n",
      " 'themes provided (around 100).\\n'\n",
      " '- The input format matches the requirement (RGB images of size 224x224 '\n",
      " 'pixels).\\n'\n",
      " '\\n'\n",
      " 'Here is the final answer:\\n'\n",
      " '\\n'\n",
      " '**‚úÖ Model name and link**  \\n'\n",
      " '`Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)` ‚Äî '\n",
      " '`https://huggingface.co/zhaospei/Model_6`\\n'\n",
      " '\\n'\n",
      " '**üìù Description (from tool):**  \\n'\n",
      " '`ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh g·ªëc '\n",
      " 'google/vit-base-patch16-224-in21k tr√™n t·∫≠p d·ªØ li·ªáu CIFAR-100. M√¥ h√¨nh ƒë∆∞·ª£c '\n",
      " 'hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o100 l·ªõp kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt '\n",
      " 'danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.`\\n'\n",
      " '\\n'\n",
      " '**üì• Input format (from tool):**  \\n'\n",
      " '`ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB, K√≠ch th∆∞·ªõc ·∫£nh:224x224`\\n'\n",
      " '\\n'\n",
      " '**üì§ Output format (from tool):**  \\n'\n",
      " '`ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits), Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch '\n",
      " 'th∆∞·ªõc [batch_size,100]`\\n'\n",
      " '\\n'\n",
      " '**üõ†Ô∏è Library Requirements (from tool) **  \\n'\n",
      " '`C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt b·∫±ng: pip install transformers torch '\n",
      " 'torchvision`\\n'\n",
      " '\\n'\n",
      " '**üß™ Example code (from tool):**  \\n'\n",
      " '```python\\n'\n",
      " 'import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " '# T·∫£i ·∫£nh c·∫ßn ph√¢n lo·∫°i\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " '\\n'\n",
      " '# T·∫£i processor v√† m√¥ h√¨nh t·ª´ Hugging Face\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " '\\n'\n",
      " '# X·ª≠ l√Ω ƒë·∫ßu v√†o\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " '\\n'\n",
      " '# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " ' outputs = model(**inputs)\\n'\n",
      " ' logits = outputs.logits\\n'\n",
      " ' predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res2['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874a1c7",
   "metadata": {},
   "source": [
    "## Output Parser Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3658cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelingOutput(BaseModel):\n",
    "    model_description: str = Field(\n",
    "        ...,\n",
    "        description=\"The detailed description of the model.\"\n",
    "    )\n",
    "    model_input_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed input format\"\n",
    "    )\n",
    "    model_output_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed output format\"\n",
    "    )\n",
    "    model_requirements: str = Field(\n",
    "        ...,\n",
    "        description=\"Requirement libraries need to install to use the model.\"\n",
    "    )\n",
    "    model_sample_code: str = Field(\n",
    "        ...,\n",
    "        description=\"Example code to use the model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0b610e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=ModelingOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "707428ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser_llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "5a1f51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_parser_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a smart AI tasked with extracting structured technical details about a machine learning model from a reasoning result.\n",
    "                    You are given a detailed text description about a model.\n",
    "                    Your goal is to fill the following fields based on the text:\n",
    "\n",
    "                    - `model_description`: A detailed explanation of what the model is and what it does.\n",
    "                    - `model_input_format`: A detailed description of the model's input format, including dimensions, data types, and expected preprocessing if mentioned.\n",
    "                    - `model_output_format`: A detailed description of the output format including dimensions, data types, label name and its meaning.\n",
    "                    - `model_requirements`: A detailed description about the requirements needed to be sastified in order to use the model\n",
    "                    - `model_sample_code`: A sample code on how to use the model.\n",
    "\n",
    "                    You MUSTN'T create any data on your own, only using the data provided in the text.\n",
    "\n",
    "                    Return the data as a JSON object matching the following structure:\n",
    "                    {formatted_output}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        ('system', system_prompt),\n",
    "        ('human', \"{input}\")\n",
    "    ]).partial(formatted_output=parser.get_format_instructions())\n",
    "    chain = prompt | llm | parser\n",
    "    response = chain.invoke({\"input\": res2['output']})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "0d82f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = output_parser_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "583a7bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('To solve this task, I need to analyze the provided models and select the '\n",
      " 'best one that fits the given task description. The task involves determining '\n",
      " 'the dominant theme in a collage image made of 9 smaller images.\\n'\n",
      " '\\n'\n",
      " 'I have already gathered detailed information about the three models:\\n'\n",
      " '\\n'\n",
      " '**Model 1: Ph√¢n lo·∫°i h√¨nh ·∫£nh ‚Äî https://huggingface.co/zhaospei/Model_7**\\n'\n",
      " '- Description: Vision Transformer (ViT) fine-tuned on CIFAR-10 dataset for '\n",
      " 'image classification into 10 classes.\\n'\n",
      " '- Input format: RGB images of size 224x224 pixels.\\n'\n",
      " '- Output format: Probabilities for each class (logits) in a tensor of size '\n",
      " '[batch_size, 10].\\n'\n",
      " '- Library requirements: transformers, torch, torchvision.\\n'\n",
      " '- Example code: Provided in the tool output.\\n'\n",
      " '\\n'\n",
      " '**Model 2: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained) '\n",
      " '‚Äî https://huggingface.co/zhaospei/Model_6**\\n'\n",
      " '- Description: Vision Transformer (ViT) fine-tuned on CIFAR-100 dataset for '\n",
      " 'image classification into 100 classes.\\n'\n",
      " '- Input format: RGB images of size 224x224 pixels.\\n'\n",
      " '- Output format: Probabilities for each class (logits) in a tensor of size '\n",
      " '[batch_size, 100].\\n'\n",
      " '- Library requirements: transformers, torch, torchvision.\\n'\n",
      " '- Example code: Provided in the tool output.\\n'\n",
      " '\\n'\n",
      " '**Model 3: Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥ ‚Äî '\n",
      " 'https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit**\\n'\n",
      " '- Description: Vision Transformer (ViT) fine-tuned on Stanford Dogs dataset '\n",
      " 'to classify dog images into 120 types of dog breeds.\\n'\n",
      " '- Input format: RGB images.\\n'\n",
      " '- Output format: Probabilities for each class (logits).\\n'\n",
      " '- Library requirements: transformers, torch.\\n'\n",
      " '- Example code: Provided in the tool output.\\n'\n",
      " '\\n'\n",
      " 'The task requires classifying a collage image made of 9 smaller images into '\n",
      " 'one of the given themes. The models provided are primarily for image '\n",
      " 'classification tasks but differ in their specific applications and class '\n",
      " 'numbers.\\n'\n",
      " '\\n'\n",
      " 'Given the task description, **Model 2: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi '\n",
      " 't∆∞·ª£ng chi ti·∫øt (fine-grained) ‚Äî https://huggingface.co/zhaospei/Model_6** '\n",
      " 'seems to be the most suitable. This is because:\\n'\n",
      " '- It can classify images into 100 classes, which is closer to the number of '\n",
      " 'themes provided (around 100).\\n'\n",
      " '- The input format matches the requirement (RGB images of size 224x224 '\n",
      " 'pixels).\\n'\n",
      " '\\n'\n",
      " 'Here is the final answer:\\n'\n",
      " '\\n'\n",
      " '**‚úÖ Model name and link**  \\n'\n",
      " '`Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)` ‚Äî '\n",
      " '`https://huggingface.co/zhaospei/Model_6`\\n'\n",
      " '\\n'\n",
      " '**üìù Description (from tool):**  \\n'\n",
      " '`ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh g·ªëc '\n",
      " 'google/vit-base-patch16-224-in21k tr√™n t·∫≠p d·ªØ li·ªáu CIFAR-100. M√¥ h√¨nh ƒë∆∞·ª£c '\n",
      " 'hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o100 l·ªõp kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt '\n",
      " 'danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.`\\n'\n",
      " '\\n'\n",
      " '**üì• Input format (from tool):**  \\n'\n",
      " '`ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB, K√≠ch th∆∞·ªõc ·∫£nh:224x224`\\n'\n",
      " '\\n'\n",
      " '**üì§ Output format (from tool):**  \\n'\n",
      " '`ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits), Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch '\n",
      " 'th∆∞·ªõc [batch_size,100]`\\n'\n",
      " '\\n'\n",
      " '**üõ†Ô∏è Library Requirements (from tool) **  \\n'\n",
      " '`C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt b·∫±ng: pip install transformers torch '\n",
      " 'torchvision`\\n'\n",
      " '\\n'\n",
      " '**üß™ Example code (from tool):**  \\n'\n",
      " '```python\\n'\n",
      " 'import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " '# T·∫£i ·∫£nh c·∫ßn ph√¢n lo·∫°i\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " '\\n'\n",
      " '# T·∫£i processor v√† m√¥ h√¨nh t·ª´ Hugging Face\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " '\\n'\n",
      " '# X·ª≠ l√Ω ƒë·∫ßu v√†o\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " '\\n'\n",
      " '# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " ' outputs = model(**inputs)\\n'\n",
      " ' logits = outputs.logits\\n'\n",
      " ' predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res2['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "530f5c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This is a Vision Transformer (ViT) model fine-tuned from the original '\n",
      " 'google/vit-base-patch16-224-in21k on the CIFAR-100 dataset. The model is '\n",
      " 'trained to classify images into 100 different classes, each representing a '\n",
      " 'specific object category.')\n",
      "'RGB images of size 224x224 pixels.'\n",
      "'Probabilities for each class (logits) in a tensor of size [batch_size,100].'\n",
      "('Install required libraries by running: pip install transformers torch '\n",
      " 'torchvision')\n",
      "('import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " '# Load image to classify\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " '\\n'\n",
      " '# Load processor and model from Hugging Face\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " '\\n'\n",
      " '# Process input\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " '\\n'\n",
      " '# Predict with model\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '    outputs = model(**inputs)\\n'\n",
      " '    logits = outputs.logits\\n'\n",
      " '    predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Predicted label: {model.config.id2label[predicted_label]}\")')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(ans.model_description)\n",
    "pprint.pprint(ans.model_input_format)\n",
    "pprint.pprint(ans.model_output_format)\n",
    "pprint.pprint(ans.model_requirements)\n",
    "pprint.pprint(ans.model_sample_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "77a72e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_list(df):\n",
    "    model_strings = []\n",
    "    for i, row in df.iterrows():\n",
    "        desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        url = str(row[2])\n",
    "        model_strings.append(f\"{i + 1} - {desc} [More info]({url})\")\n",
    "    return \"\\n\".join(model_strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3643728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentCodingState(TypedDict):\n",
    "    problem_description: str | None\n",
    "    problem_input_description: str | None\n",
    "    problem_output_description: str | None\n",
    "    model_description: str | None\n",
    "    model_input_format: str | None\n",
    "    model_output_format: str | None\n",
    "    model_requirements: str | None\n",
    "    model_sample_code: str | None\n",
    "    output_classes: str | None\n",
    "    code: str | None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "fbc5157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = r\"\"\"\n",
    "    You are a specialist in Machine Learning. Your task is to generate a *fully functional with all necessary imports and dependencies* codebase in *Python* that can be executed flawlessly.\n",
    "\n",
    "    You will be provided with:\n",
    "    - A problem description\n",
    "    - An input specification\n",
    "    - An output specification\n",
    "    - A model description\n",
    "\n",
    "    ### Input:\n",
    "    - Problem description: {problem_description}\n",
    "    - Problem input specification: {problem_input_description}\n",
    "    - Problem output specification: {problem_output_description}\n",
    "    - ML model description: {model_description}\n",
    "    - ML model input format: {model_input_format}\n",
    "    - ML model output format: {model_output_format}\n",
    "    - ML model requirements: {model_requirements}\n",
    "    - ML model sample code: {model_sample_code}\n",
    "    - Output classes: {output_classes}\n",
    "\n",
    "    ### Guidelines\n",
    "\n",
    "    You *must* strictly follow the following guidelines:\n",
    "    - The preprocessing step should be suitable for the data type.\n",
    "    - The postprocessing step should notices the differences between the data returned by the model and the output requirements. You must extract and use the exact class labels as defined in the output specification.\n",
    "    - Do *not invent new labels or translate* the class names. Use them exactly as given.\n",
    "    - You *must* make sure that your codebase can be executed flawlessly that would not encounter any errors or exceptions.\n",
    "    - You must add some tqdm to see the infer progress.\n",
    "\n",
    "    Your implementation *must strictly follow* the structure below:\n",
    "    1. *Imports*: All required libraries.\n",
    "    2. *Preprocessing*: Handle and transform the input as defined.\n",
    "    3. *Inference logic*: Use the described model for prediction. You *must* use tqdm or similar logging library to track progress.\n",
    "    4. *Postprocessing*: Format or transform the raw output into the final result as described.\n",
    "\n",
    "    You must *not* include any explanations, markdown, or logging outside what is required by the problem.\n",
    "\n",
    "    Return *only* the complete Python codebase, and you **MUST NOT** include a main function in any way. Wrap it with:\n",
    "    \\`\\`\\`python\n",
    "    # code here\n",
    "    \\`\\`\\`\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ad8e05be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputState(TypedDict):\n",
    "    output_classes: str | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c4731165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_agent(state: OutputState) -> OutputState:\n",
    "    prompt = r\"\"\"\n",
    "        Your are a specialist in machine learning. Your task is to identify the absolute classes of the given problem description, following with an output description.\n",
    "        ### Input:\n",
    "        - Problem description: {problem_description}\n",
    "        - Output description: {output_description}\n",
    "\n",
    "        You must return an array, strictly following these guidelines:\n",
    "        - Understand the context from the given problem description.\n",
    "        - Extract the class names from the output description. You **must not** invent new labels or translate the class names. Use them exactly as given in the output description.\n",
    "        - Create an array containing the classes.\n",
    "\n",
    "        You must return only the array containing those classes, without any formatting.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = prompt.format(\n",
    "        problem_description = state[\"problem_description\"],\n",
    "        output_description = state[\"output_description\"]\n",
    "    )\n",
    "    response = llm.invoke(prompt)\n",
    "    return {**state, \"output_classes\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3895d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_state = {\n",
    "    \"problem_description\": task,\n",
    "    \"output_description\": state[\"subtasks\"].subtask_four\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6a52076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = output_agent(output_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "72a9f7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['forest', 'orchid', 'spider', 'fox', 'caterpillar', 'crocodile', 'whale', 'road', 'motorcycle', 'tiger', 'woman', 'orange', 'elephant', 'otter', 'flatfish', 'cup', 'sea', 'mountain', 'cloud', 'willow_tree', 'rabbit', 'couch', 'maple_tree', 'wardrobe', 'mushroom', 'plain', 'telephone', 'shrew', 'bowl', 'tractor', 'camel', 'bus', 'cattle', 'house', 'cockroach', 'lawn_mower', 'oak_tree', 'apple', 'bicycle', 'beetle', 'palm_tree', 'sunflower', 'poppy', 'keyboard', 'porcupine', 'snake', 'can', 'pear', 'shark', 'chimpanzee', 'hamster', 'train', 'cra', 'bear', 'sweet_pepper', 'pine_tree', 'bed', 'tank', 'beaver', 'bridge', 'girl', 'skyscraper', 'wolf', 'raccoon', 'rose', 'worm', 'table', 'pickup_truck', 'turtle', 'lizard', 'leopard', 'streetcar', 'rocket', 'man', 'tulip', 'lamp', 'seal', 'lion', 'butterfly', 'kangaroo', 'television', 'ray', 'chair', 'skunk', 'trout', 'possum', 'mouse']\""
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state[\"output_classes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c3313114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coding_agent(state: AgentCodingState) -> AgentCodingState:\n",
    "    prompt = base_prompt.format(\n",
    "        problem_description=state[\"problem_description\"],\n",
    "        problem_input_description=state[\"problem_input_description\"],\n",
    "        problem_output_description=state[\"problem_output_description\"],\n",
    "        model_description=state[\"model_description\"],\n",
    "        model_input_format=state[\"model_input_format\"],\n",
    "        model_output_format=state[\"model_output_format\"],\n",
    "        model_requirements=state[\"model_requirements\"],\n",
    "        model_sample_code=state[\"model_sample_code\"],\n",
    "        output_classes=state[\"output_classes\"],\n",
    "    )\n",
    "    response = llm.invoke(prompt)\n",
    "    return {**state, \"code\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d80aeddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_state = {\n",
    "    \"problem_description\": task,\n",
    "    \"problem_input_description\": state[\"subtasks\"].subtask_three,\n",
    "    \"problem_output_description\": state[\"subtasks\"].subtask_four,\n",
    "    \"model_description\": res2['output'],\n",
    "    \"model_input_format\": ans.model_input_format,\n",
    "    \"model_output_format\": ans.model_output_format,\n",
    "    \"model_requirements\": ans.model_requirements,\n",
    "    \"model_sample_code\": ans.model_sample_code,\n",
    "    \"output_classes\": new_state[\"output_classes\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "d6a56d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_state = coding_agent(prev_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "15ad0a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import os\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "import csv\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Load model and processor\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\n",
      "\n",
      "# Define output classes\n",
      "output_classes = ['forest', 'orchid', 'spider', 'fox', 'caterpillar', 'crocodile', \n",
      "                  'whale', 'road', 'motorcycle', 'tiger', 'woman', 'orange', \n",
      "                  'elephant', 'otter', 'flatfish', 'cup', 'sea', 'mountain', 'cloud', \n",
      "                  'willow_tree', 'rabbit', 'couch', 'maple_tree', 'wardrobe', \n",
      "                  'mushroom', 'plain', 'telephone', 'shrew', 'bowl', 'tractor', \n",
      "                  'camel', 'bus', 'cattle', 'house', 'cockroach', 'lawn_mower', \n",
      "                  'oak_tree', 'apple', 'bicycle', 'beetle', 'palm_tree', 'sunflower', \n",
      "                  'poppy', 'keyboard', 'porcupine', 'snake', 'can', 'pear', 'shark', \n",
      "                  'chimpanzee', 'hamster', 'train', 'cra', 'bear', 'sweet_pepper', \n",
      "                  'pine_tree', 'bed', 'tank', 'beaver', 'bridge', 'girl', \n",
      "                  'skyscraper', 'wolf', 'raccoon', 'rose', 'worm', 'table', \n",
      "                  'pickup_truck', 'turtle', 'lizard', 'leopard', 'streetcar', \n",
      "                  'rocket', 'man', 'tulip', 'lamp', 'seal', 'lion', 'butterfly', \n",
      "                  'kangaroo', 'television', 'ray', 'chair', 'skunk', 'trout', \n",
      "                  'possum', 'mouse']\n",
      "\n",
      "# Process images in the 'images' folder\n",
      "image_folder = 'images'\n",
      "output_file = 'output.csv'\n",
      "\n",
      "with open(output_file, 'w', newline='') as csvfile:\n",
      "    fieldnames = ['file_name', 'prediction']\n",
      "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
      "\n",
      "    writer.writeheader()\n",
      "\n",
      "    for filename in tqdm(os.listdir(image_folder)):\n",
      "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
      "            image_path = os.path.join(image_folder, filename)\n",
      "            image = Image.open(image_path)\n",
      "\n",
      "            # Split image into 9 sub-images and predict each\n",
      "            sub_images = []\n",
      "            for i in range(3):\n",
      "                for j in range(3):\n",
      "                    left = j * 32\n",
      "                    top = i * 32\n",
      "                    right = left + 32\n",
      "                    bottom = top + 32\n",
      "                    sub_image = image.crop((left, top, right, bottom))\n",
      "                    sub_images.append(sub_image)\n",
      "\n",
      "            predictions = []\n",
      "            for sub_image in sub_images:\n",
      "                inputs = processor(images=sub_image, return_tensors=\"pt\")\n",
      "                with torch.no_grad():\n",
      "                    outputs = model(**inputs)\n",
      "                    logits = outputs.logits\n",
      "                    predicted_label = logits.argmax(-1).item()\n",
      "                    predictions.append(output_classes[predicted_label])\n",
      "\n",
      "            # Determine the most common theme\n",
      "            from collections import Counter\n",
      "            counter = Counter(predictions)\n",
      "            most_common_theme = counter.most_common(1)[0][0]\n",
      "\n",
      "            # Write result to CSV\n",
      "            writer.writerow({'file_name': filename, 'prediction': most_common_theme})\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(coding_state[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "888cd97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem_description': 'B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong nhi·ªÅu ·ª©ng d·ª•ng th·ª±c t·∫ø nh∆∞ t·ªïng h·ª£p n·ªôi dung h√¨nh ·∫£nh, ph√¢n lo·∫°i album, hay tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng n·ªôi dung cho ·∫£nh t·ªï h·ª£p, c·∫ßn x√°c ƒë·ªãnh ch·ªß ƒë·ªÅ ch√≠nh c·ªßa m·ªôt ·∫£nh gh√©p t·ª´ nhi·ªÅu ·∫£nh con. B√†i to√°n n√†y ƒë√≤i h·ªèi ·ª©ng d·ª•ng kh√¥ng ch·ªâ nh·∫≠n di·ªán ƒë∆∞·ª£c n·ªôi dung c·ªßa t·ª´ng ·∫£nh con, m√† c√≤n c·∫ßn x√°c ƒë·ªãnh ch·ªß ƒë·ªÅ chi·∫øm ∆∞u th·∫ø trong to√†n b·ªô ·∫£nh t·ªïng th·ªÉ.\\r\\n\\r\\nY√™u c·∫ßu c·ª• th·ªÉ c·∫ßn ƒë·∫°t ƒë∆∞·ª£c:\\r\\nM·ªói ƒë·∫ßu v√†o l√† m·ªôt ·∫£nh t·ªï h·ª£p, gh√©p t·ª´ 9 ·∫£nh m√†u nh·ªè (3x3).\\r\\nNhi·ªám v·ª• c·ªßa ·ª©ng d·ª•ng l√†:\\r\\nX√°c ƒë·ªãnh ch·ªß ƒë·ªÅ chi·∫øm s·ªë l∆∞·ª£ng nhi·ªÅu nh·∫•t trong 9 ·∫£nh con.\\r\\nTr∆∞·ªùng h·ª£p c√≥ nhi·ªÅu ch·ªß ƒë·ªÅ xu·∫•t hi·ªán v·ªõi t·∫ßn su·∫•t b·∫±ng nhau, c√≥ th·ªÉ ch·ªçn m·ªôt ch·ªß ƒë·ªÅ b·∫•t k·ª≥ trong s·ªë ƒë√≥ l√†m k·∫øt qu·∫£.\\r\\n\\r\\nƒê·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫ßu v√†o cho b√†i to√°n t·ªïng th·ªÉ:\\r\\nM·ªôt th∆∞ m·ª•c c√≥ t√™n l√† \"images\" ch·ª©a c√°c ·∫£nh m√†u RGB (288x288), ƒë∆∞·ª£c gh√©p t·ª´ 9 ·∫£nh RGB con (32x32) (3 h√†ng √ó 3 c·ªôt).\\r\\n\\r\\nƒê·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra mong mu·ªën cho b√†i to√°n t·ªïng th·ªÉ:\\r\\nFile output.csv m·ªói h√†ng l√† k·∫øt qu·∫£ d·ª± ƒëo√°n m·ªói ·∫£nh\\r\\nc√≥ c√°c c·ªôt:\\r\\nfile_name: t√™n file ·∫£nh\\r\\nprediction: nh√£n c·ªßa ·∫£nh. \\r\\nnh√£n c·ªßa ·∫£nh l√† m·ªôt nh√£n duy nh·∫•t ƒë·∫°i di·ªán cho ch·ªß ƒë·ªÅ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t trong 9 ·∫£nh con.\\r\\nC√°c ch·ªß ƒë·ªÅ sau n·∫±m trong danh s√°ch sau:\\r\\n       [\\'forest\\', \\'orchid\\', \\'spider\\', \\'fox\\', \\'caterpillar\\', \\'crocodile\\',\\r\\n       \\'whale\\', \\'road\\', \\'motorcycle\\', \\'tiger\\', \\'woman\\', \\'orange\\',\\r\\n       \\'elephant\\', \\'otter\\', \\'flatfish\\', \\'cup\\', \\'sea\\', \\'mountain\\', \\'cloud\\',\\r\\n       \\'willow_tree\\', \\'rabbit\\', \\'couch\\', \\'maple_tree\\', \\'wardrobe\\',\\r\\n       \\'mushroom\\', \\'plain\\', \\'telephone\\', \\'shrew\\', \\'bowl\\', \\'tractor\\',\\r\\n       \\'camel\\', \\'bus\\', \\'cattle\\', \\'house\\', \\'cockroach\\', \\'lawn_mower\\',\\r\\n       \\'oak_tree\\', \\'apple\\', \\'bicycle\\', \\'beetle\\', \\'palm_tree\\', \\'sunflower\\',\\r\\n       \\'poppy\\', \\'keyboard\\', \\'porcupine\\', \\'snake\\', \\'can\\', \\'pear\\', \\'shark\\',\\r\\n       \\'chimpanzee\\', \\'hamster\\', \\'train\\', \\'cra\\', \\'bear\\', \\'sweet_pepper\\',\\r\\n       \\'pine_tree\\', \\'bed\\', \\'tank\\', \\'beaver\\', \\'bridge\\', \\'girl\\',\\r\\n       \\'skyscraper\\', \\'wolf\\', \\'raccoon\\', \\'rose\\', \\'worm\\', \\'table\\',\\r\\n       \\'pickup_truck\\', \\'turtle\\', \\'lizard\\', \\'leopard\\', \\'streetcar\\',\\r\\n       \\'rocket\\', \\'man\\', \\'tulip\\', \\'lamp\\', \\'seal\\', \\'lion\\', \\'butterfly\\',\\r\\n       \\'kangaroo\\', \\'television\\', \\'ray\\', \\'chair\\', \\'skunk\\', \\'trout\\',\\r\\n       \\'possum\\', \\'mouse\\']',\n",
       " 'problem_input_description': \"X√°c ƒë·ªãnh ƒë·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫ßu v√†o. D·ªØ li·ªáu ƒë·∫ßu v√†o l√† m·ªôt th∆∞ m·ª•c c√≥ t√™n l√† 'images' ch·ª©a c√°c ·∫£nh m√†u RGB (288x288), ƒë∆∞·ª£c gh√©p t·ª´ 9 ·∫£nh RGB con (32x32) (3 h√†ng √ó3 c·ªôt).\",\n",
       " 'problem_output_description': \"X√°c ƒë·ªãnh ƒë·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra. K·∫øt qu·∫£ ƒë·∫ßu ra l√† m·ªôt file 'output.csv' v·ªõi m·ªói h√†ng l√† k·∫øt qu·∫£ d·ª± ƒëo√°n m·ªói ·∫£nh, c√≥ c√°c c·ªôt: 'file_name' (t√™n file ·∫£nh) v√† 'prediction' (nh√£n c·ªßa ·∫£nh). Nh√£n c·ªßa ·∫£nh l√† m·ªôt nh√£n duy nh·∫•t ƒë·∫°i di·ªán cho ch·ªß ƒë·ªÅ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t trong 9 ·∫£nh con. C√°c ch·ªß ƒë·ªÅ c√≥ th·ªÉ l√† m·ªôt trong c√°c nh√£n sau: ['forest', 'orchid', 'spider', 'fox', 'caterpillar', 'crocodile', 'whale', 'road', 'motorcycle', 'tiger', 'woman', 'orange', 'elephant', 'otter', 'flatfish', 'cup', 'sea', 'mountain', 'cloud', 'willow_tree', 'rabbit', 'couch', 'maple_tree', 'wardrobe', 'mushroom', 'plain', 'telephone', 'shrew', 'bowl', 'tractor', 'camel', 'bus', 'cattle', 'house', 'cockroach', 'lawn_mower', 'oak_tree', 'apple', 'bicycle', 'beetle', 'palm_tree', 'sunflower', 'poppy', 'keyboard', 'porcupine', 'snake', 'can', 'pear', 'shark', 'chimpanzee', 'hamster', 'train', 'cra', 'bear', 'sweet_pepper', 'pine_tree', 'bed', 'tank', 'beaver', 'bridge', 'girl', 'skyscraper', 'wolf', 'raccoon', 'rose', 'worm', 'table', 'pickup_truck', 'turtle', 'lizard', 'leopard', 'streetcar', 'rocket', 'man', 'tulip', 'lamp', 'seal', 'lion', 'butterfly', 'kangaroo', 'television', 'ray', 'chair', 'skunk', 'trout', 'possum', 'mouse']\",\n",
       " 'model_description': 'To solve this task, I need to analyze the provided models and select the best one that fits the given task description. The task involves determining the dominant theme in a collage image made of 9 smaller images.\\n\\nI have already gathered detailed information about the three models:\\n\\n**Model 1: Ph√¢n lo·∫°i h√¨nh ·∫£nh ‚Äî https://huggingface.co/zhaospei/Model_7**\\n- Description: Vision Transformer (ViT) fine-tuned on CIFAR-10 dataset for image classification into 10 classes.\\n- Input format: RGB images of size 224x224 pixels.\\n- Output format: Probabilities for each class (logits) in a tensor of size [batch_size, 10].\\n- Library requirements: transformers, torch, torchvision.\\n- Example code: Provided in the tool output.\\n\\n**Model 2: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained) ‚Äî https://huggingface.co/zhaospei/Model_6**\\n- Description: Vision Transformer (ViT) fine-tuned on CIFAR-100 dataset for image classification into 100 classes.\\n- Input format: RGB images of size 224x224 pixels.\\n- Output format: Probabilities for each class (logits) in a tensor of size [batch_size, 100].\\n- Library requirements: transformers, torch, torchvision.\\n- Example code: Provided in the tool output.\\n\\n**Model 3: Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥ ‚Äî https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit**\\n- Description: Vision Transformer (ViT) fine-tuned on Stanford Dogs dataset to classify dog images into 120 types of dog breeds.\\n- Input format: RGB images.\\n- Output format: Probabilities for each class (logits).\\n- Library requirements: transformers, torch.\\n- Example code: Provided in the tool output.\\n\\nThe task requires classifying a collage image made of 9 smaller images into one of the given themes. The models provided are primarily for image classification tasks but differ in their specific applications and class numbers.\\n\\nGiven the task description, **Model 2: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained) ‚Äî https://huggingface.co/zhaospei/Model_6** seems to be the most suitable. This is because:\\n- It can classify images into 100 classes, which is closer to the number of themes provided (around 100).\\n- The input format matches the requirement (RGB images of size 224x224 pixels).\\n\\nHere is the final answer:\\n\\n**‚úÖ Model name and link**  \\n`Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)` ‚Äî `https://huggingface.co/zhaospei/Model_6`\\n\\n**üìù Description (from tool):**  \\n`ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh g·ªëc google/vit-base-patch16-224-in21k tr√™n t·∫≠p d·ªØ li·ªáu CIFAR-100. M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o100 l·ªõp kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.`\\n\\n**üì• Input format (from tool):**  \\n`ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB, K√≠ch th∆∞·ªõc ·∫£nh:224x224`\\n\\n**üì§ Output format (from tool):**  \\n`ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits), Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch th∆∞·ªõc [batch_size,100]`\\n\\n**üõ†Ô∏è Library Requirements (from tool) **  \\n`C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt b·∫±ng: pip install transformers torch torchvision`\\n\\n**üß™ Example code (from tool):**  \\n```python\\nimport torch\\nfrom transformers import ViTForImageClassification, ViTImageProcessor\\nfrom PIL import Image\\n\\n# T·∫£i ·∫£nh c·∫ßn ph√¢n lo·∫°i\\nimage = Image.open(\"path_to_your_image.jpg\")\\n\\n# T·∫£i processor v√† m√¥ h√¨nh t·ª´ Hugging Face\\nprocessor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\nmodel = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n\\n# X·ª≠ l√Ω ƒë·∫ßu v√†o\\ninputs = processor(images=image, return_tensors=\"pt\")\\n\\n# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\n predicted_label = logits.argmax(-1).item()\\nprint(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\\n```',\n",
       " 'model_input_format': 'RGB images of size 224x224 pixels.',\n",
       " 'model_output_format': 'Probabilities for each class (logits) in a tensor of size [batch_size,100].',\n",
       " 'model_requirements': 'Install required libraries by running: pip install transformers torch torchvision',\n",
       " 'model_sample_code': 'import torch\\nfrom transformers import ViTForImageClassification, ViTImageProcessor\\nfrom PIL import Image\\n\\n# Load image to classify\\nimage = Image.open(\"path_to_your_image.jpg\")\\n\\n# Load processor and model from Hugging Face\\nprocessor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\nmodel = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n\\n# Process input\\ninputs = processor(images=image, return_tensors=\"pt\")\\n\\n# Predict with model\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n    logits = outputs.logits\\n    predicted_label = logits.argmax(-1).item()\\nprint(f\"Predicted label: {model.config.id2label[predicted_label]}\")',\n",
       " 'output_classes': \"['forest', 'orchid', 'spider', 'fox', 'caterpillar', 'crocodile', 'whale', 'road', 'motorcycle', 'tiger', 'woman', 'orange', 'elephant', 'otter', 'flatfish', 'cup', 'sea', 'mountain', 'cloud', 'willow_tree', 'rabbit', 'couch', 'maple_tree', 'wardrobe', 'mushroom', 'plain', 'telephone', 'shrew', 'bowl', 'tractor', 'camel', 'bus', 'cattle', 'house', 'cockroach', 'lawn_mower', 'oak_tree', 'apple', 'bicycle', 'beetle', 'palm_tree', 'sunflower', 'poppy', 'keyboard', 'porcupine', 'snake', 'can', 'pear', 'shark', 'chimpanzee', 'hamster', 'train', 'cra', 'bear', 'sweet_pepper', 'pine_tree', 'bed', 'tank', 'beaver', 'bridge', 'girl', 'skyscraper', 'wolf', 'raccoon', 'rose', 'worm', 'table', 'pickup_truck', 'turtle', 'lizard', 'leopard', 'streetcar', 'rocket', 'man', 'tulip', 'lamp', 'seal', 'lion', 'butterfly', 'kangaroo', 'television', 'ray', 'chair', 'skunk', 'trout', 'possum', 'mouse']\",\n",
       " 'code': '```python\\nimport os\\nimport torch\\nfrom transformers import ViTForImageClassification, ViTImageProcessor\\nfrom PIL import Image\\nimport csv\\nfrom tqdm import tqdm\\n\\n# Load model and processor\\nprocessor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\nmodel = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n\\n# Define output classes\\noutput_classes = [\\'forest\\', \\'orchid\\', \\'spider\\', \\'fox\\', \\'caterpillar\\', \\'crocodile\\', \\n                  \\'whale\\', \\'road\\', \\'motorcycle\\', \\'tiger\\', \\'woman\\', \\'orange\\', \\n                  \\'elephant\\', \\'otter\\', \\'flatfish\\', \\'cup\\', \\'sea\\', \\'mountain\\', \\'cloud\\', \\n                  \\'willow_tree\\', \\'rabbit\\', \\'couch\\', \\'maple_tree\\', \\'wardrobe\\', \\n                  \\'mushroom\\', \\'plain\\', \\'telephone\\', \\'shrew\\', \\'bowl\\', \\'tractor\\', \\n                  \\'camel\\', \\'bus\\', \\'cattle\\', \\'house\\', \\'cockroach\\', \\'lawn_mower\\', \\n                  \\'oak_tree\\', \\'apple\\', \\'bicycle\\', \\'beetle\\', \\'palm_tree\\', \\'sunflower\\', \\n                  \\'poppy\\', \\'keyboard\\', \\'porcupine\\', \\'snake\\', \\'can\\', \\'pear\\', \\'shark\\', \\n                  \\'chimpanzee\\', \\'hamster\\', \\'train\\', \\'cra\\', \\'bear\\', \\'sweet_pepper\\', \\n                  \\'pine_tree\\', \\'bed\\', \\'tank\\', \\'beaver\\', \\'bridge\\', \\'girl\\', \\n                  \\'skyscraper\\', \\'wolf\\', \\'raccoon\\', \\'rose\\', \\'worm\\', \\'table\\', \\n                  \\'pickup_truck\\', \\'turtle\\', \\'lizard\\', \\'leopard\\', \\'streetcar\\', \\n                  \\'rocket\\', \\'man\\', \\'tulip\\', \\'lamp\\', \\'seal\\', \\'lion\\', \\'butterfly\\', \\n                  \\'kangaroo\\', \\'television\\', \\'ray\\', \\'chair\\', \\'skunk\\', \\'trout\\', \\n                  \\'possum\\', \\'mouse\\']\\n\\n# Process images in the \\'images\\' folder\\nimage_folder = \\'images\\'\\noutput_file = \\'output.csv\\'\\n\\nwith open(output_file, \\'w\\', newline=\\'\\') as csvfile:\\n    fieldnames = [\\'file_name\\', \\'prediction\\']\\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\\n\\n    writer.writeheader()\\n\\n    for filename in tqdm(os.listdir(image_folder)):\\n        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\\n            image_path = os.path.join(image_folder, filename)\\n            image = Image.open(image_path)\\n\\n            # Split image into 9 sub-images and predict each\\n            sub_images = []\\n            for i in range(3):\\n                for j in range(3):\\n                    left = j * 32\\n                    top = i * 32\\n                    right = left + 32\\n                    bottom = top + 32\\n                    sub_image = image.crop((left, top, right, bottom))\\n                    sub_images.append(sub_image)\\n\\n            predictions = []\\n            for sub_image in sub_images:\\n                inputs = processor(images=sub_image, return_tensors=\"pt\")\\n                with torch.no_grad():\\n                    outputs = model(**inputs)\\n                    logits = outputs.logits\\n                    predicted_label = logits.argmax(-1).item()\\n                    predictions.append(output_classes[predicted_label])\\n\\n            # Determine the most common theme\\n            from collections import Counter\\n            counter = Counter(predictions)\\n            most_common_theme = counter.most_common(1)[0][0]\\n\\n            # Write result to CSV\\n            writer.writerow({\\'file_name\\': filename, \\'prediction\\': most_common_theme})\\n```'}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coding_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "47e442a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import venv\n",
    "from pathlib import Path\n",
    "\n",
    "def code_execute_agent(\n",
    "    input_path,\n",
    "    output_path,\n",
    "    code: str,\n",
    "    requirements=None,\n",
    "    timeout: int = 120000\n",
    "):\n",
    "    requirements = requirements or []\n",
    "\n",
    "    input_path = Path(input_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    temp_dir = Path(tempfile.mkdtemp(prefix=\"code_runner_\", dir=Path.cwd()))\n",
    "    venv_dir = temp_dir / \"venv\"\n",
    "    project_dir = temp_dir / \"project\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Create project structure\n",
    "        os.makedirs(project_dir, exist_ok=True)\n",
    "\n",
    "        # Step 2: Copy input to temp folder\n",
    "        # if input_path.exists():\n",
    "        #     if input_path.is_dir():\n",
    "        #         shutil.copytree(input_path, project_dir / \"input\", dirs_exist_ok=True)\n",
    "        #     else:\n",
    "        #         shutil.copy2(input_path, project_dir / input_path.name)\n",
    "\n",
    "        if input_path.exists():\n",
    "            if input_path.is_dir():\n",
    "                dest = project_dir / input_path.name\n",
    "                shutil.copytree(input_path, dest, dirs_exist_ok=True)\n",
    "            else:\n",
    "                shutil.copy2(input_path, project_dir / input_path.name)\n",
    "\n",
    "        # Step 3: Write code into main.py\n",
    "        main_file = project_dir / \"main.py\"\n",
    "        main_file.write_text(code)\n",
    "\n",
    "        # Step 4: Setup virtual environment\n",
    "        venv.create(venv_dir, with_pip=True)\n",
    "        pip_path = venv_dir / \"bin\" / \"pip\"\n",
    "        subprocess.check_call([str(pip_path), \"install\", *requirements])\n",
    "\n",
    "        # # Step 5: Execute the code\n",
    "        # python_path = venv_dir / \"bin\" / \"python\"\n",
    "        # result = subprocess.run(\n",
    "        #     [str(python_path), \"main.py\"],\n",
    "        #     cwd=project_dir,\n",
    "        #     stdout=subprocess.PIPE,\n",
    "        #     stderr=subprocess.STDOUT,\n",
    "        #     capture_output=True,\n",
    "        #     text=True,\n",
    "        #     timeout=timeout\n",
    "        # )\n",
    "\n",
    "        # if result.returncode != 0:\n",
    "        #     raise RuntimeError(f\"[Error]\\n{result.stderr}\")\n",
    "\n",
    "        # Step 5: Execute the code with realtime logging\n",
    "        python_path = venv_dir / \"bin\" / \"python\"\n",
    "        process = subprocess.Popen(\n",
    "            [str(python_path), \"main.py\"],\n",
    "            cwd=project_dir,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        log_lines = []\n",
    "        try:\n",
    "            for line in process.stdout:\n",
    "                print(line, end=\"\")              # Realtime print\n",
    "                log_lines.append(line)           # Collect for return\n",
    "            process.wait(timeout=timeout)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            process.kill()\n",
    "            raise RuntimeError(\"Code execution timed out.\")\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            raise RuntimeError(f\"Execution failed:\\n{''.join(log_lines)}\")\n",
    "\n",
    "        # Step 6: Copy specific output file to output_path\n",
    "        produced_file = project_dir / output_path.name\n",
    "        if not produced_file.exists():\n",
    "            raise FileNotFoundError(f\"Expected output file not found: {produced_file}\")\n",
    "\n",
    "        # Ensure parent folder for output_path exists\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(produced_file, output_path)\n",
    "\n",
    "        return result.stdout\n",
    "\n",
    "    finally:\n",
    "        # Step 7: Cleanup temp folder\n",
    "        shutil.rmtree(temp_dir, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "2bd1ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !gdown 1sHOzN7zdg3RDu8ijCXJXrL4KpYi4J_Vs\n",
    "# !gdown 1uUqW8itSiO_RniR-eFDFu1VaTLgOhQ8l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "78fddf58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem_description': 'B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong nhi·ªÅu ·ª©ng d·ª•ng th·ª±c t·∫ø nh∆∞ t·ªïng h·ª£p n·ªôi dung h√¨nh ·∫£nh, ph√¢n lo·∫°i album, hay tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng n·ªôi dung cho ·∫£nh t·ªï h·ª£p, c·∫ßn x√°c ƒë·ªãnh ch·ªß ƒë·ªÅ ch√≠nh c·ªßa m·ªôt ·∫£nh gh√©p t·ª´ nhi·ªÅu ·∫£nh con. B√†i to√°n n√†y ƒë√≤i h·ªèi ·ª©ng d·ª•ng kh√¥ng ch·ªâ nh·∫≠n di·ªán ƒë∆∞·ª£c n·ªôi dung c·ªßa t·ª´ng ·∫£nh con, m√† c√≤n c·∫ßn x√°c ƒë·ªãnh ch·ªß ƒë·ªÅ chi·∫øm ∆∞u th·∫ø trong to√†n b·ªô ·∫£nh t·ªïng th·ªÉ.\\r\\n\\r\\nY√™u c·∫ßu c·ª• th·ªÉ c·∫ßn ƒë·∫°t ƒë∆∞·ª£c:\\r\\nM·ªói ƒë·∫ßu v√†o l√† m·ªôt ·∫£nh t·ªï h·ª£p, gh√©p t·ª´ 9 ·∫£nh m√†u nh·ªè (3x3).\\r\\nNhi·ªám v·ª• c·ªßa ·ª©ng d·ª•ng l√†:\\r\\nX√°c ƒë·ªãnh ch·ªß ƒë·ªÅ chi·∫øm s·ªë l∆∞·ª£ng nhi·ªÅu nh·∫•t trong 9 ·∫£nh con.\\r\\nTr∆∞·ªùng h·ª£p c√≥ nhi·ªÅu ch·ªß ƒë·ªÅ xu·∫•t hi·ªán v·ªõi t·∫ßn su·∫•t b·∫±ng nhau, c√≥ th·ªÉ ch·ªçn m·ªôt ch·ªß ƒë·ªÅ b·∫•t k·ª≥ trong s·ªë ƒë√≥ l√†m k·∫øt qu·∫£.\\r\\n\\r\\nƒê·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫ßu v√†o cho b√†i to√°n t·ªïng th·ªÉ:\\r\\nM·ªôt th∆∞ m·ª•c c√≥ t√™n l√† \"images\" ch·ª©a c√°c ·∫£nh m√†u RGB (288x288), ƒë∆∞·ª£c gh√©p t·ª´ 9 ·∫£nh RGB con (32x32) (3 h√†ng √ó 3 c·ªôt).\\r\\n\\r\\nƒê·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra mong mu·ªën cho b√†i to√°n t·ªïng th·ªÉ:\\r\\nFile output.csv m·ªói h√†ng l√† k·∫øt qu·∫£ d·ª± ƒëo√°n m·ªói ·∫£nh\\r\\nc√≥ c√°c c·ªôt:\\r\\nfile_name: t√™n file ·∫£nh\\r\\nprediction: nh√£n c·ªßa ·∫£nh. \\r\\nnh√£n c·ªßa ·∫£nh l√† m·ªôt nh√£n duy nh·∫•t ƒë·∫°i di·ªán cho ch·ªß ƒë·ªÅ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t trong 9 ·∫£nh con.\\r\\nC√°c ch·ªß ƒë·ªÅ sau n·∫±m trong danh s√°ch sau:\\r\\n       [\\'forest\\', \\'orchid\\', \\'spider\\', \\'fox\\', \\'caterpillar\\', \\'crocodile\\',\\r\\n       \\'whale\\', \\'road\\', \\'motorcycle\\', \\'tiger\\', \\'woman\\', \\'orange\\',\\r\\n       \\'elephant\\', \\'otter\\', \\'flatfish\\', \\'cup\\', \\'sea\\', \\'mountain\\', \\'cloud\\',\\r\\n       \\'willow_tree\\', \\'rabbit\\', \\'couch\\', \\'maple_tree\\', \\'wardrobe\\',\\r\\n       \\'mushroom\\', \\'plain\\', \\'telephone\\', \\'shrew\\', \\'bowl\\', \\'tractor\\',\\r\\n       \\'camel\\', \\'bus\\', \\'cattle\\', \\'house\\', \\'cockroach\\', \\'lawn_mower\\',\\r\\n       \\'oak_tree\\', \\'apple\\', \\'bicycle\\', \\'beetle\\', \\'palm_tree\\', \\'sunflower\\',\\r\\n       \\'poppy\\', \\'keyboard\\', \\'porcupine\\', \\'snake\\', \\'can\\', \\'pear\\', \\'shark\\',\\r\\n       \\'chimpanzee\\', \\'hamster\\', \\'train\\', \\'cra\\', \\'bear\\', \\'sweet_pepper\\',\\r\\n       \\'pine_tree\\', \\'bed\\', \\'tank\\', \\'beaver\\', \\'bridge\\', \\'girl\\',\\r\\n       \\'skyscraper\\', \\'wolf\\', \\'raccoon\\', \\'rose\\', \\'worm\\', \\'table\\',\\r\\n       \\'pickup_truck\\', \\'turtle\\', \\'lizard\\', \\'leopard\\', \\'streetcar\\',\\r\\n       \\'rocket\\', \\'man\\', \\'tulip\\', \\'lamp\\', \\'seal\\', \\'lion\\', \\'butterfly\\',\\r\\n       \\'kangaroo\\', \\'television\\', \\'ray\\', \\'chair\\', \\'skunk\\', \\'trout\\',\\r\\n       \\'possum\\', \\'mouse\\']',\n",
       " 'problem_input_description': \"X√°c ƒë·ªãnh ƒë·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫ßu v√†o. D·ªØ li·ªáu ƒë·∫ßu v√†o l√† m·ªôt th∆∞ m·ª•c c√≥ t√™n l√† 'images' ch·ª©a c√°c ·∫£nh m√†u RGB (288x288), ƒë∆∞·ª£c gh√©p t·ª´ 9 ·∫£nh RGB con (32x32) (3 h√†ng √ó3 c·ªôt).\",\n",
       " 'problem_output_description': \"X√°c ƒë·ªãnh ƒë·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra. K·∫øt qu·∫£ ƒë·∫ßu ra l√† m·ªôt file 'output.csv' v·ªõi m·ªói h√†ng l√† k·∫øt qu·∫£ d·ª± ƒëo√°n m·ªói ·∫£nh, c√≥ c√°c c·ªôt: 'file_name' (t√™n file ·∫£nh) v√† 'prediction' (nh√£n c·ªßa ·∫£nh). Nh√£n c·ªßa ·∫£nh l√† m·ªôt nh√£n duy nh·∫•t ƒë·∫°i di·ªán cho ch·ªß ƒë·ªÅ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t trong 9 ·∫£nh con. C√°c ch·ªß ƒë·ªÅ c√≥ th·ªÉ l√† m·ªôt trong c√°c nh√£n sau: ['forest', 'orchid', 'spider', 'fox', 'caterpillar', 'crocodile', 'whale', 'road', 'motorcycle', 'tiger', 'woman', 'orange', 'elephant', 'otter', 'flatfish', 'cup', 'sea', 'mountain', 'cloud', 'willow_tree', 'rabbit', 'couch', 'maple_tree', 'wardrobe', 'mushroom', 'plain', 'telephone', 'shrew', 'bowl', 'tractor', 'camel', 'bus', 'cattle', 'house', 'cockroach', 'lawn_mower', 'oak_tree', 'apple', 'bicycle', 'beetle', 'palm_tree', 'sunflower', 'poppy', 'keyboard', 'porcupine', 'snake', 'can', 'pear', 'shark', 'chimpanzee', 'hamster', 'train', 'cra', 'bear', 'sweet_pepper', 'pine_tree', 'bed', 'tank', 'beaver', 'bridge', 'girl', 'skyscraper', 'wolf', 'raccoon', 'rose', 'worm', 'table', 'pickup_truck', 'turtle', 'lizard', 'leopard', 'streetcar', 'rocket', 'man', 'tulip', 'lamp', 'seal', 'lion', 'butterfly', 'kangaroo', 'television', 'ray', 'chair', 'skunk', 'trout', 'possum', 'mouse']\",\n",
       " 'model_description': 'To solve this task, I need to analyze the provided models and select the best one that fits the given task description. The task involves determining the dominant theme in a collage image made of 9 smaller images.\\n\\nI have already gathered detailed information about the three models:\\n\\n**Model 1: Ph√¢n lo·∫°i h√¨nh ·∫£nh ‚Äî https://huggingface.co/zhaospei/Model_7**\\n- Description: Vision Transformer (ViT) fine-tuned on CIFAR-10 dataset for image classification into 10 classes.\\n- Input format: RGB images of size 224x224 pixels.\\n- Output format: Probabilities for each class (logits) in a tensor of size [batch_size, 10].\\n- Library requirements: transformers, torch, torchvision.\\n- Example code: Provided in the tool output.\\n\\n**Model 2: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained) ‚Äî https://huggingface.co/zhaospei/Model_6**\\n- Description: Vision Transformer (ViT) fine-tuned on CIFAR-100 dataset for image classification into 100 classes.\\n- Input format: RGB images of size 224x224 pixels.\\n- Output format: Probabilities for each class (logits) in a tensor of size [batch_size, 100].\\n- Library requirements: transformers, torch, torchvision.\\n- Example code: Provided in the tool output.\\n\\n**Model 3: Ph√¢n lo·∫°i h√¨nh ·∫£nh gi·ªëng ch√≥ ‚Äî https://huggingface.co/wesleyacheng/dog-breeds-multiclass-image-classification-with-vit**\\n- Description: Vision Transformer (ViT) fine-tuned on Stanford Dogs dataset to classify dog images into 120 types of dog breeds.\\n- Input format: RGB images.\\n- Output format: Probabilities for each class (logits).\\n- Library requirements: transformers, torch.\\n- Example code: Provided in the tool output.\\n\\nThe task requires classifying a collage image made of 9 smaller images into one of the given themes. The models provided are primarily for image classification tasks but differ in their specific applications and class numbers.\\n\\nGiven the task description, **Model 2: Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained) ‚Äî https://huggingface.co/zhaospei/Model_6** seems to be the most suitable. This is because:\\n- It can classify images into 100 classes, which is closer to the number of themes provided (around 100).\\n- The input format matches the requirement (RGB images of size 224x224 pixels).\\n\\nHere is the final answer:\\n\\n**‚úÖ Model name and link**  \\n`Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)` ‚Äî `https://huggingface.co/zhaospei/Model_6`\\n\\n**üìù Description (from tool):**  \\n`ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh g·ªëc google/vit-base-patch16-224-in21k tr√™n t·∫≠p d·ªØ li·ªáu CIFAR-100. M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o100 l·ªõp kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.`\\n\\n**üì• Input format (from tool):**  \\n`ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB, K√≠ch th∆∞·ªõc ·∫£nh:224x224`\\n\\n**üì§ Output format (from tool):**  \\n`ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits), Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch th∆∞·ªõc [batch_size,100]`\\n\\n**üõ†Ô∏è Library Requirements (from tool) **  \\n`C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt b·∫±ng: pip install transformers torch torchvision`\\n\\n**üß™ Example code (from tool):**  \\n```python\\nimport torch\\nfrom transformers import ViTForImageClassification, ViTImageProcessor\\nfrom PIL import Image\\n\\n# T·∫£i ·∫£nh c·∫ßn ph√¢n lo·∫°i\\nimage = Image.open(\"path_to_your_image.jpg\")\\n\\n# T·∫£i processor v√† m√¥ h√¨nh t·ª´ Hugging Face\\nprocessor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\nmodel = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n\\n# X·ª≠ l√Ω ƒë·∫ßu v√†o\\ninputs = processor(images=image, return_tensors=\"pt\")\\n\\n# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\\nwith torch.no_grad():\\n outputs = model(**inputs)\\n logits = outputs.logits\\n predicted_label = logits.argmax(-1).item()\\nprint(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\\n```',\n",
       " 'model_input_format': 'RGB images of size 224x224 pixels.',\n",
       " 'model_output_format': 'Probabilities for each class (logits) in a tensor of size [batch_size,100].',\n",
       " 'model_requirements': 'Install required libraries by running: pip install transformers torch torchvision',\n",
       " 'model_sample_code': 'import torch\\nfrom transformers import ViTForImageClassification, ViTImageProcessor\\nfrom PIL import Image\\n\\n# Load image to classify\\nimage = Image.open(\"path_to_your_image.jpg\")\\n\\n# Load processor and model from Hugging Face\\nprocessor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\nmodel = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n\\n# Process input\\ninputs = processor(images=image, return_tensors=\"pt\")\\n\\n# Predict with model\\nwith torch.no_grad():\\n    outputs = model(**inputs)\\n    logits = outputs.logits\\n    predicted_label = logits.argmax(-1).item()\\nprint(f\"Predicted label: {model.config.id2label[predicted_label]}\")',\n",
       " 'output_classes': \"['forest', 'orchid', 'spider', 'fox', 'caterpillar', 'crocodile', 'whale', 'road', 'motorcycle', 'tiger', 'woman', 'orange', 'elephant', 'otter', 'flatfish', 'cup', 'sea', 'mountain', 'cloud', 'willow_tree', 'rabbit', 'couch', 'maple_tree', 'wardrobe', 'mushroom', 'plain', 'telephone', 'shrew', 'bowl', 'tractor', 'camel', 'bus', 'cattle', 'house', 'cockroach', 'lawn_mower', 'oak_tree', 'apple', 'bicycle', 'beetle', 'palm_tree', 'sunflower', 'poppy', 'keyboard', 'porcupine', 'snake', 'can', 'pear', 'shark', 'chimpanzee', 'hamster', 'train', 'cra', 'bear', 'sweet_pepper', 'pine_tree', 'bed', 'tank', 'beaver', 'bridge', 'girl', 'skyscraper', 'wolf', 'raccoon', 'rose', 'worm', 'table', 'pickup_truck', 'turtle', 'lizard', 'leopard', 'streetcar', 'rocket', 'man', 'tulip', 'lamp', 'seal', 'lion', 'butterfly', 'kangaroo', 'television', 'ray', 'chair', 'skunk', 'trout', 'possum', 'mouse']\",\n",
       " 'code': '```python\\nimport os\\nimport torch\\nfrom transformers import ViTForImageClassification, ViTImageProcessor\\nfrom PIL import Image\\nimport csv\\nfrom tqdm import tqdm\\n\\n# Load model and processor\\nprocessor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\nmodel = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n\\n# Define output classes\\noutput_classes = [\\'forest\\', \\'orchid\\', \\'spider\\', \\'fox\\', \\'caterpillar\\', \\'crocodile\\', \\n                  \\'whale\\', \\'road\\', \\'motorcycle\\', \\'tiger\\', \\'woman\\', \\'orange\\', \\n                  \\'elephant\\', \\'otter\\', \\'flatfish\\', \\'cup\\', \\'sea\\', \\'mountain\\', \\'cloud\\', \\n                  \\'willow_tree\\', \\'rabbit\\', \\'couch\\', \\'maple_tree\\', \\'wardrobe\\', \\n                  \\'mushroom\\', \\'plain\\', \\'telephone\\', \\'shrew\\', \\'bowl\\', \\'tractor\\', \\n                  \\'camel\\', \\'bus\\', \\'cattle\\', \\'house\\', \\'cockroach\\', \\'lawn_mower\\', \\n                  \\'oak_tree\\', \\'apple\\', \\'bicycle\\', \\'beetle\\', \\'palm_tree\\', \\'sunflower\\', \\n                  \\'poppy\\', \\'keyboard\\', \\'porcupine\\', \\'snake\\', \\'can\\', \\'pear\\', \\'shark\\', \\n                  \\'chimpanzee\\', \\'hamster\\', \\'train\\', \\'cra\\', \\'bear\\', \\'sweet_pepper\\', \\n                  \\'pine_tree\\', \\'bed\\', \\'tank\\', \\'beaver\\', \\'bridge\\', \\'girl\\', \\n                  \\'skyscraper\\', \\'wolf\\', \\'raccoon\\', \\'rose\\', \\'worm\\', \\'table\\', \\n                  \\'pickup_truck\\', \\'turtle\\', \\'lizard\\', \\'leopard\\', \\'streetcar\\', \\n                  \\'rocket\\', \\'man\\', \\'tulip\\', \\'lamp\\', \\'seal\\', \\'lion\\', \\'butterfly\\', \\n                  \\'kangaroo\\', \\'television\\', \\'ray\\', \\'chair\\', \\'skunk\\', \\'trout\\', \\n                  \\'possum\\', \\'mouse\\']\\n\\n# Process images in the \\'images\\' folder\\nimage_folder = \\'images\\'\\noutput_file = \\'output.csv\\'\\n\\nwith open(output_file, \\'w\\', newline=\\'\\') as csvfile:\\n    fieldnames = [\\'file_name\\', \\'prediction\\']\\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\\n\\n    writer.writeheader()\\n\\n    for filename in tqdm(os.listdir(image_folder)):\\n        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\\n            image_path = os.path.join(image_folder, filename)\\n            image = Image.open(image_path)\\n\\n            # Split image into 9 sub-images and predict each\\n            sub_images = []\\n            for i in range(3):\\n                for j in range(3):\\n                    left = j * 32\\n                    top = i * 32\\n                    right = left + 32\\n                    bottom = top + 32\\n                    sub_image = image.crop((left, top, right, bottom))\\n                    sub_images.append(sub_image)\\n\\n            predictions = []\\n            for sub_image in sub_images:\\n                inputs = processor(images=sub_image, return_tensors=\"pt\")\\n                with torch.no_grad():\\n                    outputs = model(**inputs)\\n                    logits = outputs.logits\\n                    predicted_label = logits.argmax(-1).item()\\n                    predictions.append(output_classes[predicted_label])\\n\\n            # Determine the most common theme\\n            from collections import Counter\\n            counter = Counter(predictions)\\n            most_common_theme = counter.most_common(1)[0][0]\\n\\n            # Write result to CSV\\n            writer.writerow({\\'file_name\\': filename, \\'prediction\\': most_common_theme})\\n```'}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coding_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7376f903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'```python\\nimport os\\nimport torch\\nfrom transformers import ViTForImageClassification, ViTImageProcessor\\nfrom PIL import Image\\nimport csv\\nfrom tqdm import tqdm\\n\\n# Load model and processor\\nprocessor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\nmodel = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n\\n# Define output classes\\noutput_classes = [\\'forest\\', \\'orchid\\', \\'spider\\', \\'fox\\', \\'caterpillar\\', \\'crocodile\\', \\n                  \\'whale\\', \\'road\\', \\'motorcycle\\', \\'tiger\\', \\'woman\\', \\'orange\\', \\n                  \\'elephant\\', \\'otter\\', \\'flatfish\\', \\'cup\\', \\'sea\\', \\'mountain\\', \\'cloud\\', \\n                  \\'willow_tree\\', \\'rabbit\\', \\'couch\\', \\'maple_tree\\', \\'wardrobe\\', \\n                  \\'mushroom\\', \\'plain\\', \\'telephone\\', \\'shrew\\', \\'bowl\\', \\'tractor\\', \\n                  \\'camel\\', \\'bus\\', \\'cattle\\', \\'house\\', \\'cockroach\\', \\'lawn_mower\\', \\n                  \\'oak_tree\\', \\'apple\\', \\'bicycle\\', \\'beetle\\', \\'palm_tree\\', \\'sunflower\\', \\n                  \\'poppy\\', \\'keyboard\\', \\'porcupine\\', \\'snake\\', \\'can\\', \\'pear\\', \\'shark\\', \\n                  \\'chimpanzee\\', \\'hamster\\', \\'train\\', \\'cra\\', \\'bear\\', \\'sweet_pepper\\', \\n                  \\'pine_tree\\', \\'bed\\', \\'tank\\', \\'beaver\\', \\'bridge\\', \\'girl\\', \\n                  \\'skyscraper\\', \\'wolf\\', \\'raccoon\\', \\'rose\\', \\'worm\\', \\'table\\', \\n                  \\'pickup_truck\\', \\'turtle\\', \\'lizard\\', \\'leopard\\', \\'streetcar\\', \\n                  \\'rocket\\', \\'man\\', \\'tulip\\', \\'lamp\\', \\'seal\\', \\'lion\\', \\'butterfly\\', \\n                  \\'kangaroo\\', \\'television\\', \\'ray\\', \\'chair\\', \\'skunk\\', \\'trout\\', \\n                  \\'possum\\', \\'mouse\\']\\n\\n# Process images in the \\'images\\' folder\\nimage_folder = \\'images\\'\\noutput_file = \\'output.csv\\'\\n\\nwith open(output_file, \\'w\\', newline=\\'\\') as csvfile:\\n    fieldnames = [\\'file_name\\', \\'prediction\\']\\n    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\\n\\n    writer.writeheader()\\n\\n    for filename in tqdm(os.listdir(image_folder)):\\n        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\\n            image_path = os.path.join(image_folder, filename)\\n            image = Image.open(image_path)\\n\\n            # Split image into 9 sub-images and predict each\\n            sub_images = []\\n            for i in range(3):\\n                for j in range(3):\\n                    left = j * 32\\n                    top = i * 32\\n                    right = left + 32\\n                    bottom = top + 32\\n                    sub_image = image.crop((left, top, right, bottom))\\n                    sub_images.append(sub_image)\\n\\n            predictions = []\\n            for sub_image in sub_images:\\n                inputs = processor(images=sub_image, return_tensors=\"pt\")\\n                with torch.no_grad():\\n                    outputs = model(**inputs)\\n                    logits = outputs.logits\\n                    predicted_label = logits.argmax(-1).item()\\n                    predictions.append(output_classes[predicted_label])\\n\\n            # Determine the most common theme\\n            from collections import Counter\\n            counter = Counter(predictions)\\n            most_common_theme = counter.most_common(1)[0][0]\\n\\n            # Write result to CSV\\n            writer.writerow({\\'file_name\\': filename, \\'prediction\\': most_common_theme})\\n```'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coding_state[\"code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "baa83050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/codespace/.python/current/lib/python312.zip', '/home/codespace/.python/current/lib/python3.12', '/home/codespace/.python/current/lib/python3.12/lib-dynload', '', '/home/codespace/.local/lib/python3.12/site-packages', '/home/codespace/.python/current/lib/python3.12/site-packages', '..', '..', '..']\n"
     ]
    }
   ],
   "source": [
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "129fd865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/56/7e/67c3fe2b8c33f40af06326a3d6ae7776b3e3a01daa8f71d125d78594d874/torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting huggingface\n",
      "  Obtaining dependency information for huggingface from https://files.pythonhosted.org/packages/f4/8c/e61fbc39c0a37140e1d4941c4af29e2d53bacf9f4559e3de24d8f4e484f0/huggingface-0.0.1-py3-none-any.whl.metadata\n",
      "  Using cached huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface_hub\n",
      "  Obtaining dependency information for huggingface_hub from https://files.pythonhosted.org/packages/33/fb/53587a89fbc00799e4179796f51b3ad713c5de6bb680b2becb6d37c94649/huggingface_hub-0.33.0-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tqdm\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/01/a5/931fc3ad333d9d87b10107d948d757d67ebcfc33b1988d5faccc39c6845c/pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/b7/f6/bc47f5fa666d5ff4145254f9e618d56e6a4ef9b874654ca74c19113bb538/numpy-2.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached numpy-2.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting safetensors\n",
      "  Obtaining dependency information for safetensors from https://files.pythonhosted.org/packages/a6/f8/dae3421624fcc87a89d42e1898a798bc7ff72c61f38973a65d60df8f124c/safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/96/f2/25b27b396af03d5b64e61976b14f7209e2939e9e806c10749b6d277c273e/transformers-4.52.4-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting pillow\n",
      "  Obtaining dependency information for pillow from https://files.pythonhosted.org/packages/fe/7c/d8b1330458e4d2f3f45d9508796d7caf0c0d3764c00c823d10f6f1a3b76d/pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/4d/36/2a115987e2d8c300a974597416d9de88f2444426de9571f4b59b2cca3acc/filelock-3.18.0-py3-none-any.whl.metadata\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Obtaining dependency information for typing-extensions>=4.10.0 from https://files.pythonhosted.org/packages/69/e0/552843e0d356fbb5256d21449fa957fa4eff3bbc135a74a691ee70c7c5da/typing_extensions-4.14.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Obtaining dependency information for setuptools from https://files.pythonhosted.org/packages/a3/dc/17031897dae0efacfea57dfd3a82fdd2a2aeb58e0ff71b77b87e44edc772/setuptools-80.9.0-py3-none-any.whl.metadata\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Obtaining dependency information for sympy>=1.13.3 from https://files.pythonhosted.org/packages/a2/09/77d55d46fd61b4a135c444fc97158ef34a095e5681d0a6c10b75bf356191/sympy-1.14.0-py3-none-any.whl.metadata\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Obtaining dependency information for networkx from https://files.pythonhosted.org/packages/eb/8d/776adee7bbf76365fdd7f2552710282c79a4ead5d2a46408c9043a2b70ba/networkx-3.5-py3-none-any.whl.metadata\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Obtaining dependency information for jinja2 from https://files.pythonhosted.org/packages/62/a1/3d680cbfd5f4b8f15abc1d571870c5fc3e594bb582bc3b64ea099db13e56/jinja2-3.1.6-py3-none-any.whl.metadata\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/bb/61/78c7b3851add1481b048b5fdc29067397a1784e2910592bc81bb3f608635/fsspec-2025.5.1-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Obtaining dependency information for nvidia-cuda-nvrtc-cu12==12.6.77 from https://files.pythonhosted.org/packages/75/2e/46030320b5a80661e88039f59060d1790298b4718944a65a7f2aeda3d9e9/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Obtaining dependency information for nvidia-cuda-runtime-cu12==12.6.77 from https://files.pythonhosted.org/packages/e1/23/e717c5ac26d26cf39a27fbc076240fad2e3b817e5889d671b67f4f9f49c5/nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Obtaining dependency information for nvidia-cuda-cupti-cu12==12.6.80 from https://files.pythonhosted.org/packages/49/60/7b6497946d74bcf1de852a21824d63baad12cd417db4195fc1bfe59db953/nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Obtaining dependency information for nvidia-cudnn-cu12==9.5.1.17 from https://files.pythonhosted.org/packages/2a/78/4535c9c7f859a64781e43c969a3a7e84c54634e319a996d43ef32ce46f83/nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Obtaining dependency information for nvidia-cublas-cu12==12.6.4.1 from https://files.pythonhosted.org/packages/af/eb/ff4b8c503fa1f1796679dce648854d58751982426e4e4b37d6fce49d259c/nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Obtaining dependency information for nvidia-cufft-cu12==11.3.0.4 from https://files.pythonhosted.org/packages/8f/16/73727675941ab8e6ffd86ca3a4b7b47065edcca7a997920b831f8147c99d/nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Obtaining dependency information for nvidia-curand-cu12==10.3.7.77 from https://files.pythonhosted.org/packages/73/1b/44a01c4e70933637c93e6e1a8063d1e998b50213a6b65ac5a9169c47e98e/nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Obtaining dependency information for nvidia-cusolver-cu12==11.7.1.2 from https://files.pythonhosted.org/packages/f0/6e/c2cf12c9ff8b872e92b4a5740701e51ff17689c4d726fca91875b07f655d/nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Obtaining dependency information for nvidia-cusparse-cu12==12.5.4.2 from https://files.pythonhosted.org/packages/06/1e/b8b7c2f4099a37b96af5c9bb158632ea9e5d9d27d7391d7eb8fc45236674/nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Obtaining dependency information for nvidia-cusparselt-cu12==0.6.3 from https://files.pythonhosted.org/packages/3b/9a/72ef35b399b0e183bc2e8f6f558036922d453c4d8237dab26c666a04244b/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Obtaining dependency information for nvidia-nccl-cu12==2.26.2 from https://files.pythonhosted.org/packages/67/ca/f42388aed0fddd64ade7493dbba36e1f534d4e6fdbdd355c6a90030ae028/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Obtaining dependency information for nvidia-nvtx-cu12==12.6.77 from https://files.pythonhosted.org/packages/56/9a/fff8376f8e3d084cd1530e1ef7b879bb7d6d265620c95c1b322725c694f4/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Obtaining dependency information for nvidia-nvjitlink-cu12==12.6.85 from https://files.pythonhosted.org/packages/9d/d7/c5383e47c7e9bf1c99d5bd2a8c935af2b6d705ad831a7ec5c97db4d82f4f/nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Obtaining dependency information for nvidia-cufile-cu12==1.11.1.6 from https://files.pythonhosted.org/packages/b2/66/cc9876340ac68ae71b15c743ddb13f8b30d5244af344ec8322b449e35426/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch)\n",
      "  Obtaining dependency information for triton==3.3.1 from https://files.pythonhosted.org/packages/24/5f/950fb373bf9c01ad4eb5a8cd5eaf32cdf9e238c02f9293557a2129b9c4ac/triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting packaging>=20.9 (from huggingface_hub)\n",
      "  Obtaining dependency information for packaging>=20.9 from https://files.pythonhosted.org/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl.metadata\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface_hub)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/b9/2b/614b4752f2e127db5cc206abc23a8c19678e92b23c3db30fc86ab731d3bd/PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface_hub)\n",
      "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/7c/e4/56027c4a6b4ae70ca9de302488c5ca95ad4a39e190093d6c1a8ace08341b/requests-2.32.4-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub)\n",
      "  Obtaining dependency information for hf-xet<2.0.0,>=1.1.2 from https://files.pythonhosted.org/packages/d1/a8/49a81d4f81b0d21cc758b6fca3880a85ca0d209e8425c8b3a6ef694881ca/hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Obtaining dependency information for python-dateutil>=2.8.2 from https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/fb/13/e3b075031a738c9598c51cfbc4c7879e26729c53aa9cca59211c44235314/regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.22,>=0.21 from https://files.pythonhosted.org/packages/8a/63/38be071b0c8e06840bc6046991636bcb30c27f6bb1e670f4f4bc87cf49cc/tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
      "  Obtaining dependency information for six>=1.5 from https://files.pythonhosted.org/packages/b7/ce/149a00dd41f10bc29e5921b496af8b574d8413afcd5e30dfa0ed46c2cc5e/six-1.17.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Obtaining dependency information for mpmath<1.4,>=1.1.0 from https://files.pythonhosted.org/packages/43/e3/7d92a15f894aa0c9c4b49b8ee9ac9850d6e63b03c9c32c0367a13ae62209/mpmath-1.3.0-py3-none-any.whl.metadata\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/f3/f0/89e7aadfb3749d0f52234a0c8c7867877876e0a20b60e2188e9850794c17/MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->huggingface_hub)\n",
      "  Obtaining dependency information for charset_normalizer<4,>=2 from https://files.pythonhosted.org/packages/8c/73/6ede2ec59bce19b3edf4209d70004253ec5f4e319f9a2e3f2f15601ed5f7/charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface_hub)\n",
      "  Obtaining dependency information for idna<4,>=2.5 from https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl.metadata\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface_hub)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/a7/c2/fe1e52489ae3122415c51f387e221dd0773709bad6c6cdaa599e8a2c5185/urllib3-2.5.0-py3-none-any.whl.metadata\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface_hub)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/84/ae/320161bd181fc06471eed047ecce67b693fd7515b16d495d8932db763426/certifi-2025.6.15-py3-none-any.whl.metadata\n",
      "  Using cached certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Using cached torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl (821.0 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "Using cached huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
      "Using cached huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "Using cached numpy-2.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Using cached pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Installing collected packages: pytz, nvidia-cusparselt-cu12, mpmath, huggingface, urllib3, tzdata, typing-extensions, tqdm, sympy, six, setuptools, safetensors, regex, pyyaml, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, hf-xet, fsspec, filelock, charset_normalizer, certifi, triton, requests, python-dateutil, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, pandas, nvidia-cusolver-cu12, huggingface_hub, torch, tokenizers, transformers\n",
      "Successfully installed MarkupSafe-3.0.2 certifi-2025.6.15 charset_normalizer-3.4.2 filelock-3.18.0 fsspec-2025.5.1 hf-xet-1.1.4 huggingface-0.0.1 huggingface_hub-0.33.0 idna-3.10 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 numpy-2.3.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 packaging-25.0 pandas-2.3.0 pillow-11.2.1 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.4 safetensors-0.5.3 setuptools-80.9.0 six-1.17.0 sympy-1.14.0 tokenizers-0.21.1 torch-2.7.1 tqdm-4.67.1 transformers-4.52.4 triton-3.3.1 typing-extensions-4.14.0 tzdata-2025.2 urllib3-2.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/workspaces/ISE-challenge/Test/code_runner_9cyxo5ti/venv/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/201 [00:00<?, ?it/s]\n",
      "  0%|          | 0/201 [00:02<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/ISE-challenge/Test/code_runner_9cyxo5ti/project/main.py\", line 63, in <module>\n",
      "    predictions.append(output_classes[predicted_label])\n",
      "                       ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "IndexError: list index out of range\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Execution failed:\n\n  0%|          | 0/201 [00:00<?, ?it/s]\n  0%|          | 0/201 [00:02<?, ?it/s]\nTraceback (most recent call last):\n  File \"/workspaces/ISE-challenge/Test/code_runner_9cyxo5ti/project/main.py\", line 63, in <module>\n    predictions.append(output_classes[predicted_label])\n                       ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\nIndexError: list index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[168]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcode_execute_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoding_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcode\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m```python\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m```\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuggingface\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuggingface_hub\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtqdm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpandas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnumpy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafetensors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtransformers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpillow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[32;43m1200000\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[163]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mcode_execute_agent\u001b[39m\u001b[34m(input_path, output_path, code, requirements, timeout)\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCode execution timed out.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m process.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecution failed:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(log_lines)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Step 6: Copy specific output file to output_path\u001b[39;00m\n\u001b[32m     90\u001b[39m produced_file = project_dir / output_path.name\n",
      "\u001b[31mRuntimeError\u001b[39m: Execution failed:\n\n  0%|          | 0/201 [00:00<?, ?it/s]\n  0%|          | 0/201 [00:02<?, ?it/s]\nTraceback (most recent call last):\n  File \"/workspaces/ISE-challenge/Test/code_runner_9cyxo5ti/project/main.py\", line 63, in <module>\n    predictions.append(output_classes[predicted_label])\n                       ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\nIndexError: list index out of range\n"
     ]
    }
   ],
   "source": [
    "code_execute_agent(\n",
    "    \"images\",\n",
    "    \"output.csv\",\n",
    "    coding_state[\"code\"].replace(\"```python\", \"\").replace(\"```\", \"\"),\n",
    "    [\"torch\", \"huggingface\", \"huggingface_hub\", \"tqdm\", \"pandas\", \"numpy\", \"safetensors\", \"transformers\", \"pillow\"],\n",
    "    1200000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe40576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
