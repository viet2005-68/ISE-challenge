{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d3e60a",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "535f3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY='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'\n",
    "FIRECRAWL_API_KEY='fc-e36282d62d1c4ee8971ec0649f3862c7'\n",
    "BASE_URL='https://api.groq.com/openai/v1'\n",
    "MODEL=\"meta-llama/llama-4-scout-17b-16e-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42744f82",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "91bed7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import Tool\n",
    "from firecrawl import FirecrawlApp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36830718",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "5827b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task = pd.read_csv(\"/mnt/g/Ai agents course/LangGraph/Challenge2/data/ISE - AutoCode Challenge 2 Public - Public task.csv\")\n",
    "df_model = pd.read_csv(\"/mnt/g/Ai agents course/LangGraph/Challenge2/data/ISE - AutoCode Challenge 2 Public - Model zoo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "22abc01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STT</th>\n",
       "      <th>Task</th>\n",
       "      <th>Validation data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt tay ...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nTrong th·ªùi ƒë·∫°i th√¥ng tin...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nƒê√¢y l√† m·ªôt nhi·ªám v·ª• h·ªìi ...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nƒêi·ªÅu quan tr·ªçng l√† c√°c c...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nTrong nhi·ªÅu ·ª©ng d·ª•ng th·ª±...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STT                                               Task Validation data\n",
       "0    1  \"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt tay ...            Link\n",
       "1    2  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nTrong th·ªùi ƒë·∫°i th√¥ng tin...            Link\n",
       "2    3  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nƒê√¢y l√† m·ªôt nhi·ªám v·ª• h·ªìi ...            Link\n",
       "3    4  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nƒêi·ªÅu quan tr·ªçng l√† c√°c c...            Link\n",
       "4    5  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nTrong nhi·ªÅu ·ª©ng d·ª•ng th·ª±...            Link"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "697add1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = df_task[\"Task\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "953cae02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt tay l√† m·ªôt b√†i to√°n c∆° b·∫£n trong lƒ©nh v·ª±c h·ªçc m√°y v√† x·ª≠ l√Ω ·∫£nh, v·ªõi nhi·ªÅu ·ª©ng d·ª•ng th·ª±c ti·ªÖn nh∆∞ nh·∫≠n d·∫°ng ch·ªØ s·ªë tr√™n phi·∫øu kh·∫£o s√°t, h√≥a ƒë∆°n hay b√†i thi t·ª± ƒë·ªông. Vi·ªác ph√¢n lo·∫°i ch·ªØ s·ªë vi·∫øt tay th√†nh s·ªë nguy√™n t·ªë ho·∫∑c kh√¥ng gi√∫p m·ªü r·ªông kh·∫£ nƒÉng ·ª©ng d·ª•ng trong c√°c b√†i to√°n to√°n h·ªçc t·ª± ƒë·ªông, ki·ªÉm tra b√†i t·∫≠p, ho·∫∑c c√°c ·ª©ng d·ª•ng gi√°o d·ª•c.\\n\\nY√™u c·∫ßu c·ª• th·ªÉ c·∫ßn ƒë·∫°t ƒë∆∞·ª£c:\\nX√¢y d·ª±ng m·ªôt h·ªá th·ªëng c√≥ kh·∫£ nƒÉng nh·∫≠n di·ªán ch·ªØ s·ªë vi·∫øt tay t·ª´ ·∫£nh v√† x√°c ƒë·ªãnh xem s·ªë ƒë√≥ c√≥ ph·∫£i l√† s·ªë nguy√™n t·ªë hay kh√¥ng.\\nƒê·∫ßu v√†o l√† ·∫£nh ch·ª©a m·ªôt ch·ªØ s·ªë vi·∫øt tay.\\nƒê·∫ßu ra l√† nh√£n ph√¢n lo·∫°i nh·ªã ph√¢n: \"\"nguy√™n t·ªë\"\" ho·∫∑c \"\"kh√¥ng nguy√™n t·ªë\"\".\\n\\nƒê·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫ßu v√†o cho b√†i to√°n t·ªïng th·ªÉ:\\nM·ªôt th∆∞ m·ª•c c√≥ t√™n l√† \"\"images\"\" ch·ª©a c√°c ·∫£nh grayscale 28x28 pixel, m·ªói ·∫£nh ch·ª©a m·ªôt ch·ªØ s·ªë vi·∫øt tay t·ª´ 0 ƒë·∫øn 9.\\n\\nƒê·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra mong mu·ªën cho b√†i to√°n t·ªïng th·ªÉ:\\nFile output.csv m·ªói h√†ng l√† k·∫øt qu·∫£ d·ª± ƒëo√°n m·ªói ·∫£nh\\nc√≥ c√°c c·ªôt:\\nfile_name: t√™n file ·∫£nh\\nprediction: nh√£n c·ªßa ·∫£nh:\\n\"\"s·ªë nguy√™n t·ªë\"\" n·∫øu s·ªë trong ·∫£nh l√† s·ªë nguy√™n t·ªë.\\n\"\"kh√¥ng nguy√™n t·ªë\"\" n·∫øu s·ªë trong ·∫£nh kh√¥ng ph·∫£i s·ªë nguy√™n t·ªë.\"'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b6909b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_list(df):\n",
    "    model_strings = []\n",
    "    for i, row in df.iterrows():\n",
    "        desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        url = str(row[2])\n",
    "        model_strings.append(f\"{i + 1} - {desc} [More info]({url})\")\n",
    "    return \"\\n\".join(model_strings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c40886",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "af3bcff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraper(url: str) -> str:\n",
    "    \"\"\"Get more details about the model using the model url\"\"\"\n",
    "    scraper = FirecrawlApp(api_key=FIRECRAWL_API_KEY)\n",
    "    scrape_status = scraper.scrape_url(\n",
    "        url,\n",
    "        formats=['markdown']\n",
    "    )\n",
    "    return scrape_status.markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "77f3a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraperParameters(BaseModel):\n",
    "    url: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b6a0e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = web_scraper('https://huggingface.co/thanhtlx/image_classification_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "52a9c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"web_scrapper\",\n",
    "        func=web_scraper,\n",
    "        description=\"Use to get more detail about the model\",\n",
    "        args_schema=WebScraperParameters\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c409f8b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "eca82430",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI( \n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "6511a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tasks(BaseModel):\n",
    "    subtask_one: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 1\"\n",
    "    )\n",
    "    subtask_two: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 2\"\n",
    "    )\n",
    "    subtask_three: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 3\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "0de6f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: HumanMessage | None\n",
    "    data: str | None\n",
    "    messages: Annotated[BaseMessage, add_messages]\n",
    "    subtasks: Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ef593964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_agent(state: AgentState) -> AgentState:\n",
    "    structured_llm = llm.with_structured_output(Tasks)\n",
    "    system_prompt = \"\"\"\n",
    "                    You are an expert in task formulation. Given a machine learning task, you will\n",
    "                    help divide the task into these subtasks:\n",
    "                    - Subtask1: for classifying the problem type and task description\n",
    "                    - Subtask2: for identifying the input of the problem\n",
    "                    - Subtask3: for identifying the output of the problem\n",
    "                    describe the each subtask as specifically as possible\n",
    "                    \"\"\"\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    messages.append(state['task'])\n",
    "    result = structured_llm.invoke(messages)\n",
    "    state['subtasks'] = result\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "14b15c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"task\": HumanMessage(content=task),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "78675acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = classify_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "bdd41986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling_agent(state: AgentState) -> AgentState:\n",
    "    system_prompt = f\"\"\"\n",
    "                    You are a machine learning expert assigned to select the **best-fit model** for a given task.\n",
    "\n",
    "                    ---\n",
    "\n",
    "                    ## üéØ Your Goal\n",
    "\n",
    "                    Given:\n",
    "                    - A user task description\n",
    "                    - A list of available models (names and links)\n",
    "                    - A tool that fetches model details from a provided URL (description, input/output format, usage code)\n",
    "\n",
    "                    Your job is to:\n",
    "                    1. Analyze the task and identify what type of problem it is (e.g., classification, regression, NLP, etc.)\n",
    "                    2. Select a small number of candidate models whose **name or purpose suggests relevance**\n",
    "                    3. Use the **provided tool** to retrieve real details about each candidate model:\n",
    "                        - ‚úÖ Model description\n",
    "                        - üì• Input format\n",
    "                        - üì§ Output format\n",
    "                        - üß™ Code sample (usage code snippet)\n",
    "\n",
    "                    ---\n",
    "\n",
    "                    ## ‚ö†Ô∏è VERY IMPORTANT RULES\n",
    "\n",
    "                    - ‚ùå DO NOT assume or invent any part of the model's description, input/output format, or code\n",
    "                    - ‚ùå DO NOT generate fake code or use your own knowledge about the model\n",
    "                    - ‚úÖ ONLY use the **actual output** returned from the tool\n",
    "                    - ‚úÖ Include tool content in your final answer exactly as returned (especially code)\n",
    "\n",
    "                    ---\n",
    "\n",
    "                    ## üì¶ Available Models:\n",
    "\n",
    "                    {format_model_list(df_model)}\n",
    "\n",
    "                    ---\n",
    "\n",
    "                    ## üß† Output Structure (Final Answer)\n",
    "\n",
    "                    After retrieving tool results, choose ONE best-fit model and output the result in the following format:\n",
    "\n",
    "                    **‚úÖ Model name and link**  \n",
    "                    `<model name>` ‚Äî `<link>`\n",
    "\n",
    "                    **üìù Description (from tool):**  \n",
    "                    <model description>\n",
    "\n",
    "                    **üì• Input format (from tool):**  \n",
    "                    <description of expected input>\n",
    "\n",
    "                    **üì§ Output format (from tool):**  \n",
    "                    <description of model output>\n",
    "\n",
    "                    **üß™ Example code (from tool):**  \n",
    "                    ```python\n",
    "                    <exact code snippet from tool>\n",
    "                    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"ai\", \"{agent_scratchpad}\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "    task_input = state[\"task\"].content if isinstance(state[\"task\"], SystemMessage) else state[\"task\"]\n",
    "\n",
    "    result = executor.invoke({\"input\": task_input})\n",
    "\n",
    "    return {\"output\": result[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "d8f92e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34131/7332665.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
      "/tmp/ipykernel_34131/7332665.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  url = str(row[2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/thanhtlx/image_classification_01'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m**YAML Metadata**\n",
      "**Warning:**\n",
      "empty or missing yaml metadata in repo card\n",
      "( [https://huggingface.co/docs/hub/model-cards#model-card-metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata))\n",
      "\n",
      "# Image Classification Model - Handwritten Digit Recognition (Color Images)\n",
      "\n",
      "## üìù M√¥ t·∫£\n",
      "\n",
      "ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c ch·ªØ s·ªë vi·∫øt tay (t·ª´ 0 ƒë·∫øn 9) trong ·∫£nh m√†u. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c Vision Transformer (ViT) v√† ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay ƒë∆∞·ª£c ch·ª•p ho·∫∑c scan d∆∞·ªõi d·∫°ng ·∫£nh m√†u.\n",
      "\n",
      "## üìå Chi ti·∫øt c√°c m√¥ h√¨nh\n",
      "\n",
      "Ph√¢n lo·∫°i ·∫£nh ch·ªØ s·ªë vi·∫øt tay t·ª´ 0 ƒë·∫øn 9\n",
      "\n",
      "## üì• ƒê·∫ßu v√†o\n",
      "\n",
      "- **ƒê·ªãnh d·∫°ng**: ·∫¢nh m√†u (RGB)\n",
      "- **Ki·ªÉu d·ªØ li·ªáu**: `PIL.Image.Image`\n",
      "- **K√≠ch th∆∞·ªõc ·∫£nh**: M√¥ h√¨nh s·∫Ω t·ª± resize theo c·∫•u h√¨nh c·ªßa `ViTImageProcessor` (th∆∞·ªùng l√† 224x224)\n",
      "- **Gi√° tr·ªã pixel**: \\[0, 255\\] (chu·∫©n h√≥a t·ª± ƒë·ªông b·ªüi processor)\n",
      "\n",
      "## üì§ ƒê·∫ßu ra\n",
      "\n",
      "- M·ªôt s·ªë nguy√™n t·ª´ **0 ƒë·∫øn 9**, l√† k·∫øt qu·∫£ ph√¢n lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi ch·ªØ s·ªë xu·∫•t hi·ªán trong ·∫£nh.\n",
      "\n",
      "## üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng `pip`:\n",
      "\n",
      "```bash\n",
      "pip install transformers torch\n",
      "\n",
      "```\n",
      "\n",
      "## üß™ S·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "\n",
      "D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n m√£ m·∫´u ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh:\n",
      "\n",
      "```python\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "import torch\n",
      "\n",
      "model_name = \"thanhtlx/image_classification_01\"\n",
      "model = ViTForImageClassification.from_pretrained(model_name)\n",
      "processor = ViTImageProcessor.from_pretrained(model_name)\n",
      "\n",
      "# ƒê·ªçc ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"images.png\")\n",
      "\n",
      "# X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "\n",
      "# L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
      "predicted_class = outputs.logits.argmax(-1).item()\n",
      "print(f\"prediction: {predicted_class}\")\n",
      "\n",
      "```\n",
      "\n",
      "Downloads last month324\n",
      "\n",
      "\n",
      "Inference Providers [NEW](https://huggingface.co/docs/inference-providers)\n",
      "\n",
      "This model isn't deployed by any Inference Provider. [üôãAsk for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=thanhtlx/image_classification_01&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bthanhtlx%2Fimage_classification_01%5D(%2Fthanhtlx%2Fimage_classification_01)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A)\n",
      "\n",
      "Inference providers allow you to run inference using different serverless providers.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/thanhtlx/image_classification_01'}`\n",
      "responded: To solve this task, we need to analyze the problem description and select a suitable model from the provided list. The task involves handwritten digit recognition and determining whether the digit is a prime number or not.\n",
      "\n",
      "## Step 1: Analyze the task description\n",
      "The task requires building a system that can recognize handwritten digits from images and classify them as prime or non-prime numbers. The input is an image containing a handwritten digit, and the output is a binary classification label: \"nguy√™n t·ªë\" (prime) or \"kh√¥ng nguy√™n t·ªë\" (non-prime).\n",
      "\n",
      "## Step 2: Identify relevant models\n",
      "From the list of available models, we can see that model 1 is specifically designed for handwritten digit recognition: \"Nh·∫≠n di·ªán ·∫£nh ch·ªØ vi·∫øt tay\" (Handwritten digit recognition). This model seems to be the most relevant for our task.\n",
      "\n",
      "## Step 3: Retrieve model details using the web_scrapper tool\n",
      "We will use the web_scrapper tool to retrieve more details about model 1.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m**YAML Metadata**\n",
      "**Warning:**\n",
      "empty or missing yaml metadata in repo card\n",
      "( [https://huggingface.co/docs/hub/model-cards#model-card-metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata))\n",
      "\n",
      "# Image Classification Model - Handwritten Digit Recognition (Color Images)\n",
      "\n",
      "## üìù M√¥ t·∫£\n",
      "\n",
      "ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c ch·ªØ s·ªë vi·∫øt tay (t·ª´ 0 ƒë·∫øn 9) trong ·∫£nh m√†u. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c Vision Transformer (ViT) v√† ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay ƒë∆∞·ª£c ch·ª•p ho·∫∑c scan d∆∞·ªõi d·∫°ng ·∫£nh m√†u.\n",
      "\n",
      "## üìå Chi ti·∫øt c√°c m√¥ h√¨nh\n",
      "\n",
      "Ph√¢n lo·∫°i ·∫£nh ch·ªØ s·ªë vi·∫øt tay t·ª´ 0 ƒë·∫øn 9\n",
      "\n",
      "## üì• ƒê·∫ßu v√†o\n",
      "\n",
      "- **ƒê·ªãnh d·∫°ng**: ·∫¢nh m√†u (RGB)\n",
      "- **Ki·ªÉu d·ªØ li·ªáu**: `PIL.Image.Image`\n",
      "- **K√≠ch th∆∞·ªõc ·∫£nh**: M√¥ h√¨nh s·∫Ω t·ª± resize theo c·∫•u h√¨nh c·ªßa `ViTImageProcessor` (th∆∞·ªùng l√† 224x224)\n",
      "- **Gi√° tr·ªã pixel**: \\[0, 255\\] (chu·∫©n h√≥a t·ª± ƒë·ªông b·ªüi processor)\n",
      "\n",
      "## üì§ ƒê·∫ßu ra\n",
      "\n",
      "- M·ªôt s·ªë nguy√™n t·ª´ **0 ƒë·∫øn 9**, l√† k·∫øt qu·∫£ ph√¢n lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi ch·ªØ s·ªë xu·∫•t hi·ªán trong ·∫£nh.\n",
      "\n",
      "## üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng `pip`:\n",
      "\n",
      "```bash\n",
      "pip install transformers torch\n",
      "\n",
      "```\n",
      "\n",
      "## üß™ S·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "\n",
      "D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n m√£ m·∫´u ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh:\n",
      "\n",
      "```python\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "import torch\n",
      "\n",
      "model_name = \"thanhtlx/image_classification_01\"\n",
      "model = ViTForImageClassification.from_pretrained(model_name)\n",
      "processor = ViTImageProcessor.from_pretrained(model_name)\n",
      "\n",
      "# ƒê·ªçc ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"images.png\")\n",
      "\n",
      "# X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "\n",
      "# L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
      "predicted_class = outputs.logits.argmax(-1).item()\n",
      "print(f\"prediction: {predicted_class}\")\n",
      "\n",
      "```\n",
      "\n",
      "Downloads last month324\n",
      "\n",
      "\n",
      "Inference Providers [NEW](https://huggingface.co/docs/inference-providers)\n",
      "\n",
      "This model isn't deployed by any Inference Provider. [üôãAsk for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=thanhtlx/image_classification_01&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bthanhtlx%2Fimage_classification_01%5D(%2Fthanhtlx%2Fimage_classification_01)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A)\n",
      "\n",
      "Inference providers allow you to run inference using different serverless providers.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/thanhtlx/image_classification_01'}`\n",
      "responded: ## Step 1: Analyze the task description\n",
      "The task requires building a system that can recognize handwritten digits from images and classify them as prime or non-prime numbers. The input is an image containing a handwritten digit, and the output is a binary classification label: \"nguy√™n t·ªë\" (prime) or \"kh√¥ng nguy√™n t·ªë\" (non-prime).\n",
      "\n",
      "## Step 2: Identify relevant models\n",
      "From the list of available models, we can see that model 1 is specifically designed for handwritten digit recognition: \"Nh·∫≠n di·ªán ·∫£nh ch·ªØ vi·∫øt tay\" (Handwritten digit recognition). This model seems to be the most relevant for our task.\n",
      "\n",
      "## Step 3: Retrieve model details using the web_scrapper tool\n",
      "We will use the web_scrapper tool to retrieve more details about model 1.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m**YAML Metadata**\n",
      "**Warning:**\n",
      "empty or missing yaml metadata in repo card\n",
      "( [https://huggingface.co/docs/hub/model-cards#model-card-metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata))\n",
      "\n",
      "# Image Classification Model - Handwritten Digit Recognition (Color Images)\n",
      "\n",
      "## üìù M√¥ t·∫£\n",
      "\n",
      "ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c ch·ªØ s·ªë vi·∫øt tay (t·ª´ 0 ƒë·∫øn 9) trong ·∫£nh m√†u. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c Vision Transformer (ViT) v√† ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay ƒë∆∞·ª£c ch·ª•p ho·∫∑c scan d∆∞·ªõi d·∫°ng ·∫£nh m√†u.\n",
      "\n",
      "## üìå Chi ti·∫øt c√°c m√¥ h√¨nh\n",
      "\n",
      "Ph√¢n lo·∫°i ·∫£nh ch·ªØ s·ªë vi·∫øt tay t·ª´ 0 ƒë·∫øn 9\n",
      "\n",
      "## üì• ƒê·∫ßu v√†o\n",
      "\n",
      "- **ƒê·ªãnh d·∫°ng**: ·∫¢nh m√†u (RGB)\n",
      "- **Ki·ªÉu d·ªØ li·ªáu**: `PIL.Image.Image`\n",
      "- **K√≠ch th∆∞·ªõc ·∫£nh**: M√¥ h√¨nh s·∫Ω t·ª± resize theo c·∫•u h√¨nh c·ªßa `ViTImageProcessor` (th∆∞·ªùng l√† 224x224)\n",
      "- **Gi√° tr·ªã pixel**: \\[0, 255\\] (chu·∫©n h√≥a t·ª± ƒë·ªông b·ªüi processor)\n",
      "\n",
      "## üì§ ƒê·∫ßu ra\n",
      "\n",
      "- M·ªôt s·ªë nguy√™n t·ª´ **0 ƒë·∫øn 9**, l√† k·∫øt qu·∫£ ph√¢n lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi ch·ªØ s·ªë xu·∫•t hi·ªán trong ·∫£nh.\n",
      "\n",
      "## üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng `pip`:\n",
      "\n",
      "```bash\n",
      "pip install transformers torch\n",
      "\n",
      "```\n",
      "\n",
      "## üß™ S·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "\n",
      "D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n m√£ m·∫´u ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh:\n",
      "\n",
      "```python\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "import torch\n",
      "\n",
      "model_name = \"thanhtlx/image_classification_01\"\n",
      "model = ViTForImageClassification.from_pretrained(model_name)\n",
      "processor = ViTImageProcessor.from_pretrained(model_name)\n",
      "\n",
      "# ƒê·ªçc ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"images.png\")\n",
      "\n",
      "# X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "\n",
      "# L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
      "predicted_class = outputs.logits.argmax(-1).item()\n",
      "print(f\"prediction: {predicted_class}\")\n",
      "\n",
      "```\n",
      "\n",
      "Downloads last month324\n",
      "\n",
      "\n",
      "Inference Providers [NEW](https://huggingface.co/docs/inference-providers)\n",
      "\n",
      "This model isn't deployed by any Inference Provider. [üôãAsk for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=thanhtlx/image_classification_01&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bthanhtlx%2Fimage_classification_01%5D(%2Fthanhtlx%2Fimage_classification_01)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A)\n",
      "\n",
      "Inference providers allow you to run inference using different serverless providers.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/thanhtlx/image_classification_01'}`\n",
      "responded: ## Step 1: Analyze the task description\n",
      "The task requires building a system that can recognize handwritten digits from images and classify them as prime or non-prime numbers. The input is an image containing a handwritten digit, and the output is a binary classification label: \"nguy√™n t·ªë\" (prime) or \"kh√¥ng nguy√™n t·ªë\" (non-prime).\n",
      "\n",
      "## Step 2: Identify relevant models\n",
      "From the list of available models, we can see that model1 is specifically designed for handwritten digit recognition: \"Nh·∫≠n di·ªán ·∫£nh ch·ªØ vi·∫øt tay\" (Handwritten digit recognition). This model seems to be the most relevant for our task.\n",
      "\n",
      "## Step 3: Retrieve model details using the web_scrapper tool\n",
      "We will use the web_scrapper tool to retrieve more details about model1.\n",
      "\n",
      "## Step 4: Evaluate model details\n",
      "Based on the model details retrieved, we will evaluate if the model fits the task requirements.\n",
      "\n",
      "## Step 5: Select the best-fit model\n",
      "After evaluating the model details, we will select the best-fit model for the task.\n",
      "\n",
      "## Step 6: Provide the final answer\n",
      "We will provide the final answer in the required format.\n",
      "\n",
      "## Step 7: Implementation\n",
      "Let's implement the solution.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m**YAML Metadata**\n",
      "**Warning:**\n",
      "empty or missing yaml metadata in repo card\n",
      "( [https://huggingface.co/docs/hub/model-cards#model-card-metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata))\n",
      "\n",
      "# Image Classification Model - Handwritten Digit Recognition (Color Images)\n",
      "\n",
      "## üìù M√¥ t·∫£\n",
      "\n",
      "ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c ch·ªØ s·ªë vi·∫øt tay (t·ª´ 0 ƒë·∫øn 9) trong ·∫£nh m√†u. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c Vision Transformer (ViT) v√† ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay ƒë∆∞·ª£c ch·ª•p ho·∫∑c scan d∆∞·ªõi d·∫°ng ·∫£nh m√†u.\n",
      "\n",
      "## üìå Chi ti·∫øt c√°c m√¥ h√¨nh\n",
      "\n",
      "Ph√¢n lo·∫°i ·∫£nh ch·ªØ s·ªë vi·∫øt tay t·ª´ 0 ƒë·∫øn 9\n",
      "\n",
      "## üì• ƒê·∫ßu v√†o\n",
      "\n",
      "- **ƒê·ªãnh d·∫°ng**: ·∫¢nh m√†u (RGB)\n",
      "- **Ki·ªÉu d·ªØ li·ªáu**: `PIL.Image.Image`\n",
      "- **K√≠ch th∆∞·ªõc ·∫£nh**: M√¥ h√¨nh s·∫Ω t·ª± resize theo c·∫•u h√¨nh c·ªßa `ViTImageProcessor` (th∆∞·ªùng l√† 224x224)\n",
      "- **Gi√° tr·ªã pixel**: \\[0, 255\\] (chu·∫©n h√≥a t·ª± ƒë·ªông b·ªüi processor)\n",
      "\n",
      "## üì§ ƒê·∫ßu ra\n",
      "\n",
      "- M·ªôt s·ªë nguy√™n t·ª´ **0 ƒë·∫øn 9**, l√† k·∫øt qu·∫£ ph√¢n lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi ch·ªØ s·ªë xu·∫•t hi·ªán trong ·∫£nh.\n",
      "\n",
      "## üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng `pip`:\n",
      "\n",
      "```bash\n",
      "pip install transformers torch\n",
      "\n",
      "```\n",
      "\n",
      "## üß™ S·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "\n",
      "D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n m√£ m·∫´u ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh:\n",
      "\n",
      "```python\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "import torch\n",
      "\n",
      "model_name = \"thanhtlx/image_classification_01\"\n",
      "model = ViTForImageClassification.from_pretrained(model_name)\n",
      "processor = ViTImageProcessor.from_pretrained(model_name)\n",
      "\n",
      "# ƒê·ªçc ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"images.png\")\n",
      "\n",
      "# X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "\n",
      "# L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
      "predicted_class = outputs.logits.argmax(-1).item()\n",
      "print(f\"prediction: {predicted_class}\")\n",
      "\n",
      "```\n",
      "\n",
      "Downloads last month324\n",
      "\n",
      "\n",
      "Inference Providers [NEW](https://huggingface.co/docs/inference-providers)\n",
      "\n",
      "This model isn't deployed by any Inference Provider. [üôãAsk for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=thanhtlx/image_classification_01&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bthanhtlx%2Fimage_classification_01%5D(%2Fthanhtlx%2Fimage_classification_01)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A)\n",
      "\n",
      "Inference providers allow you to run inference using different serverless providers.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/thanhtlx/image_classification_01'}`\n",
      "responded: ## Step 1: Analyze the task description\n",
      "The task requires building a system that can recognize handwritten digits from images and classify them as prime or non-prime numbers. The input is an image containing a handwritten digit, and the output is a binary classification label: \"nguy√™n t·ªë\" (prime) or \"kh√¥ng nguy√™n t·ªë\" (non-prime).\n",
      "\n",
      "## Step 2: Identify relevant models\n",
      "From the list of available models, we can see that model1 is specifically designed for handwritten digit recognition: \"Nh·∫≠n di·ªán ·∫£nh ch·ªØ vi·∫øt tay\" (Handwritten digit recognition). This model seems to be the most relevant for our task.\n",
      "\n",
      "## Step 3: Retrieve model details using the web_scrapper tool\n",
      "We will use the web_scrapper tool to retrieve more details about model1.\n",
      "\n",
      "## Step 4: Evaluate model details\n",
      "Based on the model details retrieved, we will evaluate if the model fits the task requirements.\n",
      "\n",
      "## Step 5: Select the best-fit model\n",
      "After evaluating the model details, we will select the best-fit model for the task.\n",
      "\n",
      "## Step 6: Provide the final answer\n",
      "We will provide the final answer in the required format.\n",
      "\n",
      "## Step 7: Implementation\n",
      "Let's implement the solution.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m**YAML Metadata**\n",
      "**Warning:**\n",
      "empty or missing yaml metadata in repo card\n",
      "( [https://huggingface.co/docs/hub/model-cards#model-card-metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata))\n",
      "\n",
      "# Image Classification Model - Handwritten Digit Recognition (Color Images)\n",
      "\n",
      "## üìù M√¥ t·∫£\n",
      "\n",
      "ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c ch·ªØ s·ªë vi·∫øt tay (t·ª´ 0 ƒë·∫øn 9) trong ·∫£nh m√†u. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c Vision Transformer (ViT) v√† ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay ƒë∆∞·ª£c ch·ª•p ho·∫∑c scan d∆∞·ªõi d·∫°ng ·∫£nh m√†u.\n",
      "\n",
      "## üìå Chi ti·∫øt c√°c m√¥ h√¨nh\n",
      "\n",
      "Ph√¢n lo·∫°i ·∫£nh ch·ªØ s·ªë vi·∫øt tay t·ª´ 0 ƒë·∫øn 9\n",
      "\n",
      "## üì• ƒê·∫ßu v√†o\n",
      "\n",
      "- **ƒê·ªãnh d·∫°ng**: ·∫¢nh m√†u (RGB)\n",
      "- **Ki·ªÉu d·ªØ li·ªáu**: `PIL.Image.Image`\n",
      "- **K√≠ch th∆∞·ªõc ·∫£nh**: M√¥ h√¨nh s·∫Ω t·ª± resize theo c·∫•u h√¨nh c·ªßa `ViTImageProcessor` (th∆∞·ªùng l√† 224x224)\n",
      "- **Gi√° tr·ªã pixel**: \\[0, 255\\] (chu·∫©n h√≥a t·ª± ƒë·ªông b·ªüi processor)\n",
      "\n",
      "## üì§ ƒê·∫ßu ra\n",
      "\n",
      "- M·ªôt s·ªë nguy√™n t·ª´ **0 ƒë·∫øn 9**, l√† k·∫øt qu·∫£ ph√¢n lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi ch·ªØ s·ªë xu·∫•t hi·ªán trong ·∫£nh.\n",
      "\n",
      "## üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng `pip`:\n",
      "\n",
      "```bash\n",
      "pip install transformers torch\n",
      "\n",
      "```\n",
      "\n",
      "## üß™ S·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "\n",
      "D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n m√£ m·∫´u ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh:\n",
      "\n",
      "```python\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "import torch\n",
      "\n",
      "model_name = \"thanhtlx/image_classification_01\"\n",
      "model = ViTForImageClassification.from_pretrained(model_name)\n",
      "processor = ViTImageProcessor.from_pretrained(model_name)\n",
      "\n",
      "# ƒê·ªçc ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"images.png\")\n",
      "\n",
      "# X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "\n",
      "# L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
      "predicted_class = outputs.logits.argmax(-1).item()\n",
      "print(f\"prediction: {predicted_class}\")\n",
      "\n",
      "```\n",
      "\n",
      "Downloads last month324\n",
      "\n",
      "\n",
      "Inference Providers [NEW](https://huggingface.co/docs/inference-providers)\n",
      "\n",
      "This model isn't deployed by any Inference Provider. [üôãAsk for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=thanhtlx/image_classification_01&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bthanhtlx%2Fimage_classification_01%5D(%2Fthanhtlx%2Fimage_classification_01)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A)\n",
      "\n",
      "Inference providers allow you to run inference using different serverless providers.\u001b[0m\u001b[32;1m\u001b[1;3m## Step 1: Analyze the task description\n",
      "The task requires building a system that can recognize handwritten digits from images and classify them as prime or non-prime numbers. The input is an image containing a handwritten digit, and the output is a binary classification label: \"nguy√™n t·ªë\" (prime) or \"kh√¥ng nguy√™n t·ªë\" (non-prime).\n",
      "\n",
      "## Step 2: Identify relevant models\n",
      "From the list of available models, we can see that model 1 is specifically designed for handwritten digit recognition: \"Nh·∫≠n di·ªán ·∫£nh ch·ªØ vi·∫øt tay\" (Handwritten digit recognition). This model seems to be the most relevant for our task.\n",
      "\n",
      "## Step 3: Retrieve model details using the web_scrapper tool\n",
      "We have retrieved the details of model 1 using the web_scrapper tool.\n",
      "\n",
      "## Step 4: Evaluate model details\n",
      "Based on the model details retrieved, we can see that the model can recognize handwritten digits from 0 to 9. However, the model does not directly classify the digits as prime or non-prime.\n",
      "\n",
      "## Step 5: Select the best-fit model\n",
      "Although the model does not directly classify digits as prime or non-prime, it can still be used as a starting point. We can use the model's output (the recognized digit) and then apply a prime-checking function to classify it as prime or non-prime.\n",
      "\n",
      "## Step 6: Provide the final answer\n",
      "We will provide the final answer in the required format.\n",
      "\n",
      "## Step 7: Implementation\n",
      "Let's implement the solution.\n",
      "\n",
      "The best-fit model for the task is:\n",
      "\n",
      "**‚úÖ Model name and link**  \n",
      "`image_classification_01` ‚Äî `https://huggingface.co/thanhtlx/image_classification_01`\n",
      "\n",
      "**üìù Description (from tool):**  \n",
      "ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c ch·ªØ s·ªë vi·∫øt tay (t·ª´0 ƒë·∫øn9) trong ·∫£nh m√†u. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c Vision Transformer (ViT) v√† ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay ƒë∆∞·ª£c ch·ª•p ho·∫∑c scan d∆∞·ªõi d·∫°ng ·∫£nh m√†u.\n",
      "\n",
      "**üì• Input format (from tool):**  \n",
      "- **ƒê·ªãnh d·∫°ng**: ·∫¢nh m√†u (RGB)\n",
      "- **Ki·ªÉu d·ªØ li·ªáu**: `PIL.Image.Image`\n",
      "- **K√≠ch th∆∞·ªõc ·∫£nh**: M√¥ h√¨nh s·∫Ω t·ª± resize theo c·∫•u h√¨nh c·ªßa `ViTImageProcessor` (th∆∞·ªùng l√†224x224)\n",
      "- **Gi√° tr·ªã pixel**: \\\\[0,255\\\\] (chu·∫©n h√≥a t·ª± ƒë·ªông b·ªüi processor)\n",
      "\n",
      "**üì§ Output format (from tool):**  \n",
      "- M·ªôt s·ªë nguy√™n t·ª´ **0 ƒë·∫øn9**, l√† k·∫øt qu·∫£ ph√¢n lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi ch·ªØ s·ªë xu·∫•t hi·ªán trong ·∫£nh.\n",
      "\n",
      "**üß™ Example code (from tool):**  \n",
      "```python\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "import torch\n",
      "\n",
      "model_name = \"thanhtlx/image_classification_01\"\n",
      "model = ViTForImageClassification.from_pretrained(model_name)\n",
      "processor = ViTImageProcessor.from_pretrained(model_name)\n",
      "\n",
      "# ƒê·ªçc ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"images.png\")\n",
      "\n",
      "# X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      " outputs = model(**inputs)\n",
      "\n",
      "# L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
      "predicted_class = outputs.logits.argmax(-1).item()\n",
      "print(f\"prediction: {predicted_class}\")\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res = modeling_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "3da68bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': '## Step 1: Analyze the task description\\n'\n",
      "           'The task requires building a system that can recognize handwritten '\n",
      "           'digits from images and classify them as prime or non-prime '\n",
      "           'numbers. The input is an image containing a handwritten digit, and '\n",
      "           'the output is a binary classification label: \"nguy√™n t·ªë\" (prime) '\n",
      "           'or \"kh√¥ng nguy√™n t·ªë\" (non-prime).\\n'\n",
      "           '\\n'\n",
      "           '## Step 2: Identify relevant models\\n'\n",
      "           'From the list of available models, we can see that model 1 is '\n",
      "           'specifically designed for handwritten digit recognition: \"Nh·∫≠n '\n",
      "           'di·ªán ·∫£nh ch·ªØ vi·∫øt tay\" (Handwritten digit recognition). This model '\n",
      "           'seems to be the most relevant for our task.\\n'\n",
      "           '\\n'\n",
      "           '## Step 3: Retrieve model details using the web_scrapper tool\\n'\n",
      "           'We have retrieved the details of model 1 using the web_scrapper '\n",
      "           'tool.\\n'\n",
      "           '\\n'\n",
      "           '## Step 4: Evaluate model details\\n'\n",
      "           'Based on the model details retrieved, we can see that the model '\n",
      "           'can recognize handwritten digits from 0 to 9. However, the model '\n",
      "           'does not directly classify the digits as prime or non-prime.\\n'\n",
      "           '\\n'\n",
      "           '## Step 5: Select the best-fit model\\n'\n",
      "           'Although the model does not directly classify digits as prime or '\n",
      "           'non-prime, it can still be used as a starting point. We can use '\n",
      "           \"the model's output (the recognized digit) and then apply a \"\n",
      "           'prime-checking function to classify it as prime or non-prime.\\n'\n",
      "           '\\n'\n",
      "           '## Step 6: Provide the final answer\\n'\n",
      "           'We will provide the final answer in the required format.\\n'\n",
      "           '\\n'\n",
      "           '## Step 7: Implementation\\n'\n",
      "           \"Let's implement the solution.\\n\"\n",
      "           '\\n'\n",
      "           'The best-fit model for the task is:\\n'\n",
      "           '\\n'\n",
      "           '**‚úÖ Model name and link**  \\n'\n",
      "           '`image_classification_01` ‚Äî '\n",
      "           '`https://huggingface.co/thanhtlx/image_classification_01`\\n'\n",
      "           '\\n'\n",
      "           '**üìù Description (from tool):**  \\n'\n",
      "           'ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c '\n",
      "           'ch·ªØ s·ªë vi·∫øt tay (t·ª´0 ƒë·∫øn9) trong ·∫£nh m√†u. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn '\n",
      "           'tr√∫c Vision Transformer (ViT) v√† ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p d·ªØ li·ªáu '\n",
      "           'g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay ƒë∆∞·ª£c ch·ª•p ho·∫∑c scan d∆∞·ªõi d·∫°ng ·∫£nh m√†u.\\n'\n",
      "           '\\n'\n",
      "           '**üì• Input format (from tool):**  \\n'\n",
      "           '- **ƒê·ªãnh d·∫°ng**: ·∫¢nh m√†u (RGB)\\n'\n",
      "           '- **Ki·ªÉu d·ªØ li·ªáu**: `PIL.Image.Image`\\n'\n",
      "           '- **K√≠ch th∆∞·ªõc ·∫£nh**: M√¥ h√¨nh s·∫Ω t·ª± resize theo c·∫•u h√¨nh c·ªßa '\n",
      "           '`ViTImageProcessor` (th∆∞·ªùng l√†224x224)\\n'\n",
      "           '- **Gi√° tr·ªã pixel**: \\\\\\\\[0,255\\\\\\\\] (chu·∫©n h√≥a t·ª± ƒë·ªông b·ªüi '\n",
      "           'processor)\\n'\n",
      "           '\\n'\n",
      "           '**üì§ Output format (from tool):**  \\n'\n",
      "           '- M·ªôt s·ªë nguy√™n t·ª´ **0 ƒë·∫øn9**, l√† k·∫øt qu·∫£ ph√¢n lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi '\n",
      "           'ch·ªØ s·ªë xu·∫•t hi·ªán trong ·∫£nh.\\n'\n",
      "           '\\n'\n",
      "           '**üß™ Example code (from tool):**  \\n'\n",
      "           '```python\\n'\n",
      "           'from transformers import ViTForImageClassification, '\n",
      "           'ViTImageProcessor\\n'\n",
      "           'from PIL import Image\\n'\n",
      "           'import torch\\n'\n",
      "           '\\n'\n",
      "           'model_name = \"thanhtlx/image_classification_01\"\\n'\n",
      "           'model = ViTForImageClassification.from_pretrained(model_name)\\n'\n",
      "           'processor = ViTImageProcessor.from_pretrained(model_name)\\n'\n",
      "           '\\n'\n",
      "           '# ƒê·ªçc ·∫£nh c·∫ßn ph√¢n lo·∫°i\\n'\n",
      "           'image = Image.open(\"images.png\")\\n'\n",
      "           '\\n'\n",
      "           '# X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\\n'\n",
      "           'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      "           '\\n'\n",
      "           '# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\\n'\n",
      "           'with torch.no_grad():\\n'\n",
      "           ' outputs = model(**inputs)\\n'\n",
      "           '\\n'\n",
      "           '# L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n\\n'\n",
      "           'predicted_class = outputs.logits.argmax(-1).item()\\n'\n",
      "           'print(f\"prediction: {predicted_class}\")\\n'\n",
      "           '```'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "41077f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Step 1: Analyze the task description\n",
      "The task requires building a system that can recognize handwritten digits from images and classify them as prime or non-prime numbers. The input is an image containing a handwritten digit, and the output is a binary classification label: \"nguy√™n t·ªë\" (prime) or \"kh√¥ng nguy√™n t·ªë\" (non-prime).\n",
      "\n",
      "## Step 2: Identify relevant models\n",
      "From the list of available models, we can see that model 1 is specifically designed for handwritten digit recognition: \"Nh·∫≠n di·ªán ·∫£nh ch·ªØ vi·∫øt tay\" (Handwritten digit recognition). This model seems to be the most relevant for our task.\n",
      "\n",
      "## Step 3: Retrieve model details using the web_scrapper tool\n",
      "We have retrieved the details of model 1 using the web_scrapper tool.\n",
      "\n",
      "## Step 4: Evaluate model details\n",
      "Based on the model details retrieved, we can see that the model can recognize handwritten digits from 0 to 9. However, the model does not directly classify the digits as prime or non-prime.\n",
      "\n",
      "## Step 5: Select the best-fit model\n",
      "Although the model does not directly classify digits as prime or non-prime, it can still be used as a starting point. We can use the model's output (the recognized digit) and then apply a prime-checking function to classify it as prime or non-prime.\n",
      "\n",
      "## Step 6: Provide the final answer\n",
      "We will provide the final answer in the required format.\n",
      "\n",
      "## Step 7: Implementation\n",
      "Let's implement the solution.\n",
      "\n",
      "The best-fit model for the task is:\n",
      "\n",
      "**‚úÖ Model name and link**  \n",
      "`image_classification_01` ‚Äî `https://huggingface.co/thanhtlx/image_classification_01`\n",
      "\n",
      "**üìù Description (from tool):**  \n",
      "ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c ch·ªØ s·ªë vi·∫øt tay (t·ª´0 ƒë·∫øn9) trong ·∫£nh m√†u. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c Vision Transformer (ViT) v√† ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay ƒë∆∞·ª£c ch·ª•p ho·∫∑c scan d∆∞·ªõi d·∫°ng ·∫£nh m√†u.\n",
      "\n",
      "**üì• Input format (from tool):**  \n",
      "- **ƒê·ªãnh d·∫°ng**: ·∫¢nh m√†u (RGB)\n",
      "- **Ki·ªÉu d·ªØ li·ªáu**: `PIL.Image.Image`\n",
      "- **K√≠ch th∆∞·ªõc ·∫£nh**: M√¥ h√¨nh s·∫Ω t·ª± resize theo c·∫•u h√¨nh c·ªßa `ViTImageProcessor` (th∆∞·ªùng l√†224x224)\n",
      "- **Gi√° tr·ªã pixel**: \\\\[0,255\\\\] (chu·∫©n h√≥a t·ª± ƒë·ªông b·ªüi processor)\n",
      "\n",
      "**üì§ Output format (from tool):**  \n",
      "- M·ªôt s·ªë nguy√™n t·ª´ **0 ƒë·∫øn9**, l√† k·∫øt qu·∫£ ph√¢n lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi ch·ªØ s·ªë xu·∫•t hi·ªán trong ·∫£nh.\n",
      "\n",
      "**üß™ Example code (from tool):**  \n",
      "```python\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "import torch\n",
      "\n",
      "model_name = \"thanhtlx/image_classification_01\"\n",
      "model = ViTForImageClassification.from_pretrained(model_name)\n",
      "processor = ViTImageProcessor.from_pretrained(model_name)\n",
      "\n",
      "# ƒê·ªçc ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"images.png\")\n",
      "\n",
      "# X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      " outputs = model(**inputs)\n",
      "\n",
      "# L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
      "predicted_class = outputs.logits.argmax(-1).item()\n",
      "print(f\"prediction: {predicted_class}\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(res[\"output\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
