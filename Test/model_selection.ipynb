{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d3e60a",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "535f3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY='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'\n",
    "FIRECRAWL_API_KEY='fc-e36282d62d1c4ee8971ec0649f3862c7'\n",
    "BASE_URL='https://api.groq.com/openai/v1'\n",
    "MODEL=\"meta-llama/llama-4-scout-17b-16e-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42744f82",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "91bed7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/mnt/g/Ai agents course/LangGraph/venv/lib/python3.12/site-packages', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "print(sys.path)\n",
    "from tools.fetching_description_from_huggingface import fetching_description_from_huggingface\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import Tool\n",
    "from firecrawl import FirecrawlApp\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36830718",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "5827b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task = pd.read_csv(\"/mnt/g/Ai agents course/LangGraph/Challenge2/data/ISE - AutoCode Challenge 2 Public - Public task.csv\")\n",
    "df_model = pd.read_csv(\"/mnt/g/Ai agents course/LangGraph/Challenge2/data/ISE - AutoCode Challenge 2 Public - Model zoo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "22abc01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STT</th>\n",
       "      <th>Task</th>\n",
       "      <th>Validation data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt tay ...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nTrong th·ªùi ƒë·∫°i th√¥ng tin...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nƒê√¢y l√† m·ªôt nhi·ªám v·ª• h·ªìi ...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nƒêi·ªÅu quan tr·ªçng l√† c√°c c...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nTrong nhi·ªÅu ·ª©ng d·ª•ng th·ª±...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STT                                               Task Validation data\n",
       "0    1  \"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt tay ...            Link\n",
       "1    2  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nTrong th·ªùi ƒë·∫°i th√¥ng tin...            Link\n",
       "2    3  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nƒê√¢y l√† m·ªôt nhi·ªám v·ª• h·ªìi ...            Link\n",
       "3    4  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nƒêi·ªÅu quan tr·ªçng l√† c√°c c...            Link\n",
       "4    5  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nTrong nhi·ªÅu ·ª©ng d·ª•ng th·ª±...            Link"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "697add1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = df_task[\"Task\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "953cae02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nTrong nhi·ªÅu ·ª©ng d·ª•ng th·ª±c t·∫ø nh∆∞ t·ªïng h·ª£p n·ªôi dung h√¨nh ·∫£nh, ph√¢n lo·∫°i album, hay tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng n·ªôi dung cho ·∫£nh t·ªï h·ª£p, c·∫ßn x√°c ƒë·ªãnh ch·ªß ƒë·ªÅ ch√≠nh c·ªßa m·ªôt ·∫£nh gh√©p t·ª´ nhi·ªÅu ·∫£nh con. B√†i to√°n n√†y ƒë√≤i h·ªèi ·ª©ng d·ª•ng kh√¥ng ch·ªâ nh·∫≠n di·ªán ƒë∆∞·ª£c n·ªôi dung c·ªßa t·ª´ng ·∫£nh con, m√† c√≤n c·∫ßn x√°c ƒë·ªãnh ch·ªß ƒë·ªÅ chi·∫øm ∆∞u th·∫ø trong to√†n b·ªô ·∫£nh t·ªïng th·ªÉ.\\n\\nY√™u c·∫ßu c·ª• th·ªÉ c·∫ßn ƒë·∫°t ƒë∆∞·ª£c:\\nM·ªói ƒë·∫ßu v√†o l√† m·ªôt ·∫£nh t·ªï h·ª£p, gh√©p t·ª´ 9 ·∫£nh m√†u nh·ªè (3x3).\\nNhi·ªám v·ª• c·ªßa ·ª©ng d·ª•ng l√†:\\nX√°c ƒë·ªãnh ch·ªß ƒë·ªÅ chi·∫øm s·ªë l∆∞·ª£ng nhi·ªÅu nh·∫•t trong 9 ·∫£nh con.\\nTr∆∞·ªùng h·ª£p c√≥ nhi·ªÅu ch·ªß ƒë·ªÅ xu·∫•t hi·ªán v·ªõi t·∫ßn su·∫•t b·∫±ng nhau, c√≥ th·ªÉ ch·ªçn m·ªôt ch·ªß ƒë·ªÅ b·∫•t k·ª≥ trong s·ªë ƒë√≥ l√†m k·∫øt qu·∫£.\\n\\nƒê·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫ßu v√†o cho b√†i to√°n t·ªïng th·ªÉ:\\nM·ªôt th∆∞ m·ª•c c√≥ t√™n l√† \"images\" ch·ª©a c√°c ·∫£nh m√†u RGB (288x288), ƒë∆∞·ª£c gh√©p t·ª´ 9 ·∫£nh RGB con (32x32) (3 h√†ng √ó 3 c·ªôt).\\n\\nƒê·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra mong mu·ªën cho b√†i to√°n t·ªïng th·ªÉ:\\nFile output.csv m·ªói h√†ng l√† k·∫øt qu·∫£ d·ª± ƒëo√°n m·ªói ·∫£nh\\nc√≥ c√°c c·ªôt:\\nfile_name: t√™n file ·∫£nh\\nprediction: nh√£n c·ªßa ·∫£nh. \\nnh√£n c·ªßa ·∫£nh l√† m·ªôt nh√£n duy nh·∫•t ƒë·∫°i di·ªán cho ch·ªß ƒë·ªÅ xu·∫•t hi·ªán nhi·ªÅu nh·∫•t trong 9 ·∫£nh con.\\nC√°c ch·ªß ƒë·ªÅ sau n·∫±m trong danh s√°ch sau:\\n       [\\'forest\\', \\'orchid\\', \\'spider\\', \\'fox\\', \\'caterpillar\\', \\'crocodile\\',\\n       \\'whale\\', \\'road\\', \\'motorcycle\\', \\'tiger\\', \\'woman\\', \\'orange\\',\\n       \\'elephant\\', \\'otter\\', \\'flatfish\\', \\'cup\\', \\'sea\\', \\'mountain\\', \\'cloud\\',\\n       \\'willow_tree\\', \\'rabbit\\', \\'couch\\', \\'maple_tree\\', \\'wardrobe\\',\\n       \\'mushroom\\', \\'plain\\', \\'telephone\\', \\'shrew\\', \\'bowl\\', \\'tractor\\',\\n       \\'camel\\', \\'bus\\', \\'cattle\\', \\'house\\', \\'cockroach\\', \\'lawn_mower\\',\\n       \\'oak_tree\\', \\'apple\\', \\'bicycle\\', \\'beetle\\', \\'palm_tree\\', \\'sunflower\\',\\n       \\'poppy\\', \\'keyboard\\', \\'porcupine\\', \\'snake\\', \\'can\\', \\'pear\\', \\'shark\\',\\n       \\'chimpanzee\\', \\'hamster\\', \\'train\\', \\'cra\\', \\'bear\\', \\'sweet_pepper\\',\\n       \\'pine_tree\\', \\'bed\\', \\'tank\\', \\'beaver\\', \\'bridge\\', \\'girl\\',\\n       \\'skyscraper\\', \\'wolf\\', \\'raccoon\\', \\'rose\\', \\'worm\\', \\'table\\',\\n       \\'pickup_truck\\', \\'turtle\\', \\'lizard\\', \\'leopard\\', \\'streetcar\\',\\n       \\'rocket\\', \\'man\\', \\'tulip\\', \\'lamp\\', \\'seal\\', \\'lion\\', \\'butterfly\\',\\n       \\'kangaroo\\', \\'television\\', \\'ray\\', \\'chair\\', \\'skunk\\', \\'trout\\',\\n       \\'possum\\', \\'mouse\\']'"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "b6909b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_list(df):\n",
    "    model_strings = []\n",
    "    for i, row in df.iterrows():\n",
    "        desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        url = str(row[2])\n",
    "        model_strings.append(f\"{i + 1} - {desc} [More info]({url})\")\n",
    "    return \"\\n\".join(model_strings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c40886",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "af3bcff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraper(url: str) -> str:\n",
    "    \"\"\"Get more details about the model using the model url\"\"\"\n",
    "    # scraper = FirecrawlApp(api_key=FIRECRAWL_API_KEY)\n",
    "    # scrape_status = scraper.scrape_url(\n",
    "    #     url,\n",
    "    #     formats=['markdown']\n",
    "    # )\n",
    "    # return scrape_status.markdown\n",
    "    result = fetching_description_from_huggingface(url)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "77f3a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraperParameters(BaseModel):\n",
    "    url: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "b6a0e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = web_scraper('https://huggingface.co/thanhtlx/image_classification_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "52a9c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"web_scrapper\",\n",
    "        func=web_scraper,\n",
    "        description=\"Use to get more detail about the model\",\n",
    "        args_schema=WebScraperParameters\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c409f8b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "eca82430",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI( \n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "6511a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tasks(BaseModel):\n",
    "    subtask_one: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 1\"\n",
    "    )\n",
    "    subtask_two: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 2\"\n",
    "    )\n",
    "    subtask_three: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 3\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "0de6f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: HumanMessage | None\n",
    "    data: str | None\n",
    "    messages: Annotated[BaseMessage, add_messages]\n",
    "    subtasks: Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "ef593964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_agent(state: AgentState) -> AgentState:\n",
    "    structured_llm = llm.with_structured_output(Tasks)\n",
    "    system_prompt = \"\"\"\n",
    "                    You are an expert in task formulation. Given a machine learning task, you will\n",
    "                    help divide the task into these subtasks:\n",
    "                    - Subtask1: for classifying the problem type and task description\n",
    "                    - Subtask2: for identifying the input of the problem\n",
    "                    - Subtask3: for identifying the output of the problem\n",
    "                    describe the each subtask as specifically as possible\n",
    "                    \"\"\"\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    messages.append(state['task'])\n",
    "    result = structured_llm.invoke(messages)\n",
    "    state['subtasks'] = result\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "14b15c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"task\": HumanMessage(content=task),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "78675acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = classify_agent(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab3d7f",
   "metadata": {},
   "source": [
    "# Modeling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "f656ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInfo(BaseModel):\n",
    "    model: str = Field(..., description=\"Model name\")\n",
    "    url: str = Field(..., description=\"URL to the model\")\n",
    "\n",
    "class ModelSelection(BaseModel):\n",
    "    models: List[ModelInfo] = Field(..., description=\"List of available models with name and URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "18dcc46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_parser = PydanticOutputParser(pydantic_object=ModelSelection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "de4ed792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the suitable model for a given task.\n",
    "                    Given:\n",
    "                    - A user description about the task\n",
    "                    - A list of available model (names and links)\n",
    "                    Your job is to choose ALL suitable models provided in the list for user's specific tasks.\n",
    "                    Return the answer in the format\n",
    "                    {structured_output}\n",
    "                    Here are the list of model:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]).partial(structured_output=model_selection_parser.get_format_instructions())\n",
    "\n",
    "    chain = prompt | llm | model_selection_parser\n",
    "\n",
    "    result = chain.invoke({\"input\": task, \"model_list\": format_model_list(df_model)})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "fd097a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27136/7332665.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
      "/tmp/ipykernel_27136/7332665.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  url = str(row[2])\n"
     ]
    }
   ],
   "source": [
    "res = model_selection_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "03a8819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelInfo(model='Ph√¢n lo·∫°i h√¨nh ·∫£nh', url='https://huggingface.co/zhaospei/Model_7'),\n",
      " ModelInfo(model='Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh 100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)', url='https://huggingface.co/zhaospei/Model_6'),\n",
      " ModelInfo(model='Nh·∫≠n di·ªán ·∫£nh ch·ªØ vi·∫øt tay', url='https://huggingface.co/thanhtlx/image_classification_01')]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "id": "3b0dc638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_using_tool_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the **Best-fit model** for a given task.\n",
    "                    Given:\n",
    "                    - A user task description\n",
    "                    - A list of available models (names and url)\n",
    "                    - A tool that fetches model details from a provided URL (it help gathers model description, input/output format, code sample usage)\n",
    "\n",
    "                    Your job is to:\n",
    "                    1. Use the **provided tool** to retrieve real details about each candidate model:\n",
    "                        - ‚úÖ Model description\n",
    "                        - üì• Input format\n",
    "                        - üì§ Output format\n",
    "                        - üõ†Ô∏è Library requirements\n",
    "                        - üß™ Code sample (usage code snippet)\n",
    "                    2. Select the best model for user given task based on the data that the tool gives you.\n",
    "\n",
    "                    ‚ö†Ô∏è VERY IMPORTANT RULES\n",
    "\n",
    "                    - ‚ùå DO NOT assume or invent any part of the model's description, input/output format, or code\n",
    "                    - ‚ùå DO NOT generate fake code or use your own knowledge about the model\n",
    "                    - ‚úÖ ONLY use the **actual output** returned from the tool\n",
    "                    - ‚úÖ Include tool content in your final answer exactly as returned (especially code)\n",
    "\n",
    "                    ## üß† Output Structure (Final Answer)\n",
    "\n",
    "                    After retrieving tool results, choose ONE best-fit model and output the result in the following format:\n",
    "\n",
    "                    **‚úÖ Model name and link**  \n",
    "                    `<model name>` ‚Äî `<link>`\n",
    "\n",
    "                    **üìù Description (from tool):**  \n",
    "                    <model description>\n",
    "\n",
    "                    **üì• Input format (from tool):**  \n",
    "                    <description of expected input>\n",
    "\n",
    "                    **üì§ Output format (from tool):**  \n",
    "                    <description of model output>\n",
    "\n",
    "                    **üõ†Ô∏è Library Requirements (from tool) **\n",
    "                    <requirements to use the model>\n",
    "\n",
    "                    **üß™ Example code (from tool):**  \n",
    "                    ```python\n",
    "                    <exact code snippet from tool>\n",
    "\n",
    "                    Here are the list of models and there corresponding URL:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"ai\", \"{agent_scratchpad}\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "    task_input = state[\"task\"].content if isinstance(state[\"task\"], SystemMessage) else state[\"task\"]\n",
    "\n",
    "    # result = executor.invoke({\"input\": task_input})\n",
    "    result = executor.invoke({\"input\": task_input, \"model_list\": res.models})\n",
    "\n",
    "    return {\"output\": result[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "id": "e8b0f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_7'}`\n",
      "responded: I understand the task. You want me to select the best-fit model for a given task based on the data retrieved from the provided tool. The task is to determine the dominant theme in a collage image composed of 9 smaller images.\n",
      "\n",
      "I will start by retrieving the details of each candidate model using the provided tool.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "üñºÔ∏è M√¥ h√¨nh Ph√¢n lo·∫°i ·∫¢nh - ViT Fine-tuned tr√™n CIFAR-10\n",
      "üìù M√¥ t·∫£\n",
      "ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh kh√°c tr√™n t·∫≠p d·ªØ li·ªáu CIFAR-10. M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o 10 l·ªõp kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.\n",
      "üìå Nhi·ªám v·ª•\n",
      "Lo·∫°i b√†i to√°n: Ph√¢n lo·∫°i ·∫£nh (Image Classification)\n",
      "S·ªë l·ªõp: 10 (T∆∞∆°ng ·ª©ng nh√£n) CIFAR-10\n",
      "üì• ƒê·∫ßu v√†o\n",
      "ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB\n",
      "K√≠ch th∆∞·ªõc ·∫£nh: 224x224 pixels\n",
      "üì§ ƒê·∫ßu ra\n",
      "ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits)\n",
      "Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch th∆∞·ªõc [batch_size, 10]\n",
      "√ù nghƒ©a: X√°c su·∫•t d·ª± ƒëo√°n cho t·ª´ng l·ªõp trong 10 l·ªõp c·ªßa CIFAR-10\n",
      "üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt b·∫±ng:\n",
      "pip install transformers torch torchvision\n",
      "üß™ C√°ch s·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "D∆∞·ªõi ƒë√¢y l√† v√≠ d·ª• v·ªÅ c√°ch s·ª≠ d·ª•ng m√¥ h√¨nh ƒë·ªÉ ph√¢n lo·∫°i m·ªôt ·∫£nh:\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "# T·∫£i ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "# T·∫£i processor v√† m√¥ h√¨nh t·ª´ Hugging Face\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_7\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_7\")\n",
      "# X·ª≠ l√Ω ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_label = logits.argmax(-1).item()\n",
      "print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_6'}`\n",
      "responded: I understand the task. You want me to select the best-fit model for a given task based on the data retrieved from the provided tool. The task is to determine the dominant theme in a collage image composed of 9 smaller images.\n",
      "\n",
      "I will start by retrieving the details of each candidate model using the provided tool.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "üñºÔ∏è M√¥ h√¨nh Ph√¢n lo·∫°i ·∫¢nh - ViT Fine-tuned tr√™n CIFAR-100\n",
      "üìù M√¥ t·∫£\n",
      "ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh g·ªëc google/vit-base-patch16-224-in21k tr√™n t·∫≠p d·ªØ li·ªáu CIFAR-100. M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o 100 l·ªõp kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.\n",
      "üìå Nhi·ªám v·ª•\n",
      "Lo·∫°i b√†i to√°n: Ph√¢n lo·∫°i ·∫£nh (Image Classification)\n",
      "S·ªë l·ªõp: 100\n",
      "T·∫≠p d·ªØ li·ªáu: CIFAR-100\n",
      "üì• ƒê·∫ßu v√†o\n",
      "ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB\n",
      "K√≠ch th∆∞·ªõc ·∫£nh:  224x224\n",
      "üì§ ƒê·∫ßu ra\n",
      "ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits)\n",
      "Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch th∆∞·ªõc [batch_size, 100]\n",
      "√ù nghƒ©a: X√°c su·∫•t d·ª± ƒëo√°n cho t·ª´ng l·ªõp trong 100 l·ªõp c·ªßa CIFAR-100\n",
      "üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt b·∫±ng:\n",
      "pip install transformers torch torchvision\n",
      "üß™ C√°ch s·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "D∆∞·ªõi ƒë√¢y l√† v√≠ d·ª• v·ªÅ c√°ch s·ª≠ d·ª•ng m√¥ h√¨nh ƒë·ªÉ ph√¢n lo·∫°i m·ªôt ·∫£nh:\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "# T·∫£i ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "# T·∫£i processor v√† m√¥ h√¨nh t·ª´ Hugging Face\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\n",
      "# X·ª≠ l√Ω ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    predicted_label = logits.argmax(-1).item()\n",
      "print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/thanhtlx/image_classification_01'}`\n",
      "responded: I understand the task. You want me to select the best-fit model for a given task based on the data retrieved from the provided tool. The task is to determine the dominant theme in a collage image composed of 9 smaller images.\n",
      "\n",
      "I will start by retrieving the details of each candidate model using the provided tool.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "Image Classification Model - Handwritten Digit Recognition (Color Images)\n",
      "üìù M√¥ t·∫£\n",
      "ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c ch·ªØ s·ªë vi·∫øt tay (t·ª´ 0 ƒë·∫øn 9) trong ·∫£nh m√†u. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c Vision Transformer (ViT) v√† ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay ƒë∆∞·ª£c ch·ª•p ho·∫∑c scan d∆∞·ªõi d·∫°ng ·∫£nh m√†u.\n",
      "üìå Chi ti·∫øt c√°c m√¥ h√¨nh\n",
      "Ph√¢n lo·∫°i ·∫£nh ch·ªØ s·ªë vi·∫øt tay t·ª´ 0 ƒë·∫øn 9\n",
      "üì• ƒê·∫ßu v√†o\n",
      "ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u (RGB)\n",
      "Ki·ªÉu d·ªØ li·ªáu: PIL.Image.Image\n",
      "K√≠ch th∆∞·ªõc ·∫£nh: M√¥ h√¨nh s·∫Ω t·ª± resize theo c·∫•u h√¨nh c·ªßa ViTImageProcessor (th∆∞·ªùng l√† 224x224)\n",
      "Gi√° tr·ªã pixel: [0, 255] (chu·∫©n h√≥a t·ª± ƒë·ªông b·ªüi processor)\n",
      "üì§ ƒê·∫ßu ra\n",
      "M·ªôt s·ªë nguy√™n t·ª´ 0 ƒë·∫øn 9, l√† k·∫øt qu·∫£ ph√¢n lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi ch·ªØ s·ªë xu·∫•t hi·ªán trong ·∫£nh.\n",
      "üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng pip:\n",
      "pip install transformers torch\n",
      "üß™ S·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n m√£ m·∫´u ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh:\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "import torch\n",
      "model_name = \"thanhtlx/image_classification_01\"\n",
      "model = ViTForImageClassification.from_pretrained(model_name)\n",
      "processor = ViTImageProcessor.from_pretrained(model_name)\n",
      "# ƒê·ªçc ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"images.png\")\n",
      "# X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "# L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
      "predicted_class = outputs.logits.argmax(-1).item()\n",
      "print(f\"prediction: {predicted_class}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mTo solve this task, we need to analyze the details of each model and select the best one that fits the requirements.\n",
      "\n",
      "The task is to determine the dominant theme in a collage image composed of 9 smaller images. The input is a 288x288 image, which is a 3x3 grid of 32x32 images. The output should be a single label representing the most dominant theme among the 9 sub-images.\n",
      "\n",
      "Let's analyze the models:\n",
      "\n",
      "1. **Ph√¢n lo·∫°i h√¨nh ·∫£nh** ‚Äî https://huggingface.co/zhaospei/Model_7\n",
      "   - Description: This is a ViT model fine-tuned on CIFAR-10 dataset for image classification into 10 classes.\n",
      "   - Input format: RGB images of size 224x224.\n",
      "   - Output format: Probabilities for each class.\n",
      "   - Library requirements: transformers, torch, torchvision.\n",
      "   - Example code: Provided.\n",
      "\n",
      "2. **Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)** ‚Äî https://huggingface.co/zhaospei/Model_6\n",
      "   - Description: This is a ViT model fine-tuned on CIFAR-100 dataset for image classification into 100 classes.\n",
      "   - Input format: RGB images of size 224x224.\n",
      "   - Output format: Probabilities for each class.\n",
      "   - Library requirements: transformers, torch, torchvision.\n",
      "   - Example code: Provided.\n",
      "\n",
      "3. **Nh·∫≠n di·ªán ·∫£nh ch·ªØ vi·∫øt tay** ‚Äî https://huggingface.co/thanhtlx/image_classification_01\n",
      "   - Description: This model is for handwritten digit recognition (0-9) in color images.\n",
      "   - Input format: Color images, resized to a suitable size by the processor.\n",
      "   - Output format: A single integer from 0 to 9 representing the recognized digit.\n",
      "   - Library requirements: transformers, torch.\n",
      "   - Example code: Provided.\n",
      "\n",
      "Given the task requirements:\n",
      "\n",
      "- The input is a 288x288 image composed of 9 smaller images.\n",
      "- The task is to identify the dominant theme among these 9 images.\n",
      "\n",
      "**Best-fit Model:** \n",
      "\n",
      "The models provided are primarily for image classification tasks but are fine-tuned on different datasets (CIFAR-10, CIFAR-100, and handwritten digits). The task at hand involves classifying a composite image into one of several themes, not exactly matching any of the model's training tasks.\n",
      "\n",
      "However, **Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)** ‚Äî https://huggingface.co/zhaospei/Model_6 seems more versatile as it deals with 100 classes, offering a broader range of themes that might encompass the variety needed for the task. \n",
      "\n",
      "But, we need to consider if the themes in the task are within the 100 classes of CIFAR-100 or closer to the 10 classes of CIFAR-10.\n",
      "\n",
      "**Assuming the themes can align with CIFAR-100 classes or are close to them:**\n",
      "\n",
      "**‚úÖ Model name and link**  \n",
      " `Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)` ‚Äî `https://huggingface.co/zhaospei/Model_6`\n",
      "\n",
      "**üìù Description (from tool):**  \n",
      " `ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh g·ªëc google/vit-base-patch16-224-in21k tr√™n t·∫≠p d·ªØ li·ªáu CIFAR-100. M√¥ h√¨nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o100 l·ªõp kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.`\n",
      "\n",
      "**üì• Input format (from tool):**  \n",
      " `ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB, K√≠ch th∆∞·ªõc ·∫£nh:224x224`\n",
      "\n",
      "**üì§ Output format (from tool):**  \n",
      " `ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits), Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch th∆∞·ªõc [batch_size,100]`\n",
      "\n",
      "**üõ†Ô∏è Library Requirements (from tool) **  \n",
      " `C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt b·∫±ng: pip install transformers torch torchvision`\n",
      "\n",
      "**üß™ Example code (from tool):**  \n",
      " ```python\n",
      "import torch\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "\n",
      "# T·∫£i ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"path_to_your_image.jpg\")\n",
      "\n",
      "# T·∫£i processor v√† m√¥ h√¨nh t·ª´ Hugging Face\n",
      "processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\n",
      "model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\n",
      "\n",
      "# X·ª≠ l√Ω ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      " outputs = model(**inputs)\n",
      " logits = outputs.logits\n",
      " predicted_label = logits.argmax(-1).item()\n",
      "print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\n",
      "```\n",
      "\n",
      "**However, it's crucial to note that:**\n",
      "\n",
      "- The input image size in the task (288x288 composed of 9 images) is different from what the models are expecting (224x224).\n",
      "- The task requires processing a 3x3 grid of images to determine the dominant theme, which is not directly supported by the provided models' example codes.\n",
      "\n",
      "**To align with the task, consider preprocessing the 288x288 image into 9 segments, classify each, and then determine the dominant theme.** \n",
      "\n",
      "This approach would require adjustments beyond the provided example codes, especially to handle the 3x3 grid of images and to map the classifications back to the task's required output format. \n",
      "\n",
      "Given the constraint and aiming for simplicity, **Model_6** seems like a reasonable choice if the themes align with CIFAR-100, but a more tailored approach might be necessary to strictly adhere to the task's specifics.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res2 = model_selection_using_tool_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "id": "cb681a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('To solve this task, we need to analyze the details of each model and select '\n",
      " 'the best one that fits the requirements.\\n'\n",
      " '\\n'\n",
      " 'The task is to determine the dominant theme in a collage image composed of 9 '\n",
      " 'smaller images. The input is a 288x288 image, which is a 3x3 grid of 32x32 '\n",
      " 'images. The output should be a single label representing the most dominant '\n",
      " 'theme among the 9 sub-images.\\n'\n",
      " '\\n'\n",
      " \"Let's analyze the models:\\n\"\n",
      " '\\n'\n",
      " '1. **Ph√¢n lo·∫°i h√¨nh ·∫£nh** ‚Äî https://huggingface.co/zhaospei/Model_7\\n'\n",
      " '   - Description: This is a ViT model fine-tuned on CIFAR-10 dataset for '\n",
      " 'image classification into 10 classes.\\n'\n",
      " '   - Input format: RGB images of size 224x224.\\n'\n",
      " '   - Output format: Probabilities for each class.\\n'\n",
      " '   - Library requirements: transformers, torch, torchvision.\\n'\n",
      " '   - Example code: Provided.\\n'\n",
      " '\\n'\n",
      " '2. **Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)** ‚Äî '\n",
      " 'https://huggingface.co/zhaospei/Model_6\\n'\n",
      " '   - Description: This is a ViT model fine-tuned on CIFAR-100 dataset for '\n",
      " 'image classification into 100 classes.\\n'\n",
      " '   - Input format: RGB images of size 224x224.\\n'\n",
      " '   - Output format: Probabilities for each class.\\n'\n",
      " '   - Library requirements: transformers, torch, torchvision.\\n'\n",
      " '   - Example code: Provided.\\n'\n",
      " '\\n'\n",
      " '3. **Nh·∫≠n di·ªán ·∫£nh ch·ªØ vi·∫øt tay** ‚Äî '\n",
      " 'https://huggingface.co/thanhtlx/image_classification_01\\n'\n",
      " '   - Description: This model is for handwritten digit recognition (0-9) in '\n",
      " 'color images.\\n'\n",
      " '   - Input format: Color images, resized to a suitable size by the '\n",
      " 'processor.\\n'\n",
      " '   - Output format: A single integer from 0 to 9 representing the recognized '\n",
      " 'digit.\\n'\n",
      " '   - Library requirements: transformers, torch.\\n'\n",
      " '   - Example code: Provided.\\n'\n",
      " '\\n'\n",
      " 'Given the task requirements:\\n'\n",
      " '\\n'\n",
      " '- The input is a 288x288 image composed of 9 smaller images.\\n'\n",
      " '- The task is to identify the dominant theme among these 9 images.\\n'\n",
      " '\\n'\n",
      " '**Best-fit Model:** \\n'\n",
      " '\\n'\n",
      " 'The models provided are primarily for image classification tasks but are '\n",
      " 'fine-tuned on different datasets (CIFAR-10, CIFAR-100, and handwritten '\n",
      " 'digits). The task at hand involves classifying a composite image into one of '\n",
      " \"several themes, not exactly matching any of the model's training tasks.\\n\"\n",
      " '\\n'\n",
      " 'However, **Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt '\n",
      " '(fine-grained)** ‚Äî https://huggingface.co/zhaospei/Model_6 seems more '\n",
      " 'versatile as it deals with 100 classes, offering a broader range of themes '\n",
      " 'that might encompass the variety needed for the task. \\n'\n",
      " '\\n'\n",
      " 'But, we need to consider if the themes in the task are within the 100 '\n",
      " 'classes of CIFAR-100 or closer to the 10 classes of CIFAR-10.\\n'\n",
      " '\\n'\n",
      " '**Assuming the themes can align with CIFAR-100 classes or are close to '\n",
      " 'them:**\\n'\n",
      " '\\n'\n",
      " '**‚úÖ Model name and link**  \\n'\n",
      " ' `Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)` ‚Äî '\n",
      " '`https://huggingface.co/zhaospei/Model_6`\\n'\n",
      " '\\n'\n",
      " '**üìù Description (from tool):**  \\n'\n",
      " ' `ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh g·ªëc '\n",
      " 'google/vit-base-patch16-224-in21k tr√™n t·∫≠p d·ªØ li·ªáu CIFAR-100. M√¥ h√¨nh ƒë∆∞·ª£c '\n",
      " 'hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o100 l·ªõp kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt '\n",
      " 'danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.`\\n'\n",
      " '\\n'\n",
      " '**üì• Input format (from tool):**  \\n'\n",
      " ' `ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB, K√≠ch th∆∞·ªõc ·∫£nh:224x224`\\n'\n",
      " '\\n'\n",
      " '**üì§ Output format (from tool):**  \\n'\n",
      " ' `ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits), Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch '\n",
      " 'th∆∞·ªõc [batch_size,100]`\\n'\n",
      " '\\n'\n",
      " '**üõ†Ô∏è Library Requirements (from tool) **  \\n'\n",
      " ' `C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt b·∫±ng: pip install transformers torch '\n",
      " 'torchvision`\\n'\n",
      " '\\n'\n",
      " '**üß™ Example code (from tool):**  \\n'\n",
      " ' ```python\\n'\n",
      " 'import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " '# T·∫£i ·∫£nh c·∫ßn ph√¢n lo·∫°i\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " '\\n'\n",
      " '# T·∫£i processor v√† m√¥ h√¨nh t·ª´ Hugging Face\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " '\\n'\n",
      " '# X·ª≠ l√Ω ƒë·∫ßu v√†o\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " '\\n'\n",
      " '# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " ' outputs = model(**inputs)\\n'\n",
      " ' logits = outputs.logits\\n'\n",
      " ' predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " \"**However, it's crucial to note that:**\\n\"\n",
      " '\\n'\n",
      " '- The input image size in the task (288x288 composed of 9 images) is '\n",
      " 'different from what the models are expecting (224x224).\\n'\n",
      " '- The task requires processing a 3x3 grid of images to determine the '\n",
      " \"dominant theme, which is not directly supported by the provided models' \"\n",
      " 'example codes.\\n'\n",
      " '\\n'\n",
      " '**To align with the task, consider preprocessing the 288x288 image into 9 '\n",
      " 'segments, classify each, and then determine the dominant theme.** \\n'\n",
      " '\\n'\n",
      " 'This approach would require adjustments beyond the provided example codes, '\n",
      " 'especially to handle the 3x3 grid of images and to map the classifications '\n",
      " \"back to the task's required output format. \\n\"\n",
      " '\\n'\n",
      " 'Given the constraint and aiming for simplicity, **Model_6** seems like a '\n",
      " 'reasonable choice if the themes align with CIFAR-100, but a more tailored '\n",
      " \"approach might be necessary to strictly adhere to the task's specifics.\")\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res2['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874a1c7",
   "metadata": {},
   "source": [
    "## Output Parser Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "3658cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelingOutput(BaseModel):\n",
    "    model_description: str = Field(\n",
    "        ...,\n",
    "        description=\"The detailed description of the model.\"\n",
    "    )\n",
    "    model_input_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed input format\"\n",
    "    )\n",
    "    model_output_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed output format\"\n",
    "    )\n",
    "    model_sample_code: str = Field(\n",
    "        ...,\n",
    "        description=\"Example code to use the model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "0b610e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=ModelingOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "707428ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser_llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "5a1f51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_parser_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a smart AI tasked with extracting structured technical details about a machine learning model from a reasoning result.\n",
    "                    You are given a detailed text description about a model.\n",
    "                    Your goal is to fill the following fields based on the text:\n",
    "\n",
    "                    - `model_description`: A detailed explanation of what the model is and what it does.\n",
    "                    - `model_input_format`: A detailed description of the model's input format, including dimensions, data types, and expected preprocessing if mentioned.\n",
    "                    - `model_output_format`: A detailed description of the output format including dimensions, data types, label name and its meaning.\n",
    "                    - `model_sample_code`: A sample code on how to use the model.\n",
    "\n",
    "                    You MUSTN'T create any data on your own, only using the data provided in the text.\n",
    "\n",
    "                    Return the data as a JSON object matching the following structure:\n",
    "                    {formatted_output}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        ('system', system_prompt),\n",
    "        ('human', \"{input}\")\n",
    "    ]).partial(formatted_output=parser.get_format_instructions())\n",
    "    chain = prompt | llm | parser\n",
    "    response = chain.invoke({\"input\": res2['output']})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "0d82f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = output_parser_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "583a7bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('To solve this task, we need to analyze the details of each model and select '\n",
      " 'the best one that fits the requirements.\\n'\n",
      " '\\n'\n",
      " 'The task is to determine the dominant theme in a collage image composed of 9 '\n",
      " 'smaller images. The input is a 288x288 image, which is a 3x3 grid of 32x32 '\n",
      " 'images. The output should be a single label representing the most dominant '\n",
      " 'theme among the 9 sub-images.\\n'\n",
      " '\\n'\n",
      " \"Let's analyze the models:\\n\"\n",
      " '\\n'\n",
      " '1. **Ph√¢n lo·∫°i h√¨nh ·∫£nh** ‚Äî https://huggingface.co/zhaospei/Model_7\\n'\n",
      " '   - Description: This is a ViT model fine-tuned on CIFAR-10 dataset for '\n",
      " 'image classification into 10 classes.\\n'\n",
      " '   - Input format: RGB images of size 224x224.\\n'\n",
      " '   - Output format: Probabilities for each class.\\n'\n",
      " '   - Library requirements: transformers, torch, torchvision.\\n'\n",
      " '   - Example code: Provided.\\n'\n",
      " '\\n'\n",
      " '2. **Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)** ‚Äî '\n",
      " 'https://huggingface.co/zhaospei/Model_6\\n'\n",
      " '   - Description: This is a ViT model fine-tuned on CIFAR-100 dataset for '\n",
      " 'image classification into 100 classes.\\n'\n",
      " '   - Input format: RGB images of size 224x224.\\n'\n",
      " '   - Output format: Probabilities for each class.\\n'\n",
      " '   - Library requirements: transformers, torch, torchvision.\\n'\n",
      " '   - Example code: Provided.\\n'\n",
      " '\\n'\n",
      " '3. **Nh·∫≠n di·ªán ·∫£nh ch·ªØ vi·∫øt tay** ‚Äî '\n",
      " 'https://huggingface.co/thanhtlx/image_classification_01\\n'\n",
      " '   - Description: This model is for handwritten digit recognition (0-9) in '\n",
      " 'color images.\\n'\n",
      " '   - Input format: Color images, resized to a suitable size by the '\n",
      " 'processor.\\n'\n",
      " '   - Output format: A single integer from 0 to 9 representing the recognized '\n",
      " 'digit.\\n'\n",
      " '   - Library requirements: transformers, torch.\\n'\n",
      " '   - Example code: Provided.\\n'\n",
      " '\\n'\n",
      " 'Given the task requirements:\\n'\n",
      " '\\n'\n",
      " '- The input is a 288x288 image composed of 9 smaller images.\\n'\n",
      " '- The task is to identify the dominant theme among these 9 images.\\n'\n",
      " '\\n'\n",
      " '**Best-fit Model:** \\n'\n",
      " '\\n'\n",
      " 'The models provided are primarily for image classification tasks but are '\n",
      " 'fine-tuned on different datasets (CIFAR-10, CIFAR-100, and handwritten '\n",
      " 'digits). The task at hand involves classifying a composite image into one of '\n",
      " \"several themes, not exactly matching any of the model's training tasks.\\n\"\n",
      " '\\n'\n",
      " 'However, **Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt '\n",
      " '(fine-grained)** ‚Äî https://huggingface.co/zhaospei/Model_6 seems more '\n",
      " 'versatile as it deals with 100 classes, offering a broader range of themes '\n",
      " 'that might encompass the variety needed for the task. \\n'\n",
      " '\\n'\n",
      " 'But, we need to consider if the themes in the task are within the 100 '\n",
      " 'classes of CIFAR-100 or closer to the 10 classes of CIFAR-10.\\n'\n",
      " '\\n'\n",
      " '**Assuming the themes can align with CIFAR-100 classes or are close to '\n",
      " 'them:**\\n'\n",
      " '\\n'\n",
      " '**‚úÖ Model name and link**  \\n'\n",
      " ' `Ph√¢n lo·∫°i h√¨nh ·∫£nh th√†nh100 l·ªõp ƒë·ªëi t∆∞·ª£ng chi ti·∫øt (fine-grained)` ‚Äî '\n",
      " '`https://huggingface.co/zhaospei/Model_6`\\n'\n",
      " '\\n'\n",
      " '**üìù Description (from tool):**  \\n'\n",
      " ' `ƒê√¢y l√† m√¥ h√¨nh Vision Transformer (ViT) ƒë∆∞·ª£c fine-tuned t·ª´ m√¥ h√¨nh g·ªëc '\n",
      " 'google/vit-base-patch16-224-in21k tr√™n t·∫≠p d·ªØ li·ªáu CIFAR-100. M√¥ h√¨nh ƒë∆∞·ª£c '\n",
      " 'hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i ·∫£nh v√†o100 l·ªõp kh√°c nhau, m·ªói l·ªõp ƒë·∫°i di·ªán cho m·ªôt '\n",
      " 'danh m·ª•c ƒë·ªëi t∆∞·ª£ng c·ª• th·ªÉ.`\\n'\n",
      " '\\n'\n",
      " '**üì• Input format (from tool):**  \\n'\n",
      " ' `ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u RGB, K√≠ch th∆∞·ªõc ·∫£nh:224x224`\\n'\n",
      " '\\n'\n",
      " '**üì§ Output format (from tool):**  \\n'\n",
      " ' `ƒê·ªãnh d·∫°ng: X√°c su·∫•t cho m·ªói l·ªõp (logits), Ki·ªÉu d·ªØ li·ªáu: Tensor c√≥ k√≠ch '\n",
      " 'th∆∞·ªõc [batch_size,100]`\\n'\n",
      " '\\n'\n",
      " '**üõ†Ô∏è Library Requirements (from tool) **  \\n'\n",
      " ' `C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt b·∫±ng: pip install transformers torch '\n",
      " 'torchvision`\\n'\n",
      " '\\n'\n",
      " '**üß™ Example code (from tool):**  \\n'\n",
      " ' ```python\\n'\n",
      " 'import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " '# T·∫£i ·∫£nh c·∫ßn ph√¢n lo·∫°i\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " '\\n'\n",
      " '# T·∫£i processor v√† m√¥ h√¨nh t·ª´ Hugging Face\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " '\\n'\n",
      " '# X·ª≠ l√Ω ƒë·∫ßu v√†o\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " '\\n'\n",
      " '# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " ' outputs = model(**inputs)\\n'\n",
      " ' logits = outputs.logits\\n'\n",
      " ' predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Nh√£n d·ª± ƒëo√°n: {model.config.id2label[predicted_label]}\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " \"**However, it's crucial to note that:**\\n\"\n",
      " '\\n'\n",
      " '- The input image size in the task (288x288 composed of 9 images) is '\n",
      " 'different from what the models are expecting (224x224).\\n'\n",
      " '- The task requires processing a 3x3 grid of images to determine the '\n",
      " \"dominant theme, which is not directly supported by the provided models' \"\n",
      " 'example codes.\\n'\n",
      " '\\n'\n",
      " '**To align with the task, consider preprocessing the 288x288 image into 9 '\n",
      " 'segments, classify each, and then determine the dominant theme.** \\n'\n",
      " '\\n'\n",
      " 'This approach would require adjustments beyond the provided example codes, '\n",
      " 'especially to handle the 3x3 grid of images and to map the classifications '\n",
      " \"back to the task's required output format. \\n\"\n",
      " '\\n'\n",
      " 'Given the constraint and aiming for simplicity, **Model_6** seems like a '\n",
      " 'reasonable choice if the themes align with CIFAR-100, but a more tailored '\n",
      " \"approach might be necessary to strictly adhere to the task's specifics.\")\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res2['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "530f5c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This is a ViT model fine-tuned on CIFAR-100 dataset for image classification '\n",
      " 'into 100 classes. The model is expected to classify images into one of 100 '\n",
      " 'different themes, which could potentially align with the themes present in a '\n",
      " '3x3 grid of images.')\n",
      "('RGB images of size 224x224. However, the task requires a 288x288 image '\n",
      " 'composed of 9 smaller 32x32 images. The input image would need to be '\n",
      " \"preprocessed to fit the model's expected input size.\")\n",
      "('Probabilities for each class. The output is a tensor of size [batch_size, '\n",
      " '100] representing the logits for each class. The predicted label is '\n",
      " 'determined by taking the argmax of the logits.')\n",
      "('import torch\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " '\\n'\n",
      " '# Load image\\n'\n",
      " 'image = Image.open(\"path_to_your_image.jpg\")\\n'\n",
      " '\\n'\n",
      " '# Load processor and model from Hugging Face\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(\"zhaospei/Model_6\")\\n'\n",
      " '\\n'\n",
      " '# Process input\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " '\\n'\n",
      " '# Predict with model\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '    outputs = model(**inputs)\\n'\n",
      " '    logits = outputs.logits\\n'\n",
      " '    predicted_label = logits.argmax(-1).item()\\n'\n",
      " 'print(f\"Predicted label: {model.config.id2label[predicted_label]}\")')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(ans.model_description)\n",
    "pprint.pprint(ans.model_input_format)\n",
    "pprint.pprint(ans.model_output_format)\n",
    "pprint.pprint(ans.model_sample_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
