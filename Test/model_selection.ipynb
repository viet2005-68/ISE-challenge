{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d3e60a",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "535f3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY='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'\n",
    "FIRECRAWL_API_KEY='fc-e36282d62d1c4ee8971ec0649f3862c7'\n",
    "BASE_URL='https://api.groq.com/openai/v1'\n",
    "MODEL=\"meta-llama/llama-4-scout-17b-16e-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42744f82",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "91bed7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/codespace/.python/current/lib/python312.zip', '/home/codespace/.python/current/lib/python3.12', '/home/codespace/.python/current/lib/python3.12/lib-dynload', '', '/home/codespace/.local/lib/python3.12/site-packages', '/home/codespace/.python/current/lib/python3.12/site-packages', '..', '..']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "print(sys.path)\n",
    "from tools.fetching_description_from_huggingface import fetching_description_from_huggingface\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import Tool\n",
    "from firecrawl import FirecrawlApp\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36830718",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5827b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task = pd.read_csv(\"data/ISE - AutoCode Challenge 2 Public - Public task.csv\")\n",
    "df_model = pd.read_csv(\"data/ISE - AutoCode Challenge 2 Public - Model zoo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "22abc01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STT</th>\n",
       "      <th>Task</th>\n",
       "      <th>Validation data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt ta...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong th·ªùi ƒë·∫°i th√¥ng t...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒê√¢y l√† m·ªôt nhi·ªám v·ª• h·ªì...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒêi·ªÅu quan tr·ªçng l√† c√°c...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong nhi·ªÅu ·ª©ng d·ª•ng t...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>üêæ Story: Alice‚Äôs Magical Pet Photo Box\\r\\nAlic...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STT                                               Task Validation data\n",
       "0    1  \"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt ta...            Link\n",
       "1    2  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong th·ªùi ƒë·∫°i th√¥ng t...            Link\n",
       "2    3  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒê√¢y l√† m·ªôt nhi·ªám v·ª• h·ªì...            Link\n",
       "3    4  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nƒêi·ªÅu quan tr·ªçng l√† c√°c...            Link\n",
       "4    5  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong nhi·ªÅu ·ª©ng d·ª•ng t...            Link\n",
       "5    6  üêæ Story: Alice‚Äôs Magical Pet Photo Box\\r\\nAlic...            Link"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "697add1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = df_task[\"Task\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "953cae02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong th·ªùi ƒë·∫°i th√¥ng tin hi·ªán nay, ng∆∞·ªùi d√πng ph·∫£i ti·∫øp nh·∫≠n m·ªôt l∆∞·ª£ng l·ªõn tin t·ª©c m·ªói ng√†y t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau. Vi·ªác t·ª± ƒë·ªông ph√¢n lo·∫°i c√°c ƒëo·∫°n tin t·ª©c theo ch·ªß ƒë·ªÅ gi√∫p h·ªá th·ªëng qu·∫£n l√Ω n·ªôi dung hi·ªáu qu·∫£ h∆°n, ƒë·ªìng th·ªùi h·ªó tr·ª£ ng∆∞·ªùi d√πng t√¨m ki·∫øm, l·ªçc, v√† ti·∫øp c·∫≠n th√¥ng tin theo lƒ©nh v·ª±c quan t√¢m. ƒê√¢y l√† m·ªôt b√†i to√°n ƒëi·ªÉn h√¨nh trong lƒ©nh v·ª±c x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP), c√≥ th·ªÉ √°p d·ª•ng trong c√°c h·ªá th·ªëng b√°o ch√≠ ƒëi·ªán t·ª≠, n·ªÅn t·∫£ng m·∫°ng x√£ h·ªôi, hay c√¥ng c·ª• t·ªïng h·ª£p tin t·ª©c.\\r\\n\\r\\nY√™u c·∫ßu c·ª• th·ªÉ c·∫ßn ƒë·∫°t ƒë∆∞·ª£c:\\r\\nX√¢y d·ª±ng m·ªôt ·ª©ng d·ª•ng c√≥ kh·∫£ nƒÉng ph√¢n lo·∫°i ƒëo·∫°n vƒÉn b·∫£n ti·∫øng Anh m√¥ t·∫£ n·ªôi dung m·ªôt b·∫£n tin v√†o ƒë√∫ng nh√≥m ch·ªß ƒë·ªÅ t∆∞∆°ng ·ª©ng.\\r\\n·ª®ng d·ª•ng c·∫ßn d·ª± ƒëo√°n ch√≠nh x√°c ch·ªß ƒë·ªÅ ch√≠nh c·ªßa ƒëo·∫°n tin t·ª©c d·ª±a tr√™n n·ªôi dung ng·ªØ nghƒ©a.\\r\\n\\r\\nƒê·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫ßu v√†o cho b√†i to√°n t·ªïng th·ªÉ:\\r\\nM·ªôt ƒëo·∫°n vƒÉn b·∫£n ng·∫Øn b·∫±ng ti·∫øng Anh, c√≥ n·ªôi dung m√¥ t·∫£ m·ªôt b·∫£n tin, \\r\\n\\r\\nƒê·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra mong mu·ªën cho b√†i to√°n t·ªïng th·ªÉ:\\r\\nM·ªôt nh√£n ch·ªß ƒë·ªÅ duy nh·∫•t cho m·ªói ƒëo·∫°n tin t·ª©c ƒë·∫ßu v√†o.\\r\\nC√°c nh√£n thu·ªôc t·∫≠p sau: 'Marketplace', 'Recreation', 'Technology', 'Politics', 'Religion'\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b6909b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_list(df):\n",
    "    model_strings = []\n",
    "    for i, row in df.iterrows():\n",
    "        desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        url = str(row[2])\n",
    "        model_strings.append(f\"{i + 1} - {desc} [More info]({url})\")\n",
    "    return \"\\n\".join(model_strings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c40886",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "af3bcff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraper(url: str) -> str:\n",
    "    \"\"\"Get more details about the model using the model url\"\"\"\n",
    "    # scraper = FirecrawlApp(api_key=FIRECRAWL_API_KEY)\n",
    "    # scrape_status = scraper.scrape_url(\n",
    "    #     url,\n",
    "    #     formats=['markdown']\n",
    "    # )\n",
    "    # return scrape_status.markdown\n",
    "    result = fetching_description_from_huggingface(url)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "77f3a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraperParameters(BaseModel):\n",
    "    url: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b6a0e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = web_scraper('https://huggingface.co/thanhtlx/image_classification_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "52a9c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"web_scrapper\",\n",
    "        func=web_scraper,\n",
    "        description=\"Use to get more detail about the model\",\n",
    "        args_schema=WebScraperParameters\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c409f8b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "eca82430",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI( \n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6511a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tasks(BaseModel):\n",
    "    subtask_one: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 1\"\n",
    "    )\n",
    "    subtask_two: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 2\"\n",
    "    )\n",
    "    subtask_three: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 3\"\n",
    "    )\n",
    "    subtask_four: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 4\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0de6f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: HumanMessage | None\n",
    "    data: str | None\n",
    "    messages: Annotated[BaseMessage, add_messages]\n",
    "    subtasks: Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ef593964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_agent(state: AgentState) -> AgentState:\n",
    "    structured_llm = llm.with_structured_output(Tasks)\n",
    "    system_prompt = \"\"\"\n",
    "        You are an expert in task formulation. Given a machine learning task, you will\n",
    "        help divide the task into these subtasks:\n",
    "        - Subtask1: for identifying task description and classify the problem type.\n",
    "        - Subtask2: for identifying the problem requirements as described in the \"Y√™u c·∫ßu c·ª• th·ªÉ c·∫ßn ƒë·∫°t ƒë∆∞·ª£c\" (or other equivalent) section, including: problem requirements, expected input and expected output.\n",
    "        - Subtask3: for identifying the input format of the problem, as being described in the \"ƒê·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫ßu v√†o cho b√†i to√°n t·ªïng th·ªÉ\" (or other equivalent) section.\n",
    "        - Subtask4: for identifying the output format of the problem with the given classes, as being described in the \"ƒê·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra mong mu·ªën cho b√†i to√°n t·ªïng th·ªÉ\" (or other equivalent) section.\n",
    "\n",
    "        You **MUST** follow these guidelines:\n",
    "        - Do not mistake the output classes with the one described in the problem requirements. The output classes **MUST** contain the classes described in the \"ƒê·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra mong mu·ªën cho b√†i to√°n t·ªïng th·ªÉ\" section.\n",
    "        - Describe the subtasks as specific as possible.\n",
    "        \"\"\"\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    messages.append(state['task'])\n",
    "    result = structured_llm.invoke(messages)\n",
    "    state['subtasks'] = result\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39d4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "14b15c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"task\": HumanMessage(content=task),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "78675acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = classify_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bf9d721c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X√°c ƒë·ªãnh m√¥ t·∫£ b√†i to√°n v√† ph√¢n lo·∫°i lo·∫°i v·∫•n ƒë·ªÅ. B√†i to√°n n√†y thu·ªôc lƒ©nh '\n",
      " 'v·ª±c x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP) v·ªõi m·ª•c ti√™u ph√¢n lo·∫°i vƒÉn b·∫£n ti·∫øng Anh '\n",
      " 'th√†nh c√°c ch·ªß ƒë·ªÅ c·ª• th·ªÉ.')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(state['subtasks'].subtask_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c0e51e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X√°c ƒë·ªãnh y√™u c·∫ßu c·ªßa v·∫•n ƒë·ªÅ, ƒë·∫ßu v√†o v√† ƒë·∫ßu ra. Y√™u c·∫ßu l√† ph√¢n lo·∫°i ƒëo·∫°n '\n",
      " 'vƒÉn b·∫£n ti·∫øng Anh v√†o ƒë√∫ng nh√≥m ch·ªß ƒë·ªÅ t∆∞∆°ng ·ª©ng. ƒê·∫ßu v√†o l√† m·ªôt ƒëo·∫°n vƒÉn '\n",
      " 'b·∫£n ng·∫Øn b·∫±ng ti·∫øng Anh m√¥ t·∫£ n·ªôi dung m·ªôt b·∫£n tin. ƒê·∫ßu ra l√† m·ªôt nh√£n ch·ªß '\n",
      " 'ƒë·ªÅ duy nh·∫•t cho m·ªói ƒëo·∫°n tin t·ª©c ƒë·∫ßu v√†o.')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(state['subtasks'].subtask_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e38609cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X√°c ƒë·ªãnh ƒë·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫ßu v√†o. D·ªØ li·ªáu ƒë·∫ßu v√†o l√† m·ªôt ƒëo·∫°n vƒÉn b·∫£n ng·∫Øn '\n",
      " 'b·∫±ng ti·∫øng Anh m√¥ t·∫£ n·ªôi dung m·ªôt b·∫£n tin.')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(state['subtasks'].subtask_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "9b1ddbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X√°c ƒë·ªãnh ƒë·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra. K·∫øt qu·∫£ ƒë·∫ßu ra l√† m·ªôt nh√£n ch·ªß ƒë·ªÅ duy '\n",
      " \"nh·∫•t cho m·ªói ƒëo·∫°n tin t·ª©c ƒë·∫ßu v√†o, v·ªõi c√°c nh√£n c√≥ th·ªÉ l√† 'Marketplace', \"\n",
      " \"'Recreation', 'Technology', 'Politics', 'Religion'.\")\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(state['subtasks'].subtask_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab3d7f",
   "metadata": {},
   "source": [
    "# Modeling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f656ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInfo(BaseModel):\n",
    "    model: str = Field(..., description=\"Model name\")\n",
    "    url: str = Field(..., description=\"URL to the model\")\n",
    "\n",
    "class ModelSelection(BaseModel):\n",
    "    models: List[ModelInfo] = Field(..., description=\"List of available models with name and URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "18dcc46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_parser = PydanticOutputParser(pydantic_object=ModelSelection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "de4ed792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the suitable model for a given task.\n",
    "                    Given:\n",
    "                    - A user description about the task\n",
    "                    - A list of available model (names and links)\n",
    "                    Your job is to choose ALL suitable models provided in the list for user's specific tasks.\n",
    "                    Return the answer in the format\n",
    "                    {structured_output}\n",
    "                    Here are the list of model:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]).partial(structured_output=model_selection_parser.get_format_instructions())\n",
    "\n",
    "    chain = prompt | llm | model_selection_parser\n",
    "\n",
    "    result = chain.invoke({\"input\": task, \"model_list\": format_model_list(df_model)})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fd097a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51128/7332665.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
      "/tmp/ipykernel_51128/7332665.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  url = str(row[2])\n"
     ]
    }
   ],
   "source": [
    "res = model_selection_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "03a8819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelInfo(model='Ph√¢n lo·∫°i vƒÉn b·∫£n th√†nh c√°c topic', url='https://huggingface.co/thanhtlx/text_classification_1')]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3b0dc638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_using_tool_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the **Best-fit model** for a given task.\n",
    "                    Given:\n",
    "                    - A user task description\n",
    "                    - A list of available models (names and url)\n",
    "                    - A tool that fetches model details from a provided URL (it help gathers model description, input/output format, code sample usage)\n",
    "\n",
    "                    Your job is to:\n",
    "                    1. Use the **provided tool** to retrieve real details about each candidate model:\n",
    "                        - ‚úÖ Model description\n",
    "                        - üì• Input format\n",
    "                        - üì§ Output format\n",
    "                        - üõ†Ô∏è Library requirements\n",
    "                        - üß™ Code sample (usage code snippet)\n",
    "                    2. Select the best model for user given task based on the data that the tool gives you.\n",
    "\n",
    "                    ‚ö†Ô∏è VERY IMPORTANT RULES\n",
    "\n",
    "                    - ‚ùå DO NOT assume or invent any part of the model's description, input/output format, or code\n",
    "                    - ‚ùå DO NOT generate fake code or use your own knowledge about the model\n",
    "                    - ‚úÖ ONLY use the **actual output** returned from the tool\n",
    "                    - ‚úÖ Include tool content in your final answer exactly as returned (especially code)\n",
    "\n",
    "                    ## üß† Output Structure (Final Answer)\n",
    "\n",
    "                    After retrieving tool results, choose ONE best-fit model and output the result in the following format:\n",
    "\n",
    "                    **‚úÖ Model name and link**  \n",
    "                    `<model name>` ‚Äî `<link>`\n",
    "\n",
    "                    **üìù Description (from tool):**  \n",
    "                    <model description>\n",
    "\n",
    "                    **üì• Input format (from tool):**  \n",
    "                    <description of expected input>\n",
    "\n",
    "                    **üì§ Output format (from tool):**  \n",
    "                    <description of model output>\n",
    "\n",
    "                    **üõ†Ô∏è Library Requirements (from tool) **\n",
    "                    <requirements to use the model>\n",
    "\n",
    "                    **üß™ Example code (from tool):**  \n",
    "                    ```python\n",
    "                    <exact code snippet from tool>\n",
    "\n",
    "                    Here are the list of models and there corresponding URL:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"ai\", \"{agent_scratchpad}\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "    task_input = state[\"task\"].content if isinstance(state[\"task\"], SystemMessage) else state[\"task\"]\n",
    "\n",
    "    # result = executor.invoke({\"input\": task_input})\n",
    "    result = executor.invoke({\"input\": task_input, \"model_list\": res.models})\n",
    "\n",
    "    return {\"output\": result[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e8b0f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/thanhtlx/text_classification_1'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "Text Classification Model - 20 Newsgroups Topic Classification\n",
      "üìù M√¥ t·∫£\n",
      "ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i vƒÉn b·∫£n ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i c√°c ƒëo·∫°n vƒÉn b·∫£n ti·∫øng Anh m√¥ t·∫£ n·ªôi dung m·ªôt b·∫£n tin v√†o m·ªôt trong 20 nh√≥m ch·ªß ƒë·ªÅ t∆∞∆°ng ·ª©ng. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c BERT v√† ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n t·∫≠p d·ªØ li·ªáu 20 Newsgroups.\n",
      "üìå Chi ti·∫øt c√°c m√¥ h√¨nh\n",
      "T√™n model\n",
      "Ki·∫øn tr√∫c\n",
      "Ch·ª©c nƒÉng ch√≠nh\n",
      "thanhtlx/text_classification_1\n",
      "BERT\n",
      "Ph√¢n lo·∫°i vƒÉn b·∫£n v√†o 20 ch·ªß ƒë·ªÅ tin t·ª©c kh√°c nhau\n",
      "üì• ƒê·∫ßu v√†o\n",
      "ƒê·ªãnh d·∫°ng: ƒêo·∫°n vƒÉn b·∫£n ti·∫øng Anh\n",
      "Ki·ªÉu d·ªØ li·ªáu: Chu·ªói vƒÉn b·∫£n (str)\n",
      "X·ª≠ l√Ω: VƒÉn b·∫£n s·∫Ω ƒë∆∞·ª£c m√£ h√≥a b·ªüi tokenizer c·ªßa m√¥ h√¨nh (t·ª± ƒë·ªông c·∫Øt ng·∫Øn v√† ƒë·ªám n·∫øu c·∫ßn)\n",
      "üì§ ƒê·∫ßu ra\n",
      "M·ªôt s·ªë nguy√™n t·ª´ 0 ƒë·∫øn 19, t∆∞∆°ng ·ª©ng v·ªõi m·ªôt trong 20 ch·ªß ƒë·ªÅ sau:\n",
      "0: alt.atheism\n",
      "1: comp.graphics\n",
      "2: comp.os.ms-windows.misc\n",
      "3: comp.sys.ibm.pc.hardware\n",
      "4: comp.sys.mac.hardware\n",
      "5: comp.windows.x\n",
      "6: misc.forsale\n",
      "7: rec.autos\n",
      "8: rec.motorcycles\n",
      "9: rec.sport.baseball\n",
      "10: rec.sport.hockey\n",
      "11: sci.crypt\n",
      "12: sci.electronics\n",
      "13: sci.med\n",
      "14: sci.space\n",
      "15: soc.religion.christian\n",
      "16: talk.politics.guns\n",
      "17: talk.politics.mideast\n",
      "18: talk.politics.misc\n",
      "19: talk.religion.misc\n",
      "üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng pip:\n",
      "pip install transformers tensorflow\n",
      "üß™ S·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n m√£ m·∫´u ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh:\n",
      "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
      "import tensorflow as tf\n",
      "# T·∫£i tokenizer v√† m√¥ h√¨nh\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"thanhtlx/text_classification_1\")\n",
      "model = TFAutoModelForSequenceClassification.from_pretrained(\"thanhtlx/text_classification_1\")\n",
      "# VƒÉn b·∫£n c·∫ßn ph√¢n lo·∫°i\n",
      "text = \"NASA is planning a new mission to Mars.\"\n",
      "# M√£ h√≥a vƒÉn b·∫£n\n",
      "inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n",
      "# D·ª± ƒëo√°n\n",
      "outputs = model(**inputs)\n",
      "logits = outputs.logits\n",
      "# Chuy·ªÉn logits th√†nh x√°c su·∫•t\n",
      "probs = tf.nn.softmax(logits, axis=1)\n",
      "# L·∫•y nh√£n c√≥ x√°c su·∫•t cao nh·∫•t\n",
      "predicted_class = tf.argmax(probs, axis=1).numpy()[0]\n",
      "print(f\"Predicted class: {predicted_class}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/thanhtlx/text_classification_1'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "Text Classification Model - 20 Newsgroups Topic Classification\n",
      "üìù M√¥ t·∫£\n",
      "ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i vƒÉn b·∫£n ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i c√°c ƒëo·∫°n vƒÉn b·∫£n ti·∫øng Anh m√¥ t·∫£ n·ªôi dung m·ªôt b·∫£n tin v√†o m·ªôt trong 20 nh√≥m ch·ªß ƒë·ªÅ t∆∞∆°ng ·ª©ng. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c BERT v√† ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n t·∫≠p d·ªØ li·ªáu 20 Newsgroups.\n",
      "üìå Chi ti·∫øt c√°c m√¥ h√¨nh\n",
      "T√™n model\n",
      "Ki·∫øn tr√∫c\n",
      "Ch·ª©c nƒÉng ch√≠nh\n",
      "thanhtlx/text_classification_1\n",
      "BERT\n",
      "Ph√¢n lo·∫°i vƒÉn b·∫£n v√†o 20 ch·ªß ƒë·ªÅ tin t·ª©c kh√°c nhau\n",
      "üì• ƒê·∫ßu v√†o\n",
      "ƒê·ªãnh d·∫°ng: ƒêo·∫°n vƒÉn b·∫£n ti·∫øng Anh\n",
      "Ki·ªÉu d·ªØ li·ªáu: Chu·ªói vƒÉn b·∫£n (str)\n",
      "X·ª≠ l√Ω: VƒÉn b·∫£n s·∫Ω ƒë∆∞·ª£c m√£ h√≥a b·ªüi tokenizer c·ªßa m√¥ h√¨nh (t·ª± ƒë·ªông c·∫Øt ng·∫Øn v√† ƒë·ªám n·∫øu c·∫ßn)\n",
      "üì§ ƒê·∫ßu ra\n",
      "M·ªôt s·ªë nguy√™n t·ª´ 0 ƒë·∫øn 19, t∆∞∆°ng ·ª©ng v·ªõi m·ªôt trong 20 ch·ªß ƒë·ªÅ sau:\n",
      "0: alt.atheism\n",
      "1: comp.graphics\n",
      "2: comp.os.ms-windows.misc\n",
      "3: comp.sys.ibm.pc.hardware\n",
      "4: comp.sys.mac.hardware\n",
      "5: comp.windows.x\n",
      "6: misc.forsale\n",
      "7: rec.autos\n",
      "8: rec.motorcycles\n",
      "9: rec.sport.baseball\n",
      "10: rec.sport.hockey\n",
      "11: sci.crypt\n",
      "12: sci.electronics\n",
      "13: sci.med\n",
      "14: sci.space\n",
      "15: soc.religion.christian\n",
      "16: talk.politics.guns\n",
      "17: talk.politics.mideast\n",
      "18: talk.politics.misc\n",
      "19: talk.religion.misc\n",
      "üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng pip:\n",
      "pip install transformers tensorflow\n",
      "üß™ S·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n m√£ m·∫´u ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh:\n",
      "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
      "import tensorflow as tf\n",
      "# T·∫£i tokenizer v√† m√¥ h√¨nh\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"thanhtlx/text_classification_1\")\n",
      "model = TFAutoModelForSequenceClassification.from_pretrained(\"thanhtlx/text_classification_1\")\n",
      "# VƒÉn b·∫£n c·∫ßn ph√¢n lo·∫°i\n",
      "text = \"NASA is planning a new mission to Mars.\"\n",
      "# M√£ h√≥a vƒÉn b·∫£n\n",
      "inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n",
      "# D·ª± ƒëo√°n\n",
      "outputs = model(**inputs)\n",
      "logits = outputs.logits\n",
      "# Chuy·ªÉn logits th√†nh x√°c su·∫•t\n",
      "probs = tf.nn.softmax(logits, axis=1)\n",
      "# L·∫•y nh√£n c√≥ x√°c su·∫•t cao nh·∫•t\n",
      "predicted_class = tf.argmax(probs, axis=1).numpy()[0]\n",
      "print(f\"Predicted class: {predicted_class}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/thanhtlx/text_classification_1'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "Text Classification Model - 20 Newsgroups Topic Classification\n",
      "üìù M√¥ t·∫£\n",
      "ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i vƒÉn b·∫£n ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i c√°c ƒëo·∫°n vƒÉn b·∫£n ti·∫øng Anh m√¥ t·∫£ n·ªôi dung m·ªôt b·∫£n tin v√†o m·ªôt trong 20 nh√≥m ch·ªß ƒë·ªÅ t∆∞∆°ng ·ª©ng. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c BERT v√† ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n t·∫≠p d·ªØ li·ªáu 20 Newsgroups.\n",
      "üìå Chi ti·∫øt c√°c m√¥ h√¨nh\n",
      "T√™n model\n",
      "Ki·∫øn tr√∫c\n",
      "Ch·ª©c nƒÉng ch√≠nh\n",
      "thanhtlx/text_classification_1\n",
      "BERT\n",
      "Ph√¢n lo·∫°i vƒÉn b·∫£n v√†o 20 ch·ªß ƒë·ªÅ tin t·ª©c kh√°c nhau\n",
      "üì• ƒê·∫ßu v√†o\n",
      "ƒê·ªãnh d·∫°ng: ƒêo·∫°n vƒÉn b·∫£n ti·∫øng Anh\n",
      "Ki·ªÉu d·ªØ li·ªáu: Chu·ªói vƒÉn b·∫£n (str)\n",
      "X·ª≠ l√Ω: VƒÉn b·∫£n s·∫Ω ƒë∆∞·ª£c m√£ h√≥a b·ªüi tokenizer c·ªßa m√¥ h√¨nh (t·ª± ƒë·ªông c·∫Øt ng·∫Øn v√† ƒë·ªám n·∫øu c·∫ßn)\n",
      "üì§ ƒê·∫ßu ra\n",
      "M·ªôt s·ªë nguy√™n t·ª´ 0 ƒë·∫øn 19, t∆∞∆°ng ·ª©ng v·ªõi m·ªôt trong 20 ch·ªß ƒë·ªÅ sau:\n",
      "0: alt.atheism\n",
      "1: comp.graphics\n",
      "2: comp.os.ms-windows.misc\n",
      "3: comp.sys.ibm.pc.hardware\n",
      "4: comp.sys.mac.hardware\n",
      "5: comp.windows.x\n",
      "6: misc.forsale\n",
      "7: rec.autos\n",
      "8: rec.motorcycles\n",
      "9: rec.sport.baseball\n",
      "10: rec.sport.hockey\n",
      "11: sci.crypt\n",
      "12: sci.electronics\n",
      "13: sci.med\n",
      "14: sci.space\n",
      "15: soc.religion.christian\n",
      "16: talk.politics.guns\n",
      "17: talk.politics.mideast\n",
      "18: talk.politics.misc\n",
      "19: talk.religion.misc\n",
      "üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng pip:\n",
      "pip install transformers tensorflow\n",
      "üß™ S·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n m√£ m·∫´u ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh:\n",
      "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
      "import tensorflow as tf\n",
      "# T·∫£i tokenizer v√† m√¥ h√¨nh\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"thanhtlx/text_classification_1\")\n",
      "model = TFAutoModelForSequenceClassification.from_pretrained(\"thanhtlx/text_classification_1\")\n",
      "# VƒÉn b·∫£n c·∫ßn ph√¢n lo·∫°i\n",
      "text = \"NASA is planning a new mission to Mars.\"\n",
      "# M√£ h√≥a vƒÉn b·∫£n\n",
      "inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n",
      "# D·ª± ƒëo√°n\n",
      "outputs = model(**inputs)\n",
      "logits = outputs.logits\n",
      "# Chuy·ªÉn logits th√†nh x√°c su·∫•t\n",
      "probs = tf.nn.softmax(logits, axis=1)\n",
      "# L·∫•y nh√£n c√≥ x√°c su·∫•t cao nh·∫•t\n",
      "predicted_class = tf.argmax(probs, axis=1).numpy()[0]\n",
      "print(f\"Predicted class: {predicted_class}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mBased on the provided task description and the available model, I will select the best-fit model and output the result in the required format.\n",
      "\n",
      "\n",
      "**‚úÖ Model name and link**  \n",
      "`thanhtlx/text_classification_1` ‚Äî `https://huggingface.co/thanhtlx/text_classification_1`\n",
      "\n",
      "**üìù Description (from tool):**  \n",
      "`ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i vƒÉn b·∫£n ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i c√°c ƒëo·∫°n vƒÉn b·∫£n ti·∫øng Anh m√¥ t·∫£ n·ªôi dung m·ªôt b·∫£n tin v√†o m·ªôt trong20 nh√≥m ch·ªß ƒë·ªÅ t∆∞∆°ng ·ª©ng. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c BERT v√† ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n t·∫≠p d·ªØ li·ªáu20 Newsgroups.`\n",
      "\n",
      "**üì• Input format (from tool):**  \n",
      "`ƒê·ªãnh d·∫°ng: ƒêo·∫°n vƒÉn b·∫£n ti·∫øng Anh\n",
      "Ki·ªÉu d·ªØ li·ªáu: Chu·ªói vƒÉn b·∫£n (str)\n",
      "X·ª≠ l√Ω: VƒÉn b·∫£n s·∫Ω ƒë∆∞·ª£c m√£ h√≥a b·ªüi tokenizer c·ªßa m√¥ h√¨nh (t·ª± ƒë·ªông c·∫Øt ng·∫Øn v√† ƒë·ªám n·∫øu c·∫ßn)`\n",
      "\n",
      "**üì§ Output format (from tool):**  \n",
      "`M·ªôt s·ªë nguy√™n t·ª´0 ƒë·∫øn19, t∆∞∆°ng ·ª©ng v·ªõi m·ªôt trong20 ch·ªß ƒë·ªÅ sau:\n",
      "0: alt.atheism\n",
      "1: comp.graphics\n",
      "2: comp.os.ms-windows.misc\n",
      "3: comp.sys.ibm.pc.hardware\n",
      "4: comp.sys.mac.hardware\n",
      "5: comp.windows.x\n",
      "6: misc.forsale\n",
      "7: rec.autos\n",
      "8: rec.motorcycles\n",
      "9: rec.sport.baseball\n",
      "10: rec.sport.hockey\n",
      "11: sci.crypt\n",
      "12: sci.electronics\n",
      "13: sci.med\n",
      "14: sci.space\n",
      "15: soc.religion.christian\n",
      "16: talk.politics.guns\n",
      "17: talk.politics.mideast\n",
      "18: talk.politics.misc\n",
      "19: talk.religion.misc`\n",
      "\n",
      "However, the output format does not exactly match the required output format. The model outputs a number from 0 to 19, but the task requires one of the following labels: 'Marketplace', 'Recreation', 'Technology', 'Politics', 'Religion'. \n",
      "\n",
      "**üõ†Ô∏è Library Requirements (from tool) **  \n",
      "`C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng pip:\n",
      "pip install transformers tensorflow`\n",
      "\n",
      "**üß™ Example code (from tool):**  \n",
      "```python\n",
      "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
      "import tensorflow as tf\n",
      "\n",
      "# T·∫£i tokenizer v√† m√¥ h√¨nh\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"thanhtlx/text_classification_1\")\n",
      "model = TFAutoModelForSequenceClassification.from_pretrained(\"thanhtlx/text_classification_1\")\n",
      "\n",
      "# VƒÉn b·∫£n c·∫ßn ph√¢n lo·∫°i\n",
      "text = \"NASA is planning a new mission to Mars.\"\n",
      "\n",
      "# M√£ h√≥a vƒÉn b·∫£n\n",
      "inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n",
      "\n",
      "# D·ª± ƒëo√°n\n",
      "outputs = model(**inputs)\n",
      "logits = outputs.logits\n",
      "\n",
      "# Chuy·ªÉn logits th√†nh x√°c su·∫•t\n",
      "probs = tf.nn.softmax(logits, axis=1)\n",
      "\n",
      "# L·∫•y nh√£n c√≥ x√°c su·∫•t cao nh·∫•t\n",
      "predicted_class = tf.argmax(probs, axis=1).numpy()[0]\n",
      "print(f\"Predicted class: {predicted_class}\")\n",
      "```\n",
      "\n",
      "To match the required output format, we need to map the predicted class to the corresponding label. However, the provided model does not have a direct mapping to the required labels. We would need to either retrain the model with the desired labels or create a custom mapping.\n",
      "\n",
      "Given the constraints, I will assume that the model's output can be used as is, and we can create a custom mapping to the required labels.\n",
      "\n",
      "Please note that this might not be the optimal solution, and further adjustments might be needed to achieve the best results.\n",
      "\n",
      "Let's assume the following mapping:\n",
      "- Technology: 1, 3, 5, 11, 12, 14\n",
      "- Politics: 16, 17, 18\n",
      "- Recreation: 7, 8, 9, 10\n",
      "- Marketplace: 6, 15\n",
      "- Religion: 0, 19\n",
      "\n",
      "We can create a custom function to map the predicted class to the corresponding label.\n",
      "\n",
      "```python\n",
      "def map_predicted_class(predicted_class):\n",
      "    if predicted_class in [1, 3, 5, 11, 12, 14]:\n",
      "        return 'Technology'\n",
      "    elif predicted_class in [16, 17, 18]:\n",
      "        return 'Politics'\n",
      "    elif predicted_class in [7, 8, 9, 10]:\n",
      "        return 'Recreation'\n",
      "    elif predicted_class in [6, 15]:\n",
      "        return 'Marketplace'\n",
      "    elif predicted_class in [0, 19]:\n",
      "        return 'Religion'\n",
      "    else:\n",
      "        return 'Unknown'\n",
      "\n",
      "predicted_class = tf.argmax(probs, axis=1).numpy()[0]\n",
      "label = map_predicted_class(predicted_class)\n",
      "print(f\"Predicted label: {label}\")\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res2 = model_selection_using_tool_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cb681a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Based on the provided task description and the available model, I will '\n",
      " 'select the best-fit model and output the result in the required format.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '**‚úÖ Model name and link**  \\n'\n",
      " '`thanhtlx/text_classification_1` ‚Äî '\n",
      " '`https://huggingface.co/thanhtlx/text_classification_1`\\n'\n",
      " '\\n'\n",
      " '**üìù Description (from tool):**  \\n'\n",
      " '`ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i vƒÉn b·∫£n ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i c√°c ƒëo·∫°n '\n",
      " 'vƒÉn b·∫£n ti·∫øng Anh m√¥ t·∫£ n·ªôi dung m·ªôt b·∫£n tin v√†o m·ªôt trong20 nh√≥m ch·ªß ƒë·ªÅ '\n",
      " 't∆∞∆°ng ·ª©ng. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c BERT v√† ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n t·∫≠p d·ªØ '\n",
      " 'li·ªáu20 Newsgroups.`\\n'\n",
      " '\\n'\n",
      " '**üì• Input format (from tool):**  \\n'\n",
      " '`ƒê·ªãnh d·∫°ng: ƒêo·∫°n vƒÉn b·∫£n ti·∫øng Anh\\n'\n",
      " 'Ki·ªÉu d·ªØ li·ªáu: Chu·ªói vƒÉn b·∫£n (str)\\n'\n",
      " 'X·ª≠ l√Ω: VƒÉn b·∫£n s·∫Ω ƒë∆∞·ª£c m√£ h√≥a b·ªüi tokenizer c·ªßa m√¥ h√¨nh (t·ª± ƒë·ªông c·∫Øt ng·∫Øn v√† '\n",
      " 'ƒë·ªám n·∫øu c·∫ßn)`\\n'\n",
      " '\\n'\n",
      " '**üì§ Output format (from tool):**  \\n'\n",
      " '`M·ªôt s·ªë nguy√™n t·ª´0 ƒë·∫øn19, t∆∞∆°ng ·ª©ng v·ªõi m·ªôt trong20 ch·ªß ƒë·ªÅ sau:\\n'\n",
      " '0: alt.atheism\\n'\n",
      " '1: comp.graphics\\n'\n",
      " '2: comp.os.ms-windows.misc\\n'\n",
      " '3: comp.sys.ibm.pc.hardware\\n'\n",
      " '4: comp.sys.mac.hardware\\n'\n",
      " '5: comp.windows.x\\n'\n",
      " '6: misc.forsale\\n'\n",
      " '7: rec.autos\\n'\n",
      " '8: rec.motorcycles\\n'\n",
      " '9: rec.sport.baseball\\n'\n",
      " '10: rec.sport.hockey\\n'\n",
      " '11: sci.crypt\\n'\n",
      " '12: sci.electronics\\n'\n",
      " '13: sci.med\\n'\n",
      " '14: sci.space\\n'\n",
      " '15: soc.religion.christian\\n'\n",
      " '16: talk.politics.guns\\n'\n",
      " '17: talk.politics.mideast\\n'\n",
      " '18: talk.politics.misc\\n'\n",
      " '19: talk.religion.misc`\\n'\n",
      " '\\n'\n",
      " 'However, the output format does not exactly match the required output '\n",
      " 'format. The model outputs a number from 0 to 19, but the task requires one '\n",
      " \"of the following labels: 'Marketplace', 'Recreation', 'Technology', \"\n",
      " \"'Politics', 'Religion'. \\n\"\n",
      " '\\n'\n",
      " '**üõ†Ô∏è Library Requirements (from tool) **  \\n'\n",
      " '`C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng pip:\\n'\n",
      " 'pip install transformers tensorflow`\\n'\n",
      " '\\n'\n",
      " '**üß™ Example code (from tool):**  \\n'\n",
      " '```python\\n'\n",
      " 'from transformers import AutoTokenizer, '\n",
      " 'TFAutoModelForSequenceClassification\\n'\n",
      " 'import tensorflow as tf\\n'\n",
      " '\\n'\n",
      " '# T·∫£i tokenizer v√† m√¥ h√¨nh\\n'\n",
      " 'tokenizer = AutoTokenizer.from_pretrained(\"thanhtlx/text_classification_1\")\\n'\n",
      " 'model = '\n",
      " 'TFAutoModelForSequenceClassification.from_pretrained(\"thanhtlx/text_classification_1\")\\n'\n",
      " '\\n'\n",
      " '# VƒÉn b·∫£n c·∫ßn ph√¢n lo·∫°i\\n'\n",
      " 'text = \"NASA is planning a new mission to Mars.\"\\n'\n",
      " '\\n'\n",
      " '# M√£ h√≥a vƒÉn b·∫£n\\n'\n",
      " 'inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, '\n",
      " 'padding=True)\\n'\n",
      " '\\n'\n",
      " '# D·ª± ƒëo√°n\\n'\n",
      " 'outputs = model(**inputs)\\n'\n",
      " 'logits = outputs.logits\\n'\n",
      " '\\n'\n",
      " '# Chuy·ªÉn logits th√†nh x√°c su·∫•t\\n'\n",
      " 'probs = tf.nn.softmax(logits, axis=1)\\n'\n",
      " '\\n'\n",
      " '# L·∫•y nh√£n c√≥ x√°c su·∫•t cao nh·∫•t\\n'\n",
      " 'predicted_class = tf.argmax(probs, axis=1).numpy()[0]\\n'\n",
      " 'print(f\"Predicted class: {predicted_class}\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'To match the required output format, we need to map the predicted class to '\n",
      " 'the corresponding label. However, the provided model does not have a direct '\n",
      " 'mapping to the required labels. We would need to either retrain the model '\n",
      " 'with the desired labels or create a custom mapping.\\n'\n",
      " '\\n'\n",
      " \"Given the constraints, I will assume that the model's output can be used as \"\n",
      " 'is, and we can create a custom mapping to the required labels.\\n'\n",
      " '\\n'\n",
      " 'Please note that this might not be the optimal solution, and further '\n",
      " 'adjustments might be needed to achieve the best results.\\n'\n",
      " '\\n'\n",
      " \"Let's assume the following mapping:\\n\"\n",
      " '- Technology: 1, 3, 5, 11, 12, 14\\n'\n",
      " '- Politics: 16, 17, 18\\n'\n",
      " '- Recreation: 7, 8, 9, 10\\n'\n",
      " '- Marketplace: 6, 15\\n'\n",
      " '- Religion: 0, 19\\n'\n",
      " '\\n'\n",
      " 'We can create a custom function to map the predicted class to the '\n",
      " 'corresponding label.\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'def map_predicted_class(predicted_class):\\n'\n",
      " '    if predicted_class in [1, 3, 5, 11, 12, 14]:\\n'\n",
      " \"        return 'Technology'\\n\"\n",
      " '    elif predicted_class in [16, 17, 18]:\\n'\n",
      " \"        return 'Politics'\\n\"\n",
      " '    elif predicted_class in [7, 8, 9, 10]:\\n'\n",
      " \"        return 'Recreation'\\n\"\n",
      " '    elif predicted_class in [6, 15]:\\n'\n",
      " \"        return 'Marketplace'\\n\"\n",
      " '    elif predicted_class in [0, 19]:\\n'\n",
      " \"        return 'Religion'\\n\"\n",
      " '    else:\\n'\n",
      " \"        return 'Unknown'\\n\"\n",
      " '\\n'\n",
      " 'predicted_class = tf.argmax(probs, axis=1).numpy()[0]\\n'\n",
      " 'label = map_predicted_class(predicted_class)\\n'\n",
      " 'print(f\"Predicted label: {label}\")\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res2['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874a1c7",
   "metadata": {},
   "source": [
    "## Output Parser Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3658cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelingOutput(BaseModel):\n",
    "    model_description: str = Field(\n",
    "        ...,\n",
    "        description=\"The detailed description of the model.\"\n",
    "    )\n",
    "    model_input_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed input format\"\n",
    "    )\n",
    "    model_output_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed output format\"\n",
    "    )\n",
    "    model_requirements: str = Field(\n",
    "        ...,\n",
    "        description=\"Requirement libraries need to install to use the model.\"\n",
    "    )\n",
    "    model_sample_code: str = Field(\n",
    "        ...,\n",
    "        description=\"Example code to use the model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0b610e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=ModelingOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "707428ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser_llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5a1f51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_parser_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a smart AI tasked with extracting structured technical details about a machine learning model from a reasoning result.\n",
    "                    You are given a detailed text description about a model.\n",
    "                    Your goal is to fill the following fields based on the text:\n",
    "\n",
    "                    - `model_description`: A detailed explanation of what the model is and what it does.\n",
    "                    - `model_input_format`: A detailed description of the model's input format, including dimensions, data types, and expected preprocessing if mentioned.\n",
    "                    - `model_output_format`: A detailed description of the output format including dimensions, data types, label name and its meaning.\n",
    "                    - `model_requirements`: A detailed description about the requirements needed to be sastified in order to use the model\n",
    "                    - `model_sample_code`: A sample code on how to use the model.\n",
    "\n",
    "                    You MUSTN'T create any data on your own, only using the data provided in the text.\n",
    "\n",
    "                    Return the data as a JSON object matching the following structure:\n",
    "                    {formatted_output}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        ('system', system_prompt),\n",
    "        ('human', \"{input}\")\n",
    "    ]).partial(formatted_output=parser.get_format_instructions())\n",
    "    chain = prompt | llm | parser\n",
    "    response = chain.invoke({\"input\": res2['output']})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0d82f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = output_parser_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "583a7bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Based on the provided task description and the available model, I will '\n",
      " 'select the best-fit model and output the result in the required format.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '**‚úÖ Model name and link**  \\n'\n",
      " '`thanhtlx/text_classification_1` ‚Äî '\n",
      " '`https://huggingface.co/thanhtlx/text_classification_1`\\n'\n",
      " '\\n'\n",
      " '**üìù Description (from tool):**  \\n'\n",
      " '`ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i vƒÉn b·∫£n ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i c√°c ƒëo·∫°n '\n",
      " 'vƒÉn b·∫£n ti·∫øng Anh m√¥ t·∫£ n·ªôi dung m·ªôt b·∫£n tin v√†o m·ªôt trong20 nh√≥m ch·ªß ƒë·ªÅ '\n",
      " 't∆∞∆°ng ·ª©ng. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c BERT v√† ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n t·∫≠p d·ªØ '\n",
      " 'li·ªáu20 Newsgroups.`\\n'\n",
      " '\\n'\n",
      " '**üì• Input format (from tool):**  \\n'\n",
      " '`ƒê·ªãnh d·∫°ng: ƒêo·∫°n vƒÉn b·∫£n ti·∫øng Anh\\n'\n",
      " 'Ki·ªÉu d·ªØ li·ªáu: Chu·ªói vƒÉn b·∫£n (str)\\n'\n",
      " 'X·ª≠ l√Ω: VƒÉn b·∫£n s·∫Ω ƒë∆∞·ª£c m√£ h√≥a b·ªüi tokenizer c·ªßa m√¥ h√¨nh (t·ª± ƒë·ªông c·∫Øt ng·∫Øn v√† '\n",
      " 'ƒë·ªám n·∫øu c·∫ßn)`\\n'\n",
      " '\\n'\n",
      " '**üì§ Output format (from tool):**  \\n'\n",
      " '`M·ªôt s·ªë nguy√™n t·ª´0 ƒë·∫øn19, t∆∞∆°ng ·ª©ng v·ªõi m·ªôt trong20 ch·ªß ƒë·ªÅ sau:\\n'\n",
      " '0: alt.atheism\\n'\n",
      " '1: comp.graphics\\n'\n",
      " '2: comp.os.ms-windows.misc\\n'\n",
      " '3: comp.sys.ibm.pc.hardware\\n'\n",
      " '4: comp.sys.mac.hardware\\n'\n",
      " '5: comp.windows.x\\n'\n",
      " '6: misc.forsale\\n'\n",
      " '7: rec.autos\\n'\n",
      " '8: rec.motorcycles\\n'\n",
      " '9: rec.sport.baseball\\n'\n",
      " '10: rec.sport.hockey\\n'\n",
      " '11: sci.crypt\\n'\n",
      " '12: sci.electronics\\n'\n",
      " '13: sci.med\\n'\n",
      " '14: sci.space\\n'\n",
      " '15: soc.religion.christian\\n'\n",
      " '16: talk.politics.guns\\n'\n",
      " '17: talk.politics.mideast\\n'\n",
      " '18: talk.politics.misc\\n'\n",
      " '19: talk.religion.misc`\\n'\n",
      " '\\n'\n",
      " 'However, the output format does not exactly match the required output '\n",
      " 'format. The model outputs a number from 0 to 19, but the task requires one '\n",
      " \"of the following labels: 'Marketplace', 'Recreation', 'Technology', \"\n",
      " \"'Politics', 'Religion'. \\n\"\n",
      " '\\n'\n",
      " '**üõ†Ô∏è Library Requirements (from tool) **  \\n'\n",
      " '`C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng pip:\\n'\n",
      " 'pip install transformers tensorflow`\\n'\n",
      " '\\n'\n",
      " '**üß™ Example code (from tool):**  \\n'\n",
      " '```python\\n'\n",
      " 'from transformers import AutoTokenizer, '\n",
      " 'TFAutoModelForSequenceClassification\\n'\n",
      " 'import tensorflow as tf\\n'\n",
      " '\\n'\n",
      " '# T·∫£i tokenizer v√† m√¥ h√¨nh\\n'\n",
      " 'tokenizer = AutoTokenizer.from_pretrained(\"thanhtlx/text_classification_1\")\\n'\n",
      " 'model = '\n",
      " 'TFAutoModelForSequenceClassification.from_pretrained(\"thanhtlx/text_classification_1\")\\n'\n",
      " '\\n'\n",
      " '# VƒÉn b·∫£n c·∫ßn ph√¢n lo·∫°i\\n'\n",
      " 'text = \"NASA is planning a new mission to Mars.\"\\n'\n",
      " '\\n'\n",
      " '# M√£ h√≥a vƒÉn b·∫£n\\n'\n",
      " 'inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, '\n",
      " 'padding=True)\\n'\n",
      " '\\n'\n",
      " '# D·ª± ƒëo√°n\\n'\n",
      " 'outputs = model(**inputs)\\n'\n",
      " 'logits = outputs.logits\\n'\n",
      " '\\n'\n",
      " '# Chuy·ªÉn logits th√†nh x√°c su·∫•t\\n'\n",
      " 'probs = tf.nn.softmax(logits, axis=1)\\n'\n",
      " '\\n'\n",
      " '# L·∫•y nh√£n c√≥ x√°c su·∫•t cao nh·∫•t\\n'\n",
      " 'predicted_class = tf.argmax(probs, axis=1).numpy()[0]\\n'\n",
      " 'print(f\"Predicted class: {predicted_class}\")\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'To match the required output format, we need to map the predicted class to '\n",
      " 'the corresponding label. However, the provided model does not have a direct '\n",
      " 'mapping to the required labels. We would need to either retrain the model '\n",
      " 'with the desired labels or create a custom mapping.\\n'\n",
      " '\\n'\n",
      " \"Given the constraints, I will assume that the model's output can be used as \"\n",
      " 'is, and we can create a custom mapping to the required labels.\\n'\n",
      " '\\n'\n",
      " 'Please note that this might not be the optimal solution, and further '\n",
      " 'adjustments might be needed to achieve the best results.\\n'\n",
      " '\\n'\n",
      " \"Let's assume the following mapping:\\n\"\n",
      " '- Technology: 1, 3, 5, 11, 12, 14\\n'\n",
      " '- Politics: 16, 17, 18\\n'\n",
      " '- Recreation: 7, 8, 9, 10\\n'\n",
      " '- Marketplace: 6, 15\\n'\n",
      " '- Religion: 0, 19\\n'\n",
      " '\\n'\n",
      " 'We can create a custom function to map the predicted class to the '\n",
      " 'corresponding label.\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'def map_predicted_class(predicted_class):\\n'\n",
      " '    if predicted_class in [1, 3, 5, 11, 12, 14]:\\n'\n",
      " \"        return 'Technology'\\n\"\n",
      " '    elif predicted_class in [16, 17, 18]:\\n'\n",
      " \"        return 'Politics'\\n\"\n",
      " '    elif predicted_class in [7, 8, 9, 10]:\\n'\n",
      " \"        return 'Recreation'\\n\"\n",
      " '    elif predicted_class in [6, 15]:\\n'\n",
      " \"        return 'Marketplace'\\n\"\n",
      " '    elif predicted_class in [0, 19]:\\n'\n",
      " \"        return 'Religion'\\n\"\n",
      " '    else:\\n'\n",
      " \"        return 'Unknown'\\n\"\n",
      " '\\n'\n",
      " 'predicted_class = tf.argmax(probs, axis=1).numpy()[0]\\n'\n",
      " 'label = map_predicted_class(predicted_class)\\n'\n",
      " 'print(f\"Predicted label: {label}\")\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res2['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "530f5c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This is a text classification model trained to classify English text into '\n",
      " 'one of 20 corresponding topics. The model uses the BERT architecture and was '\n",
      " 'trained on the 20 Newsgroups dataset.')\n",
      "('The input format is a segment of English text. The data type is a string '\n",
      " \"(str). The text will be encoded by the model's tokenizer (automatically \"\n",
      " 'truncated and padded if necessary).')\n",
      "('The output is a number from 0 to 19, corresponding to one of the 20 topics. '\n",
      " 'However, to match the required output format, the predicted class needs to '\n",
      " \"be mapped to one of the following labels: 'Marketplace', 'Recreation', \"\n",
      " \"'Technology', 'Politics', 'Religion'.\")\n",
      "('The following libraries need to be installed using pip: transformers, '\n",
      " 'tensorflow')\n",
      "('```python\\n'\n",
      " 'from transformers import AutoTokenizer, '\n",
      " 'TFAutoModelForSequenceClassification\\n'\n",
      " 'import tensorflow as tf\\n'\n",
      " '\\n'\n",
      " '# Load tokenizer and model\\n'\n",
      " 'tokenizer = AutoTokenizer.from_pretrained(\"thanhtlx/text_classification_1\")\\n'\n",
      " 'model = '\n",
      " 'TFAutoModelForSequenceClassification.from_pretrained(\"thanhtlx/text_classification_1\")\\n'\n",
      " '\\n'\n",
      " '# Text to classify\\n'\n",
      " 'text = \"NASA is planning a new mission to Mars.\"\\n'\n",
      " '\\n'\n",
      " '# Encode text\\n'\n",
      " 'inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, '\n",
      " 'padding=True)\\n'\n",
      " '\\n'\n",
      " '# Predict\\n'\n",
      " 'outputs = model(**inputs)\\n'\n",
      " 'logits = outputs.logits\\n'\n",
      " '\\n'\n",
      " '# Convert logits to probabilities\\n'\n",
      " 'probs = tf.nn.softmax(logits, axis=1)\\n'\n",
      " '\\n'\n",
      " '# Get the class with the highest probability\\n'\n",
      " 'predicted_class = tf.argmax(probs, axis=1).numpy()[0]\\n'\n",
      " 'print(f\"Predicted class: {predicted_class}\")\\n'\n",
      " '\\n'\n",
      " 'def map_predicted_class(predicted_class):\\n'\n",
      " '    if predicted_class in [1,3,5,11,12,14]:\\n'\n",
      " \"        return 'Technology'\\n\"\n",
      " '    elif predicted_class in [16,17,18]:\\n'\n",
      " \"        return 'Politics'\\n\"\n",
      " '    elif predicted_class in [7,8,9,10]:\\n'\n",
      " \"        return 'Recreation'\\n\"\n",
      " '    elif predicted_class in [6,15]:\\n'\n",
      " \"        return 'Marketplace'\\n\"\n",
      " '    elif predicted_class in [0,19]:\\n'\n",
      " \"        return 'Religion'\\n\"\n",
      " '    else:\\n'\n",
      " \"        return 'Unknown'\\n\"\n",
      " '\\n'\n",
      " 'label = map_predicted_class(predicted_class)\\n'\n",
      " 'print(f\"Predicted label: {label}\")\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(ans.model_description)\n",
    "pprint.pprint(ans.model_input_format)\n",
    "pprint.pprint(ans.model_output_format)\n",
    "pprint.pprint(ans.model_requirements)\n",
    "pprint.pprint(ans.model_sample_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "77a72e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_list(df):\n",
    "    model_strings = []\n",
    "    for i, row in df.iterrows():\n",
    "        desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        url = str(row[2])\n",
    "        model_strings.append(f\"{i + 1} - {desc} [More info]({url})\")\n",
    "    return \"\\n\".join(model_strings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2131d8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e0f6c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv(override=True)\n",
    "llm = ChatOpenAI(\n",
    "    model = 'gpt-4.1-nano-2025-04-14',\n",
    "    api_key= os.getenv('OPENAI_API_KEY'),\n",
    "    temperature=0.68\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3643728a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentCodingState(TypedDict):\n",
    "    problem_description: str | None\n",
    "    problem_input_description: str | None\n",
    "    problem_output_description: str | None\n",
    "    model_description: str | None\n",
    "    model_input_format: str | None\n",
    "    model_output_format: str | None\n",
    "    model_requirements: str | None\n",
    "    model_sample_code: str | None\n",
    "    output_classes: str | None\n",
    "    code: str | None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fbc5157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_prompt = r\"\"\"\n",
    "    You are a specialist in Machine Learning. Your task is to generate a *fully functional with all necessary imports and dependencies* codebase in *Python* that can be executed flawlessly.\n",
    "\n",
    "    You will be provided with:\n",
    "    - A problem description\n",
    "    - An input specification\n",
    "    - An output specification\n",
    "    - A model description\n",
    "\n",
    "    ### Input:\n",
    "    - Problem description: {problem_description}\n",
    "    - Problem input specification: {problem_input_description}\n",
    "    - Problem output specification: {problem_output_description}\n",
    "    - ML model description: {model_description}\n",
    "    - ML model input format: {model_input_format}\n",
    "    - ML model output format: {model_output_format}\n",
    "    - ML model requirements: {model_requirements}\n",
    "    - ML model sample code: {model_sample_code}\n",
    "    - Output classes: {output_classes}\n",
    "\n",
    "    ### Guidelines\n",
    "\n",
    "    You *must* strictly follow the following guidelines:\n",
    "    \n",
    "    - DO NOT generate any new input. ONLY use the given input file from the problem input description provided above\n",
    "    - if NO input file is provided, assume that the input file will be \"test.csv\"\n",
    "    - The preprocessing step should be suitable for the data type.\n",
    "    - The postprocessing step should notices the differences between the data returned by the model and the output requirements. You must extract and use the exact class labels as defined in the output specification.\n",
    "    - Do *not invent new labels or translate* the class names. Use them exactly as given.\n",
    "    - You *must* make sure that your codebase can be executed flawlessly that would not encounter any errors or exceptions.\n",
    "    - You must add some tqdm to see the infer progress.\n",
    "    - Output file name MUST be \"predictions.csv\"\n",
    "    \n",
    "    \n",
    "    Your implementation *must strictly follow* the structure below:\n",
    "    1. *Imports*: All required libraries.\n",
    "    2. *Preprocessing*: Handle and transform the input as defined.\n",
    "    3. *Inference logic*: Use the described model for prediction. You *must* use tqdm or similar logging library to track progress.\n",
    "    4. *Postprocessing*: Format or transform the raw output into the final result as described.\n",
    "    5. *Output*: Export the predicted results into a suitable file type as described above (MUST CSV FILE)\n",
    "\n",
    "    You must *not* include any explanations, markdown, or logging outside what is required by the problem.\n",
    "\n",
    "    Return *only* the complete Python codebase, and you **MUST NOT** include a main function in any way. Wrap it with:\n",
    "    \\`\\`\\`python\n",
    "    # code here\n",
    "    \\`\\`\\`\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ad8e05be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputState(TypedDict):\n",
    "    output_classes: str | None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0095fa63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c4731165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_agent(state: OutputState) -> OutputState:\n",
    "    prompt = r\"\"\"\n",
    "        Your are a specialist in machine learning. Your task is to identify the absolute classes of the given problem description, following with an output description.\n",
    "        ### Input:\n",
    "        - Problem description: {problem_description}\n",
    "        - Output description: {output_description}\n",
    "\n",
    "        You must return an array, strictly following these guidelines:\n",
    "        - Understand the context from the given problem description.\n",
    "        - Extract the class names from the output description. You **must not** invent new labels or translate the class names. Use them exactly as given in the output description.\n",
    "        - Create an array containing the classes.\n",
    "\n",
    "        You must return only the array containing those classes, without any formatting.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = prompt.format(\n",
    "        problem_description = state[\"problem_description\"],\n",
    "        output_description = state[\"output_description\"]\n",
    "    )\n",
    "    response = llm.invoke(prompt)\n",
    "    return {**state, \"output_classes\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3895d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_state = {\n",
    "    \"problem_description\": task,\n",
    "    \"output_description\": state[\"subtasks\"].subtask_four\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "6a52076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state = output_agent(output_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "72a9f7bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['Marketplace', 'Recreation', 'Technology', 'Politics', 'Religion']\""
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state[\"output_classes\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c3313114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coding_agent(state: AgentCodingState) -> AgentCodingState:\n",
    "    prompt = base_prompt.format(\n",
    "        problem_description=state[\"problem_description\"],\n",
    "        problem_input_description=state[\"problem_input_description\"],\n",
    "        problem_output_description=state[\"problem_output_description\"],\n",
    "        model_description=state[\"model_description\"],\n",
    "        model_input_format=state[\"model_input_format\"],\n",
    "        model_output_format=state[\"model_output_format\"],\n",
    "        model_requirements=state[\"model_requirements\"],\n",
    "        model_sample_code=state[\"model_sample_code\"],\n",
    "        output_classes=state[\"output_classes\"],\n",
    "    )\n",
    "    response = llm.invoke(prompt)\n",
    "    return {**state, \"code\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d80aeddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_state = {\n",
    "    \"problem_description\": task,\n",
    "    \"problem_input_description\": state[\"subtasks\"].subtask_three,\n",
    "    \"problem_output_description\": state[\"subtasks\"].subtask_four,\n",
    "    \"model_description\": res2['output'],\n",
    "    \"model_input_format\": ans.model_input_format,\n",
    "    \"model_output_format\": ans.model_output_format,\n",
    "    \"model_requirements\": ans.model_requirements,\n",
    "    \"model_sample_code\": ans.model_sample_code,\n",
    "    \"output_classes\": new_state[\"output_classes\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d6a56d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_state = coding_agent(prev_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4da0f500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input format is a segment of English text. The data type is a string (str). The text will be encoded by the model's tokenizer (automatically truncated and padded if necessary).\n"
     ]
    }
   ],
   "source": [
    "print(coding_state['model_input_format'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "15ad0a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
      "import tensorflow as tf\n",
      "import pandas as pd\n",
      "from tqdm import tqdm\n",
      "\n",
      "# Load tokenizer and model\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"thanhtlx/text_classification_1\")\n",
      "model = TFAutoModelForSequenceClassification.from_pretrained(\"thanhtlx/text_classification_1\")\n",
      "\n",
      "# Define label mapping based on predicted class\n",
      "def map_predicted_class(predicted_class):\n",
      "    if predicted_class in [1, 3, 5, 11, 12, 14]:\n",
      "        return 'Technology'\n",
      "    elif predicted_class in [16, 17, 18]:\n",
      "        return 'Politics'\n",
      "    elif predicted_class in [7, 8, 9, 10]:\n",
      "        return 'Recreation'\n",
      "    elif predicted_class in [6, 15]:\n",
      "        return 'Marketplace'\n",
      "    elif predicted_class in [0, 19]:\n",
      "        return 'Religion'\n",
      "    else:\n",
      "        return 'Unknown'\n",
      "\n",
      "# Read input data from 'test.csv'\n",
      "# Assuming the CSV has a column named 'text' containing the news descriptions\n",
      "input_df = pd.read_csv('test.csv')\n",
      "texts = input_df['text'].astype(str).tolist()\n",
      "\n",
      "predictions = []\n",
      "\n",
      "# Process each text with tqdm progress bar\n",
      "for text in tqdm(texts, desc='Classifying texts'):\n",
      "    # Encode text\n",
      "    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n",
      "    # Predict\n",
      "    outputs = model(**inputs)\n",
      "    logits = outputs.logits\n",
      "    # Convert logits to probabilities\n",
      "    probs = tf.nn.softmax(logits, axis=1)\n",
      "    # Get predicted class\n",
      "    predicted_class = tf.argmax(probs, axis=1).numpy()[0]\n",
      "    # Map to label\n",
      "    label = map_predicted_class(predicted_class)\n",
      "    predictions.append(label)\n",
      "\n",
      "# Save predictions to 'predictions.csv'\n",
      "output_df = pd.DataFrame({'text': texts, 'predicted_label': predictions})\n",
      "output_df.to_csv('predictions.csv', index=False)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(coding_state[\"code\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "888cd97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'problem_description': \"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\r\\nTrong th·ªùi ƒë·∫°i th√¥ng tin hi·ªán nay, ng∆∞·ªùi d√πng ph·∫£i ti·∫øp nh·∫≠n m·ªôt l∆∞·ª£ng l·ªõn tin t·ª©c m·ªói ng√†y t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau. Vi·ªác t·ª± ƒë·ªông ph√¢n lo·∫°i c√°c ƒëo·∫°n tin t·ª©c theo ch·ªß ƒë·ªÅ gi√∫p h·ªá th·ªëng qu·∫£n l√Ω n·ªôi dung hi·ªáu qu·∫£ h∆°n, ƒë·ªìng th·ªùi h·ªó tr·ª£ ng∆∞·ªùi d√πng t√¨m ki·∫øm, l·ªçc, v√† ti·∫øp c·∫≠n th√¥ng tin theo lƒ©nh v·ª±c quan t√¢m. ƒê√¢y l√† m·ªôt b√†i to√°n ƒëi·ªÉn h√¨nh trong lƒ©nh v·ª±c x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n (NLP), c√≥ th·ªÉ √°p d·ª•ng trong c√°c h·ªá th·ªëng b√°o ch√≠ ƒëi·ªán t·ª≠, n·ªÅn t·∫£ng m·∫°ng x√£ h·ªôi, hay c√¥ng c·ª• t·ªïng h·ª£p tin t·ª©c.\\r\\n\\r\\nY√™u c·∫ßu c·ª• th·ªÉ c·∫ßn ƒë·∫°t ƒë∆∞·ª£c:\\r\\nX√¢y d·ª±ng m·ªôt ·ª©ng d·ª•ng c√≥ kh·∫£ nƒÉng ph√¢n lo·∫°i ƒëo·∫°n vƒÉn b·∫£n ti·∫øng Anh m√¥ t·∫£ n·ªôi dung m·ªôt b·∫£n tin v√†o ƒë√∫ng nh√≥m ch·ªß ƒë·ªÅ t∆∞∆°ng ·ª©ng.\\r\\n·ª®ng d·ª•ng c·∫ßn d·ª± ƒëo√°n ch√≠nh x√°c ch·ªß ƒë·ªÅ ch√≠nh c·ªßa ƒëo·∫°n tin t·ª©c d·ª±a tr√™n n·ªôi dung ng·ªØ nghƒ©a.\\r\\n\\r\\nƒê·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫ßu v√†o cho b√†i to√°n t·ªïng th·ªÉ:\\r\\nM·ªôt ƒëo·∫°n vƒÉn b·∫£n ng·∫Øn b·∫±ng ti·∫øng Anh, c√≥ n·ªôi dung m√¥ t·∫£ m·ªôt b·∫£n tin, \\r\\n\\r\\nƒê·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra mong mu·ªën cho b√†i to√°n t·ªïng th·ªÉ:\\r\\nM·ªôt nh√£n ch·ªß ƒë·ªÅ duy nh·∫•t cho m·ªói ƒëo·∫°n tin t·ª©c ƒë·∫ßu v√†o.\\r\\nC√°c nh√£n thu·ªôc t·∫≠p sau: 'Marketplace', 'Recreation', 'Technology', 'Politics', 'Religion'\",\n",
       " 'problem_input_description': 'X√°c ƒë·ªãnh ƒë·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫ßu v√†o. D·ªØ li·ªáu ƒë·∫ßu v√†o l√† m·ªôt ƒëo·∫°n vƒÉn b·∫£n ng·∫Øn b·∫±ng ti·∫øng Anh m√¥ t·∫£ n·ªôi dung m·ªôt b·∫£n tin.',\n",
       " 'problem_output_description': \"X√°c ƒë·ªãnh ƒë·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra. K·∫øt qu·∫£ ƒë·∫ßu ra l√† m·ªôt nh√£n ch·ªß ƒë·ªÅ duy nh·∫•t cho m·ªói ƒëo·∫°n tin t·ª©c ƒë·∫ßu v√†o, v·ªõi c√°c nh√£n c√≥ th·ªÉ l√† 'Marketplace', 'Recreation', 'Technology', 'Politics', 'Religion'.\",\n",
       " 'model_description': 'Based on the provided task description and the available model, I will select the best-fit model and output the result in the required format.\\n\\n\\n**‚úÖ Model name and link**  \\n`thanhtlx/text_classification_1` ‚Äî `https://huggingface.co/thanhtlx/text_classification_1`\\n\\n**üìù Description (from tool):**  \\n`ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i vƒÉn b·∫£n ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ ph√¢n lo·∫°i c√°c ƒëo·∫°n vƒÉn b·∫£n ti·∫øng Anh m√¥ t·∫£ n·ªôi dung m·ªôt b·∫£n tin v√†o m·ªôt trong20 nh√≥m ch·ªß ƒë·ªÅ t∆∞∆°ng ·ª©ng. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c BERT v√† ƒë∆∞·ª£c hu·∫•n luy·ªán tr√™n t·∫≠p d·ªØ li·ªáu20 Newsgroups.`\\n\\n**üì• Input format (from tool):**  \\n`ƒê·ªãnh d·∫°ng: ƒêo·∫°n vƒÉn b·∫£n ti·∫øng Anh\\nKi·ªÉu d·ªØ li·ªáu: Chu·ªói vƒÉn b·∫£n (str)\\nX·ª≠ l√Ω: VƒÉn b·∫£n s·∫Ω ƒë∆∞·ª£c m√£ h√≥a b·ªüi tokenizer c·ªßa m√¥ h√¨nh (t·ª± ƒë·ªông c·∫Øt ng·∫Øn v√† ƒë·ªám n·∫øu c·∫ßn)`\\n\\n**üì§ Output format (from tool):**  \\n`M·ªôt s·ªë nguy√™n t·ª´0 ƒë·∫øn19, t∆∞∆°ng ·ª©ng v·ªõi m·ªôt trong20 ch·ªß ƒë·ªÅ sau:\\n0: alt.atheism\\n1: comp.graphics\\n2: comp.os.ms-windows.misc\\n3: comp.sys.ibm.pc.hardware\\n4: comp.sys.mac.hardware\\n5: comp.windows.x\\n6: misc.forsale\\n7: rec.autos\\n8: rec.motorcycles\\n9: rec.sport.baseball\\n10: rec.sport.hockey\\n11: sci.crypt\\n12: sci.electronics\\n13: sci.med\\n14: sci.space\\n15: soc.religion.christian\\n16: talk.politics.guns\\n17: talk.politics.mideast\\n18: talk.politics.misc\\n19: talk.religion.misc`\\n\\nHowever, the output format does not exactly match the required output format. The model outputs a number from 0 to 19, but the task requires one of the following labels: \\'Marketplace\\', \\'Recreation\\', \\'Technology\\', \\'Politics\\', \\'Religion\\'. \\n\\n**üõ†Ô∏è Library Requirements (from tool) **  \\n`C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng pip:\\npip install transformers tensorflow`\\n\\n**üß™ Example code (from tool):**  \\n```python\\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\\nimport tensorflow as tf\\n\\n# T·∫£i tokenizer v√† m√¥ h√¨nh\\ntokenizer = AutoTokenizer.from_pretrained(\"thanhtlx/text_classification_1\")\\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"thanhtlx/text_classification_1\")\\n\\n# VƒÉn b·∫£n c·∫ßn ph√¢n lo·∫°i\\ntext = \"NASA is planning a new mission to Mars.\"\\n\\n# M√£ h√≥a vƒÉn b·∫£n\\ninputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\\n\\n# D·ª± ƒëo√°n\\noutputs = model(**inputs)\\nlogits = outputs.logits\\n\\n# Chuy·ªÉn logits th√†nh x√°c su·∫•t\\nprobs = tf.nn.softmax(logits, axis=1)\\n\\n# L·∫•y nh√£n c√≥ x√°c su·∫•t cao nh·∫•t\\npredicted_class = tf.argmax(probs, axis=1).numpy()[0]\\nprint(f\"Predicted class: {predicted_class}\")\\n```\\n\\nTo match the required output format, we need to map the predicted class to the corresponding label. However, the provided model does not have a direct mapping to the required labels. We would need to either retrain the model with the desired labels or create a custom mapping.\\n\\nGiven the constraints, I will assume that the model\\'s output can be used as is, and we can create a custom mapping to the required labels.\\n\\nPlease note that this might not be the optimal solution, and further adjustments might be needed to achieve the best results.\\n\\nLet\\'s assume the following mapping:\\n- Technology: 1, 3, 5, 11, 12, 14\\n- Politics: 16, 17, 18\\n- Recreation: 7, 8, 9, 10\\n- Marketplace: 6, 15\\n- Religion: 0, 19\\n\\nWe can create a custom function to map the predicted class to the corresponding label.\\n\\n```python\\ndef map_predicted_class(predicted_class):\\n    if predicted_class in [1, 3, 5, 11, 12, 14]:\\n        return \\'Technology\\'\\n    elif predicted_class in [16, 17, 18]:\\n        return \\'Politics\\'\\n    elif predicted_class in [7, 8, 9, 10]:\\n        return \\'Recreation\\'\\n    elif predicted_class in [6, 15]:\\n        return \\'Marketplace\\'\\n    elif predicted_class in [0, 19]:\\n        return \\'Religion\\'\\n    else:\\n        return \\'Unknown\\'\\n\\npredicted_class = tf.argmax(probs, axis=1).numpy()[0]\\nlabel = map_predicted_class(predicted_class)\\nprint(f\"Predicted label: {label}\")\\n```',\n",
       " 'model_input_format': \"The input format is a segment of English text. The data type is a string (str). The text will be encoded by the model's tokenizer (automatically truncated and padded if necessary).\",\n",
       " 'model_output_format': \"The output is a number from 0 to 19, corresponding to one of the 20 topics. However, to match the required output format, the predicted class needs to be mapped to one of the following labels: 'Marketplace', 'Recreation', 'Technology', 'Politics', 'Religion'.\",\n",
       " 'model_requirements': 'The following libraries need to be installed using pip: transformers, tensorflow',\n",
       " 'model_sample_code': '```python\\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\\nimport tensorflow as tf\\n\\n# Load tokenizer and model\\ntokenizer = AutoTokenizer.from_pretrained(\"thanhtlx/text_classification_1\")\\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"thanhtlx/text_classification_1\")\\n\\n# Text to classify\\ntext = \"NASA is planning a new mission to Mars.\"\\n\\n# Encode text\\ninputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\\n\\n# Predict\\noutputs = model(**inputs)\\nlogits = outputs.logits\\n\\n# Convert logits to probabilities\\nprobs = tf.nn.softmax(logits, axis=1)\\n\\n# Get the class with the highest probability\\npredicted_class = tf.argmax(probs, axis=1).numpy()[0]\\nprint(f\"Predicted class: {predicted_class}\")\\n\\ndef map_predicted_class(predicted_class):\\n    if predicted_class in [1,3,5,11,12,14]:\\n        return \\'Technology\\'\\n    elif predicted_class in [16,17,18]:\\n        return \\'Politics\\'\\n    elif predicted_class in [7,8,9,10]:\\n        return \\'Recreation\\'\\n    elif predicted_class in [6,15]:\\n        return \\'Marketplace\\'\\n    elif predicted_class in [0,19]:\\n        return \\'Religion\\'\\n    else:\\n        return \\'Unknown\\'\\n\\nlabel = map_predicted_class(predicted_class)\\nprint(f\"Predicted label: {label}\")\\n```',\n",
       " 'output_classes': \"['Marketplace', 'Recreation', 'Technology', 'Politics', 'Religion']\",\n",
       " 'code': '```python\\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification\\nimport tensorflow as tf\\nimport pandas as pd\\nfrom tqdm import tqdm\\n\\n# Load tokenizer and model\\ntokenizer = AutoTokenizer.from_pretrained(\"thanhtlx/text_classification_1\")\\nmodel = TFAutoModelForSequenceClassification.from_pretrained(\"thanhtlx/text_classification_1\")\\n\\n# Define label mapping based on predicted class\\ndef map_predicted_class(predicted_class):\\n    if predicted_class in [1, 3, 5, 11, 12, 14]:\\n        return \\'Technology\\'\\n    elif predicted_class in [16, 17, 18]:\\n        return \\'Politics\\'\\n    elif predicted_class in [7, 8, 9, 10]:\\n        return \\'Recreation\\'\\n    elif predicted_class in [6, 15]:\\n        return \\'Marketplace\\'\\n    elif predicted_class in [0, 19]:\\n        return \\'Religion\\'\\n    else:\\n        return \\'Unknown\\'\\n\\n# Read input data from \\'test.csv\\'\\n# Assuming the CSV has a column named \\'text\\' containing the news descriptions\\ninput_df = pd.read_csv(\\'test.csv\\')\\ntexts = input_df[\\'text\\'].astype(str).tolist()\\n\\npredictions = []\\n\\n# Process each text with tqdm progress bar\\nfor text in tqdm(texts, desc=\\'Classifying texts\\'):\\n    # Encode text\\n    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\\n    # Predict\\n    outputs = model(**inputs)\\n    logits = outputs.logits\\n    # Convert logits to probabilities\\n    probs = tf.nn.softmax(logits, axis=1)\\n    # Get predicted class\\n    predicted_class = tf.argmax(probs, axis=1).numpy()[0]\\n    # Map to label\\n    label = map_predicted_class(predicted_class)\\n    predictions.append(label)\\n\\n# Save predictions to \\'predictions.csv\\'\\noutput_df = pd.DataFrame({\\'text\\': texts, \\'predicted_label\\': predictions})\\noutput_df.to_csv(\\'predictions.csv\\', index=False)\\n```'}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coding_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "28346794",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "llm = ChatOpenAI(\n",
    "    model= 'gpt-4.1-nano-2025-04-14',\n",
    "    api_key= os.getenv('OPENAI_API_KEY'),\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "06f1a4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Summary:\n",
      "**Evaluation Summary:**\n",
      "\n",
      "*Strengths:*\n",
      "- The code correctly loads the pre-trained tokenizer and model.\n",
      "- It processes each input text individually, performing tokenization, prediction, and mapping to labels.\n",
      "- The code is clear, with a straightforward loop and progress bar for predictions.\n",
      "- The label mapping function aligns predictions to the desired categories.\n",
      "\n",
      "*Areas for improvement:*\n",
      "- The current implementation processes inputs sequentially, which is inefficient for large datasets. Batch processing is more optimal.\n",
      "- Input shape validation and explicit tensor reshaping are missing, which could lead to runtime shape errors.\n",
      "- Adding shape validation and error handling enhances robustness.\n",
      "- The code should process the entire DataFrame in batches rather than per-row to improve performance, but since the original logic processes line-by-line, I will preserve that approach with shape validation and explicit reshaping.\n",
      "- The mapping from model's predicted class (0-19) to desired labels is correct, but it can be simplified and made more explicit.\n",
      "- No change is needed to the output format or the core logic of generating predictions.\n",
      "\n",
      "---\n",
      "\n",
      "**Improved Code:**\n",
      "\n",
      "Improved Code:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from tqdm import tqdm\n",
      "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
      "import tensorflow as tf\n",
      "\n",
      "# Load tokenizer and model\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"thanhtlx/text_classification_1\")\n",
      "model = TFAutoModelForSequenceClassification.from_pretrained(\"thanhtlx/text_classification_1\")\n",
      "\n",
      "# Define label mapping based on predicted class numbers\n",
      "def map_predicted_class(predicted_class):\n",
      "    if predicted_class in [1, 3, 5, 11, 12, 14]:\n",
      "        return 'Technology'\n",
      "    elif predicted_class in [16, 17, 18]:\n",
      "        return 'Politics'\n",
      "    elif predicted_class in [7, 8, 9, 10]:\n",
      "        return 'Recreation'\n",
      "    elif predicted_class in [6, 15]:\n",
      "        return 'Marketplace'\n",
      "    elif predicted_class in [0, 19]:\n",
      "        return 'Religion'\n",
      "    else:\n",
      "        return 'Unknown'\n",
      "\n",
      "# Read input data\n",
      "# Assuming input file is \"test.csv\" with a column named 'text'\n",
      "input_df = pd.read_csv('test.csv')  # Replace with actual input filename if different\n",
      "\n",
      "predictions = []\n",
      "\n",
      "for idx in tqdm(range(len(input_df)), desc=\"Predicting\"):\n",
      "    text = input_df.iloc[idx]['text']\n",
      "    # Encode text\n",
      "    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n",
      "    \n",
      "    # Validate input tensor shape\n",
      "    input_shape = inputs['input_ids'].shape\n",
      "    # Expect batch size of 1\n",
      "    if len(input_shape) != 2:\n",
      "        raise ValueError(f\"Unexpected input shape: {input_shape}. Expected shape: (batch_size, sequence_length).\")\n",
      "    if input_shape[0] != 1:\n",
      "        # Reshape to batch size 1 if needed\n",
      "        inputs = {k: tf.reshape(v, (1, -1)) for k, v in inputs.items()}\n",
      "        # Re-validate\n",
      "        new_shape = inputs['input_ids'].shape\n",
      "        if new_shape[0] != 1:\n",
      "            raise ValueError(f\"Failed to reshape input tensors to batch size 1, current shape: {new_shape}\")\n",
      "\n",
      "    # Predict\n",
      "    outputs = model(**inputs)\n",
      "\n",
      "    # Validate model output logits\n",
      "    logits = outputs.logits\n",
      "    logits_shape = logits.shape\n",
      "    if len(logits_shape) != 2:\n",
      "        raise ValueError(f\"Unexpected logits shape: {logits_shape}. Expected shape: (batch_size, num_classes).\")\n",
      "    if logits_shape[0] != 1:\n",
      "        raise ValueError(f\"Logits batch size is not 1: {logits_shape}\")\n",
      "\n",
      "    # Convert logits to probabilities\n",
      "    probs = tf.nn.softmax(logits, axis=1)\n",
      "\n",
      "    # Get predicted class\n",
      "    predicted_class = tf.argmax(probs, axis=1).numpy()[0]\n",
      "    label = map_predicted_class(predicted_class)\n",
      "    predictions.append(label)\n",
      "\n",
      "# Save predictions to predictions.csv\n",
      "output_df = pd.DataFrame({\n",
      "    'text': input_df['text'],\n",
      "    'predicted_label': predictions\n",
      "})\n",
      "\n",
      "output_df.to_csv('predictions.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "class CodeEvaluatorState(TypedDict):\n",
    "    code: str\n",
    "    problem_description: str\n",
    "    problem_input_description: str\n",
    "    problem_output_description: str\n",
    "    model_description: str\n",
    "    model_input_format: str\n",
    "    model_output_format: str\n",
    "    model_requirements: str\n",
    "    improved_code: str | None\n",
    "    evaluation_summary: str | None\n",
    "\n",
    "def code_evaluator_agent(state: CodeEvaluatorState) -> CodeEvaluatorState:\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert code reviewer and machine learning engineer. Your task is to evaluate and improve the given Python code.\n",
    "    \n",
    "    You will be provided with:\n",
    "    - The original code\n",
    "    - Problem description and requirements\n",
    "    - Model information\n",
    "    \n",
    "    ### Input:\n",
    "    - Code to evaluate: {code}\n",
    "    - Problem description: {problem_description}\n",
    "    - Problem input specification: {problem_input_description}\n",
    "    - Problem output specification: {problem_output_description}\n",
    "    - ML model description: {model_description}\n",
    "    - ML model input format: {model_input_format}\n",
    "    - ML model output format: {model_output_format}\n",
    "    - ML model requirements: {model_requirements}\n",
    "\n",
    "    ### Evaluation Criteria\n",
    "    - MANDATORY: Ensure input data shape exactly matches model's expected input format\n",
    "    - MANDATORY: Validate tensor dimensions and batch sizes before model prediction\n",
    "    - MANDATORY: Handle shape mismatches with proper reshaping/preprocessing\n",
    "    - MANDATORY: Add explicit shape validation and error handling for model I/O\n",
    "    - MANDATORY: Ensure model input format matches: {model_input_format}\n",
    "    - MANDATORY: Ensure model output format matches: {model_output_format}\n",
    "    - MANDATORY: Add shape debugging information when shape errors occur\n",
    "       \n",
    "    ### Output Format:\n",
    "    You must return:\n",
    "    1. An evaluation summary highlighting strengths and areas for improvement\n",
    "    2. The improved version of the code incorporating all necessary enhancements\n",
    "    3. You MUST NOT contain logging code\n",
    "    \n",
    "    ### STRICT RULES:\n",
    "    - You MUST ONLY modify how the model works, including necessary processing steps and the model behaviour. DO NOT change the model architecture or any other code fields.\n",
    "    - DO NOT generate any new input. Only use the provided input files from the original codebase.\n",
    "    - DO NOT include any main() function or code block (no if _name_ == \"_main_\" or similar).\n",
    "    - DO NOT include any logging code\n",
    "    - DO NOT change how the output is generated ‚Äî the output must remain identical in structure and content.\n",
    "    - ONLY MODIFY THE ORGINAL CODE BASE IF IT IS NECESSARY TO RUNTIME ERRORS\n",
    "    \n",
    "    ### EXECUTION REQUIREMENTS\n",
    "    - Code must be immediately executable without any user intervention\n",
    "    - Code must export a file of predicted labels in the same format and behaviour in the previous codebase.\n",
    "    - Results should be generated automatically when the script is run\n",
    "    \n",
    "    ### The improved code must:\n",
    "    - DO NOT change how the output is generated ‚Äî the output must remain identical in structure and content.\n",
    "    - Include all necessary imports\n",
    "    - Only modify the model behaviour and input shapes if being mismatch\n",
    "    - Add input validation (especially shape validation for ML models)\n",
    "    - Add proper tensor reshaping to match model's expected input format\n",
    "    - Handle batch dimensions correctly\n",
    "    - Add clear error messages for shape mismatches\n",
    "    \"\"\"\n",
    "    prompt = system_prompt.format(\n",
    "        code=state[\"code\"],\n",
    "        problem_description=state[\"problem_description\"],\n",
    "        problem_input_description=state[\"problem_input_description\"],\n",
    "        problem_output_description=state[\"problem_output_description\"],\n",
    "        model_description=state[\"model_description\"],\n",
    "        model_input_format=state[\"model_input_format\"],\n",
    "        model_output_format=state[\"model_output_format\"],\n",
    "        model_requirements=state[\"model_requirements\"],\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "\n",
    "    # Extract evaluation summary and improved code from response\n",
    "    content = response.content\n",
    "    parts = content.split(\"```python\")\n",
    "    if len(parts) > 1:\n",
    "        evaluation_summary = parts[0].strip()\n",
    "        improved_code = parts[1].split(\"```\")[0].strip()\n",
    "    else:\n",
    "        evaluation_summary = content\n",
    "        improved_code = state[\"code\"]\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"evaluation_summary\": evaluation_summary,\n",
    "        \"improved_code\": improved_code,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage after coding_agent generates code\n",
    "coding_state = coding_agent(prev_state)\n",
    "evaluator_state = {\n",
    "    \"code\": coding_state[\"code\"],\n",
    "    \"problem_description\": prev_state[\"problem_description\"],\n",
    "    \"problem_input_description\": prev_state[\"problem_input_description\"],\n",
    "    \"problem_output_description\": prev_state[\"problem_output_description\"],\n",
    "    \"model_description\": prev_state[\"model_description\"],\n",
    "    \"model_input_format\": prev_state[\"model_input_format\"],\n",
    "    \"model_output_format\": prev_state[\"model_output_format\"],\n",
    "    \"model_requirements\": prev_state[\"model_requirements\"],\n",
    "    \"improved_code\": None,\n",
    "    \"evaluation_summary\": None,\n",
    "}\n",
    "\n",
    "evaluated_state = code_evaluator_agent(evaluator_state)\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "print(evaluated_state[\"evaluation_summary\"])\n",
    "print(\"\\nImproved Code:\")\n",
    "print(evaluated_state[\"improved_code\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "707f22f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import pandas as pd\n",
      "import numpy as np\n",
      "from tqdm import tqdm\n",
      "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
      "import tensorflow as tf\n",
      "\n",
      "# Load tokenizer and model\n",
      "tokenizer = AutoTokenizer.from_pretrained(\"thanhtlx/text_classification_1\")\n",
      "model = TFAutoModelForSequenceClassification.from_pretrained(\"thanhtlx/text_classification_1\")\n",
      "\n",
      "# Define label mapping based on predicted class numbers\n",
      "def map_predicted_class(predicted_class):\n",
      "    if predicted_class in [1, 3, 5, 11, 12, 14]:\n",
      "        return 'Technology'\n",
      "    elif predicted_class in [16, 17, 18]:\n",
      "        return 'Politics'\n",
      "    elif predicted_class in [7, 8, 9, 10]:\n",
      "        return 'Recreation'\n",
      "    elif predicted_class in [6, 15]:\n",
      "        return 'Marketplace'\n",
      "    elif predicted_class in [0, 19]:\n",
      "        return 'Religion'\n",
      "    else:\n",
      "        return 'Unknown'\n",
      "\n",
      "# Read input data\n",
      "# Assuming input file is \"test.csv\" with a column named 'text'\n",
      "input_df = pd.read_csv('test.csv')  # Replace with actual input filename if different\n",
      "\n",
      "predictions = []\n",
      "\n",
      "for idx in tqdm(range(len(input_df)), desc=\"Predicting\"):\n",
      "    text = input_df.iloc[idx]['text']\n",
      "    # Encode text\n",
      "    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n",
      "    \n",
      "    # Validate input tensor shape\n",
      "    input_shape = inputs['input_ids'].shape\n",
      "    # Expect batch size of 1\n",
      "    if len(input_shape) != 2:\n",
      "        raise ValueError(f\"Unexpected input shape: {input_shape}. Expected shape: (batch_size, sequence_length).\")\n",
      "    if input_shape[0] != 1:\n",
      "        # Reshape to batch size 1 if needed\n",
      "        inputs = {k: tf.reshape(v, (1, -1)) for k, v in inputs.items()}\n",
      "        # Re-validate\n",
      "        new_shape = inputs['input_ids'].shape\n",
      "        if new_shape[0] != 1:\n",
      "            raise ValueError(f\"Failed to reshape input tensors to batch size 1, current shape: {new_shape}\")\n",
      "\n",
      "    # Predict\n",
      "    outputs = model(**inputs)\n",
      "\n",
      "    # Validate model output logits\n",
      "    logits = outputs.logits\n",
      "    logits_shape = logits.shape\n",
      "    if len(logits_shape) != 2:\n",
      "        raise ValueError(f\"Unexpected logits shape: {logits_shape}. Expected shape: (batch_size, num_classes).\")\n",
      "    if logits_shape[0] != 1:\n",
      "        raise ValueError(f\"Logits batch size is not 1: {logits_shape}\")\n",
      "\n",
      "    # Convert logits to probabilities\n",
      "    probs = tf.nn.softmax(logits, axis=1)\n",
      "\n",
      "    # Get predicted class\n",
      "    predicted_class = tf.argmax(probs, axis=1).numpy()[0]\n",
      "    label = map_predicted_class(predicted_class)\n",
      "    predictions.append(label)\n",
      "\n",
      "# Save predictions to predictions.csv\n",
      "output_df = pd.DataFrame({\n",
      "    'text': input_df['text'],\n",
      "    'predicted_label': predictions\n",
      "})\n",
      "\n",
      "output_df.to_csv('predictions.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "print(evaluated_state['improved_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "fed251de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/56/7e/67c3fe2b8c33f40af06326a3d6ae7776b3e3a01daa8f71d125d78594d874/torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting huggingface\n",
      "  Obtaining dependency information for huggingface from https://files.pythonhosted.org/packages/f4/8c/e61fbc39c0a37140e1d4941c4af29e2d53bacf9f4559e3de24d8f4e484f0/huggingface-0.0.1-py3-none-any.whl.metadata\n",
      "  Using cached huggingface-0.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface_hub\n",
      "  Obtaining dependency information for huggingface_hub from https://files.pythonhosted.org/packages/33/fb/53587a89fbc00799e4179796f51b3ad713c5de6bb680b2becb6d37c94649/huggingface_hub-0.33.0-py3-none-any.whl.metadata\n",
      "  Using cached huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/96/f2/25b27b396af03d5b64e61976b14f7209e2939e9e806c10749b6d277c273e/transformers-4.52.4-py3-none-any.whl.metadata\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting tqdm\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/01/a5/931fc3ad333d9d87b10107d948d757d67ebcfc33b1988d5faccc39c6845c/pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/b7/f6/bc47f5fa666d5ff4145254f9e618d56e6a4ef9b874654ca74c19113bb538/numpy-2.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached numpy-2.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting safetensors\n",
      "  Obtaining dependency information for safetensors from https://files.pythonhosted.org/packages/a6/f8/dae3421624fcc87a89d42e1898a798bc7ff72c61f38973a65d60df8f124c/safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tf-keras\n",
      "  Obtaining dependency information for tf-keras from https://files.pythonhosted.org/packages/45/6b/d245122d108a94df5969ee7408ad343af1627730e91478e01ef098976bfa/tf_keras-2.19.0-py3-none-any.whl.metadata\n",
      "  Using cached tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting pillow\n",
      "  Obtaining dependency information for pillow from https://files.pythonhosted.org/packages/fe/7c/d8b1330458e4d2f3f45d9508796d7caf0c0d3764c00c823d10f6f1a3b76d/pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/01/12/a8ad8322a7cb2818e658a073feb2aa541d0e6a32b8e5ac838d46e0882687/tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/4d/36/2a115987e2d8c300a974597416d9de88f2444426de9571f4b59b2cca3acc/filelock-3.18.0-py3-none-any.whl.metadata\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Obtaining dependency information for typing-extensions>=4.10.0 from https://files.pythonhosted.org/packages/69/e0/552843e0d356fbb5256d21449fa957fa4eff3bbc135a74a691ee70c7c5da/typing_extensions-4.14.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Obtaining dependency information for setuptools from https://files.pythonhosted.org/packages/a3/dc/17031897dae0efacfea57dfd3a82fdd2a2aeb58e0ff71b77b87e44edc772/setuptools-80.9.0-py3-none-any.whl.metadata\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Obtaining dependency information for sympy>=1.13.3 from https://files.pythonhosted.org/packages/a2/09/77d55d46fd61b4a135c444fc97158ef34a095e5681d0a6c10b75bf356191/sympy-1.14.0-py3-none-any.whl.metadata\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Obtaining dependency information for networkx from https://files.pythonhosted.org/packages/eb/8d/776adee7bbf76365fdd7f2552710282c79a4ead5d2a46408c9043a2b70ba/networkx-3.5-py3-none-any.whl.metadata\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Obtaining dependency information for jinja2 from https://files.pythonhosted.org/packages/62/a1/3d680cbfd5f4b8f15abc1d571870c5fc3e594bb582bc3b64ea099db13e56/jinja2-3.1.6-py3-none-any.whl.metadata\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/bb/61/78c7b3851add1481b048b5fdc29067397a1784e2910592bc81bb3f608635/fsspec-2025.5.1-py3-none-any.whl.metadata\n",
      "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Obtaining dependency information for nvidia-cuda-nvrtc-cu12==12.6.77 from https://files.pythonhosted.org/packages/75/2e/46030320b5a80661e88039f59060d1790298b4718944a65a7f2aeda3d9e9/nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Obtaining dependency information for nvidia-cuda-runtime-cu12==12.6.77 from https://files.pythonhosted.org/packages/e1/23/e717c5ac26d26cf39a27fbc076240fad2e3b817e5889d671b67f4f9f49c5/nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Obtaining dependency information for nvidia-cuda-cupti-cu12==12.6.80 from https://files.pythonhosted.org/packages/49/60/7b6497946d74bcf1de852a21824d63baad12cd417db4195fc1bfe59db953/nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Obtaining dependency information for nvidia-cudnn-cu12==9.5.1.17 from https://files.pythonhosted.org/packages/2a/78/4535c9c7f859a64781e43c969a3a7e84c54634e319a996d43ef32ce46f83/nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Obtaining dependency information for nvidia-cublas-cu12==12.6.4.1 from https://files.pythonhosted.org/packages/af/eb/ff4b8c503fa1f1796679dce648854d58751982426e4e4b37d6fce49d259c/nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Obtaining dependency information for nvidia-cufft-cu12==11.3.0.4 from https://files.pythonhosted.org/packages/8f/16/73727675941ab8e6ffd86ca3a4b7b47065edcca7a997920b831f8147c99d/nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Obtaining dependency information for nvidia-curand-cu12==10.3.7.77 from https://files.pythonhosted.org/packages/73/1b/44a01c4e70933637c93e6e1a8063d1e998b50213a6b65ac5a9169c47e98e/nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Obtaining dependency information for nvidia-cusolver-cu12==11.7.1.2 from https://files.pythonhosted.org/packages/f0/6e/c2cf12c9ff8b872e92b4a5740701e51ff17689c4d726fca91875b07f655d/nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Obtaining dependency information for nvidia-cusparse-cu12==12.5.4.2 from https://files.pythonhosted.org/packages/06/1e/b8b7c2f4099a37b96af5c9bb158632ea9e5d9d27d7391d7eb8fc45236674/nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Obtaining dependency information for nvidia-cusparselt-cu12==0.6.3 from https://files.pythonhosted.org/packages/3b/9a/72ef35b399b0e183bc2e8f6f558036922d453c4d8237dab26c666a04244b/nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Obtaining dependency information for nvidia-nccl-cu12==2.26.2 from https://files.pythonhosted.org/packages/67/ca/f42388aed0fddd64ade7493dbba36e1f534d4e6fdbdd355c6a90030ae028/nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Obtaining dependency information for nvidia-nvtx-cu12==12.6.77 from https://files.pythonhosted.org/packages/56/9a/fff8376f8e3d084cd1530e1ef7b879bb7d6d265620c95c1b322725c694f4/nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Obtaining dependency information for nvidia-nvjitlink-cu12==12.6.85 from https://files.pythonhosted.org/packages/9d/d7/c5383e47c7e9bf1c99d5bd2a8c935af2b6d705ad831a7ec5c97db4d82f4f/nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Obtaining dependency information for nvidia-cufile-cu12==1.11.1.6 from https://files.pythonhosted.org/packages/b2/66/cc9876340ac68ae71b15c743ddb13f8b30d5244af344ec8322b449e35426/nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch)\n",
      "  Obtaining dependency information for triton==3.3.1 from https://files.pythonhosted.org/packages/24/5f/950fb373bf9c01ad4eb5a8cd5eaf32cdf9e238c02f9293557a2129b9c4ac/triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata\n",
      "  Using cached triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting packaging>=20.9 (from huggingface_hub)\n",
      "  Obtaining dependency information for packaging>=20.9 from https://files.pythonhosted.org/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl.metadata\n",
      "  Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from huggingface_hub)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/b9/2b/614b4752f2e127db5cc206abc23a8c19678e92b23c3db30fc86ab731d3bd/PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface_hub)\n",
      "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/7c/e4/56027c4a6b4ae70ca9de302488c5ca95ad4a39e190093d6c1a8ace08341b/requests-2.32.4-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub)\n",
      "  Obtaining dependency information for hf-xet<2.0.0,>=1.1.2 from https://files.pythonhosted.org/packages/d1/a8/49a81d4f81b0d21cc758b6fca3880a85ca0d209e8425c8b3a6ef694881ca/hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/fb/13/e3b075031a738c9598c51cfbc4c7879e26729c53aa9cca59211c44235314/regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.22,>=0.21 from https://files.pythonhosted.org/packages/8a/63/38be071b0c8e06840bc6046991636bcb30c27f6bb1e670f4f4bc87cf49cc/tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Obtaining dependency information for python-dateutil>=2.8.2 from https://files.pythonhosted.org/packages/ec/57/56b9bcc3c9c6a792fcbaf139543cee77261f3651ca9da0c93f5c1221264b/python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/81/c4/34e93fe5f5429d7570ec1fa436f1986fb1f00c3e0f43a589fe2bbcd22c3f/pytz-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Obtaining dependency information for tzdata>=2022.7 from https://files.pythonhosted.org/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for absl-py>=1.0.0 from https://files.pythonhosted.org/packages/87/04/9d75e1d3bb4ab8ec67ff10919476ccdee06c098bcfcf3a352da5f985171d/absl_py-2.3.0-py3-none-any.whl.metadata\n",
      "  Using cached absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Obtaining dependency information for astunparse>=1.6.0 from https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl.metadata\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=24.3.25 from https://files.pythonhosted.org/packages/b8/25/155f9f080d5e4bc0082edfda032ea2bc2b8fab3f4d25d46c1e9dd22a1a89/flatbuffers-25.2.10-py2.py3-none-any.whl.metadata\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Obtaining dependency information for gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 from https://files.pythonhosted.org/packages/a3/61/8001b38461d751cd1a0c3a6ae84346796a5758123f3ed97a1b121dfbf4f3/gast-0.6.0-py3-none-any.whl.metadata\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Obtaining dependency information for google-pasta>=0.1.1 from https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl.metadata\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/1d/fc/716c1e62e512ef1c160e7984a73a5fc7df45166f2ff3f254e71c58076f7c/libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Obtaining dependency information for opt-einsum>=2.3.2 from https://files.pythonhosted.org/packages/23/cd/066e86230ae37ed0be70aae89aabf03ca8d9f39c8aea0dec8029455b5540/opt_einsum-3.4.0-py3-none-any.whl.metadata\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/85/e4/07c80521879c2d15f321465ac24c70efe2381378c00bf5e56a0f4fbac8cd/protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata\n",
      "  Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting six>=1.12.0 (from tensorflow)\n",
      "  Obtaining dependency information for six>=1.12.0 from https://files.pythonhosted.org/packages/b7/ce/149a00dd41f10bc29e5921b496af8b574d8413afcd5e30dfa0ed46c2cc5e/six-1.17.0-py2.py3-none-any.whl.metadata\n",
      "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Obtaining dependency information for termcolor>=1.1.0 from https://files.pythonhosted.org/packages/4f/bd/de8d508070629b6d84a30d01d57e4a65c69aa7f5abe7560b8fad3b50ea59/termcolor-3.1.0-py3-none-any.whl.metadata\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Obtaining dependency information for wrapt>=1.11.0 from https://files.pythonhosted.org/packages/2a/5a/04cde32b07a7431d4ed0553a76fdb7a61270e78c5fd5a603e190ac389f14/wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/b0/e6/13cfea15e3b8f79c4ae7b676cb21fab70978b0fde1e1d28bb0e073291290/grpcio-1.73.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached grpcio-1.73.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorboard~=2.19.0 from https://files.pythonhosted.org/packages/5d/12/4f70e8e2ba0dbe72ea978429d8530b0333f0ed2140cc571a48802878ef99/tensorboard-2.19.0-py3-none-any.whl.metadata\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Obtaining dependency information for keras>=3.5.0 from https://files.pythonhosted.org/packages/95/e6/4179c461a5fc43e3736880f64dbdc9b1a5349649f0ae32ded927c0e3a227/keras-3.10.0-py3-none-any.whl.metadata\n",
      "  Using cached keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting numpy\n",
      "  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/9e/3e/3757f304c704f2f0294a6b8340fcf2be244038be07da4cccf390fa678a9f/numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Obtaining dependency information for h5py>=3.11.0 from https://files.pythonhosted.org/packages/86/f9/f00de11c82c88bfc1ef22633557bfba9e271e0cb3189ad704183fc4a2644/h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Obtaining dependency information for ml-dtypes<1.0.0,>=0.5.1 from https://files.pythonhosted.org/packages/08/57/5d58fad4124192b1be42f68bd0c0ddaa26e44a730ff8c9337adade2f5632/ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Obtaining dependency information for wheel<1.0,>=0.23.0 from https://files.pythonhosted.org/packages/0b/2c/87f3254fd8ffd29e4c02732eee68a83a1d3c346ae39bc6822dcbcb697f2b/wheel-0.45.1-py3-none-any.whl.metadata\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Obtaining dependency information for rich from https://files.pythonhosted.org/packages/0d/9b/63f4c7ebc259242c89b3acafdb37b41d1185c07ff0011164674e9076b491/rich-14.0.0-py3-none-any.whl.metadata\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Obtaining dependency information for namex from https://files.pythonhosted.org/packages/b2/bc/465daf1de06409cdd4532082806770ee0d8d7df434da79c76564d0f69741/namex-0.1.0-py3-none-any.whl.metadata\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Obtaining dependency information for optree from https://files.pythonhosted.org/packages/e5/33/48ac6749986e440838990a16beb830ddc1c30d8dba150a030a53377abf77/optree-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached optree-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->huggingface_hub)\n",
      "  Obtaining dependency information for charset_normalizer<4,>=2 from https://files.pythonhosted.org/packages/8c/73/6ede2ec59bce19b3edf4209d70004253ec5f4e319f9a2e3f2f15601ed5f7/charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface_hub)\n",
      "  Obtaining dependency information for idna<4,>=2.5 from https://files.pythonhosted.org/packages/76/c6/c88e154df9c4e1a2a66ccf0005a88dfb2650c1dffb6f5ce603dfbd452ce3/idna-3.10-py3-none-any.whl.metadata\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface_hub)\n",
      "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/a7/c2/fe1e52489ae3122415c51f387e221dd0773709bad6c6cdaa599e8a2c5185/urllib3-2.5.0-py3-none-any.whl.metadata\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface_hub)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/84/ae/320161bd181fc06471eed047ecce67b693fd7515b16d495d8932db763426/certifi-2025.6.15-py3-none-any.whl.metadata\n",
      "  Using cached certifi-2025.6.15-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Obtaining dependency information for mpmath<1.4,>=1.1.0 from https://files.pythonhosted.org/packages/43/e3/7d92a15f894aa0c9c4b49b8ee9ac9850d6e63b03c9c32c0367a13ae62209/mpmath-1.3.0-py3-none-any.whl.metadata\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/96/2b/34cc11786bc00d0f04d0f5fdc3a2b1ae0b6239eef72d3d345805f9ad92a1/markdown-3.8.2-py3-none-any.whl.metadata\n",
      "  Using cached markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/73/c6/825dab04195756cf8ff2e12698f22513b3db2f64925bdd41671bfb33aaa5/tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Obtaining dependency information for werkzeug>=1.0.1 from https://files.pythonhosted.org/packages/52/24/ab44c871b0f07f491e5d2ad12c9bd7358e527510618cb1b803a88e986db1/werkzeug-3.1.3-py3-none-any.whl.metadata\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/f3/f0/89e7aadfb3749d0f52234a0c8c7867877876e0a20b60e2188e9850794c17/MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Obtaining dependency information for markdown-it-py>=2.2.0 from https://files.pythonhosted.org/packages/42/d7/1ec15b46af6af88f19b8e5ffea08fa375d433c998b8a7639e76935c14f1f/markdown_it_py-3.0.0-py3-none-any.whl.metadata\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Obtaining dependency information for pygments<3.0.0,>=2.13.0 from https://files.pythonhosted.org/packages/8a/0b/9fcc47d19c48b59121088dd6da2488a49d5f72dacf8262e2790a1d2c7d15/pygments-2.19.1-py3-none-any.whl.metadata\n",
      "  Using cached pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Obtaining dependency information for mdurl~=0.1 from https://files.pythonhosted.org/packages/b3/38/89ba8ad64ae25be8de66a6d463314cf1eb366222074cfda9ee839c56a4b4/mdurl-0.1.2-py3-none-any.whl.metadata\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached torch-2.7.1-cp312-cp312-manylinux_2_28_x86_64.whl (821.0 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "Using cached huggingface-0.0.1-py3-none-any.whl (2.5 kB)\n",
      "Using cached huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "Using cached pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "Using cached tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645.0 MB)\n",
      "Using cached numpy-2.1.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "Using cached absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.73.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "Using cached h5py-3.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "Using cached hf_xet-1.1.4-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached ml_dtypes-0.5.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached packaging-25.0-py3-none-any.whl (66 kB)\n",
      "Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached wrapt-1.17.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached certifi-2025.6.15-py3-none-any.whl (157 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached optree-0.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, nvidia-cusparselt-cu12, namex, mpmath, libclang, huggingface, flatbuffers, wrapt, wheel, urllib3, tzdata, typing-extensions, tqdm, termcolor, tensorboard-data-server, sympy, six, setuptools, safetensors, regex, pyyaml, pygments, protobuf, pillow, packaging, opt-einsum, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, mdurl, MarkupSafe, markdown, idna, hf-xet, grpcio, gast, fsspec, filelock, charset_normalizer, certifi, absl-py, werkzeug, triton, requests, python-dateutil, optree, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, ml-dtypes, markdown-it-py, jinja2, h5py, google-pasta, astunparse, tensorboard, rich, pandas, nvidia-cusolver-cu12, huggingface_hub, torch, tokenizers, keras, transformers, tensorflow, tf-keras\n",
      "Successfully installed MarkupSafe-3.0.2 absl-py-2.3.0 astunparse-1.6.3 certifi-2025.6.15 charset_normalizer-3.4.2 filelock-3.18.0 flatbuffers-25.2.10 fsspec-2025.5.1 gast-0.6.0 google-pasta-0.2.0 grpcio-1.73.0 h5py-3.14.0 hf-xet-1.1.4 huggingface-0.0.1 huggingface_hub-0.33.0 idna-3.10 jinja2-3.1.6 keras-3.10.0 libclang-18.1.1 markdown-3.8.2 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 mpmath-1.3.0 namex-0.1.0 networkx-3.5 numpy-2.1.3 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 opt-einsum-3.4.0 optree-0.16.0 packaging-25.0 pandas-2.3.0 pillow-11.2.1 protobuf-5.29.5 pygments-2.19.1 python-dateutil-2.9.0.post0 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.4 rich-14.0.0 safetensors-0.5.3 setuptools-80.9.0 six-1.17.0 sympy-1.14.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-3.1.0 tf-keras-2.19.0 tokenizers-0.21.1 torch-2.7.1 tqdm-4.67.1 transformers-4.52.4 triton-3.3.1 typing-extensions-4.14.0 tzdata-2025.2 urllib3-2.5.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/workspaces/ISE-challenge/Test/code_runner_7l98gqas/venv/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-20 10:05:58.310465: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750413958.368278   65725 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750413958.384897   65725 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1750413958.492040   65725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750413958.492074   65725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750413958.492076   65725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1750413958.492078   65725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-20 10:05:58.502375: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-20 10:06:01.796371: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2025-06-20 10:06:01.865865: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2025-06-20 10:06:02.082749: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2025-06-20 10:06:02.107848: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "2025-06-20 10:06:03.411647: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "Some layers from the model checkpoint at thanhtlx/text_classification_1 were not used when initializing TFDistilBertForSequenceClassification: ['dropout_79']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at thanhtlx/text_classification_1 and are newly initialized: ['dropout_19']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Predicting:   0%|          | 0/200 [00:00<?, ?it/s]\n",
      "Predicting:   0%|          | 1/200 [00:00<01:05,  3.06it/s]\n",
      "Predicting:   1%|          | 2/200 [00:00<01:05,  3.02it/s]\n",
      "Predicting:   2%|‚ñè         | 3/200 [00:00<00:50,  3.92it/s]\n",
      "Predicting:   2%|‚ñè         | 4/200 [00:01<01:09,  2.82it/s]\n",
      "Predicting:   2%|‚ñé         | 5/200 [00:01<00:51,  3.76it/s]\n",
      "Predicting:   3%|‚ñé         | 6/200 [00:01<01:05,  2.94it/s]\n",
      "Predicting:   4%|‚ñé         | 7/200 [00:02<00:55,  3.50it/s]\n",
      "Predicting:   4%|‚ñç         | 8/200 [00:02<00:45,  4.18it/s]\n",
      "Predicting:   5%|‚ñå         | 10/200 [00:02<00:35,  5.37it/s]\n",
      "Predicting:   6%|‚ñå         | 11/200 [00:02<00:35,  5.36it/s]\n",
      "Predicting:   6%|‚ñå         | 12/200 [00:02<00:32,  5.80it/s]\n",
      "Predicting:   6%|‚ñã         | 13/200 [00:03<00:38,  4.87it/s]\n",
      "Predicting:   7%|‚ñã         | 14/200 [00:03<00:39,  4.66it/s]\n",
      "Predicting:   8%|‚ñä         | 15/200 [00:03<00:45,  4.05it/s]\n",
      "Predicting:   8%|‚ñä         | 16/200 [00:03<00:41,  4.47it/s]\n",
      "Predicting:   8%|‚ñä         | 17/200 [00:03<00:35,  5.09it/s]\n",
      "Predicting:   9%|‚ñâ         | 18/200 [00:04<00:42,  4.27it/s]\n",
      "Predicting:  10%|‚ñâ         | 19/200 [00:04<00:37,  4.85it/s]\n",
      "Predicting:  10%|‚ñà         | 20/200 [00:04<00:34,  5.16it/s]\n",
      "Predicting:  10%|‚ñà         | 21/200 [00:04<00:34,  5.16it/s]\n",
      "Predicting:  11%|‚ñà         | 22/200 [00:04<00:31,  5.59it/s]\n",
      "Predicting:  12%|‚ñà‚ñè        | 23/200 [00:05<00:30,  5.89it/s]\n",
      "Predicting:  12%|‚ñà‚ñè        | 24/200 [00:05<00:27,  6.45it/s]\n",
      "Predicting:  12%|‚ñà‚ñé        | 25/200 [00:05<00:27,  6.35it/s]\n",
      "Predicting:  13%|‚ñà‚ñé        | 26/200 [00:05<00:44,  3.93it/s]\n",
      "Predicting:  14%|‚ñà‚ñé        | 27/200 [00:05<00:37,  4.63it/s]\n",
      "Predicting:  14%|‚ñà‚ñç        | 28/200 [00:06<00:33,  5.14it/s]\n",
      "Predicting:  14%|‚ñà‚ñç        | 29/200 [00:06<00:34,  4.99it/s]\n",
      "Predicting:  15%|‚ñà‚ñå        | 30/200 [00:06<00:29,  5.82it/s]\n",
      "Predicting:  16%|‚ñà‚ñå        | 31/200 [00:06<00:28,  5.85it/s]\n",
      "Predicting:  16%|‚ñà‚ñå        | 32/200 [00:06<00:34,  4.88it/s]\n",
      "Predicting:  16%|‚ñà‚ñã        | 33/200 [00:07<00:37,  4.50it/s]\n",
      "Predicting:  17%|‚ñà‚ñã        | 34/200 [00:07<00:32,  5.19it/s]\n",
      "Predicting:  18%|‚ñà‚ñä        | 35/200 [00:07<00:46,  3.55it/s]\n",
      "Predicting:  18%|‚ñà‚ñä        | 36/200 [00:07<00:38,  4.31it/s]\n",
      "Predicting:  18%|‚ñà‚ñä        | 37/200 [00:08<00:45,  3.57it/s]\n",
      "Predicting:  19%|‚ñà‚ñâ        | 38/200 [00:08<00:46,  3.52it/s]\n",
      "Predicting:  20%|‚ñà‚ñâ        | 39/200 [00:09<00:53,  3.00it/s]\n",
      "Predicting:  20%|‚ñà‚ñà        | 40/200 [00:09<00:48,  3.31it/s]\n",
      "Predicting:  20%|‚ñà‚ñà        | 41/200 [00:09<00:39,  4.00it/s]\n",
      "Predicting:  21%|‚ñà‚ñà        | 42/200 [00:09<00:34,  4.61it/s]\n",
      "Predicting:  22%|‚ñà‚ñà‚ñè       | 43/200 [00:09<00:30,  5.13it/s]\n",
      "Predicting:  22%|‚ñà‚ñà‚ñè       | 44/200 [00:09<00:29,  5.26it/s]\n",
      "Predicting:  22%|‚ñà‚ñà‚ñé       | 45/200 [00:10<00:38,  3.99it/s]\n",
      "Predicting:  23%|‚ñà‚ñà‚ñé       | 46/200 [00:10<00:32,  4.73it/s]\n",
      "Predicting:  24%|‚ñà‚ñà‚ñé       | 47/200 [00:10<00:32,  4.64it/s]\n",
      "Predicting:  24%|‚ñà‚ñà‚ñç       | 48/200 [00:10<00:27,  5.49it/s]\n",
      "Predicting:  24%|‚ñà‚ñà‚ñç       | 49/200 [00:10<00:24,  6.06it/s]\n",
      "Predicting:  25%|‚ñà‚ñà‚ñå       | 50/200 [00:10<00:25,  5.93it/s]\n",
      "Predicting:  26%|‚ñà‚ñà‚ñå       | 51/200 [00:11<00:26,  5.69it/s]\n",
      "Predicting:  26%|‚ñà‚ñà‚ñå       | 52/200 [00:11<00:32,  4.54it/s]\n",
      "Predicting:  26%|‚ñà‚ñà‚ñã       | 53/200 [00:11<00:34,  4.25it/s]\n",
      "Predicting:  27%|‚ñà‚ñà‚ñã       | 54/200 [00:12<00:37,  3.85it/s]\n",
      "Predicting:  28%|‚ñà‚ñà‚ñä       | 55/200 [00:12<00:40,  3.62it/s]\n",
      "Predicting:  28%|‚ñà‚ñà‚ñä       | 56/200 [00:12<00:37,  3.81it/s]\n",
      "Predicting:  28%|‚ñà‚ñà‚ñä       | 57/200 [00:12<00:31,  4.51it/s]\n",
      "Predicting:  29%|‚ñà‚ñà‚ñâ       | 58/200 [00:13<00:42,  3.35it/s]\n",
      "Predicting:  29%|‚ñà‚ñà‚ñâ       | 58/200 [00:13<00:32,  4.38it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/ISE-challenge/Test/code_runner_7l98gqas/project/main.py\", line 37, in <module>\n",
      "    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/ISE-challenge/Test/code_runner_7l98gqas/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2867, in __call__\n",
      "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspaces/ISE-challenge/Test/code_runner_7l98gqas/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2927, in _call_one\n",
      "    raise ValueError(\n",
      "ValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Execution failed:\n2025-06-20 10:05:58.310465: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750413958.368278   65725 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750413958.384897   65725 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1750413958.492040   65725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1750413958.492074   65725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1750413958.492076   65725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1750413958.492078   65725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-06-20 10:05:58.502375: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-06-20 10:06:01.796371: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n2025-06-20 10:06:01.865865: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n2025-06-20 10:06:02.082749: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n2025-06-20 10:06:02.107848: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n2025-06-20 10:06:03.411647: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\nSome layers from the model checkpoint at thanhtlx/text_classification_1 were not used when initializing TFDistilBertForSequenceClassification: ['dropout_79']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at thanhtlx/text_classification_1 and are newly initialized: ['dropout_19']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nPredicting:   0%|          | 0/200 [00:00<?, ?it/s]\nPredicting:   0%|          | 1/200 [00:00<01:05,  3.06it/s]\nPredicting:   1%|          | 2/200 [00:00<01:05,  3.02it/s]\nPredicting:   2%|‚ñè         | 3/200 [00:00<00:50,  3.92it/s]\nPredicting:   2%|‚ñè         | 4/200 [00:01<01:09,  2.82it/s]\nPredicting:   2%|‚ñé         | 5/200 [00:01<00:51,  3.76it/s]\nPredicting:   3%|‚ñé         | 6/200 [00:01<01:05,  2.94it/s]\nPredicting:   4%|‚ñé         | 7/200 [00:02<00:55,  3.50it/s]\nPredicting:   4%|‚ñç         | 8/200 [00:02<00:45,  4.18it/s]\nPredicting:   5%|‚ñå         | 10/200 [00:02<00:35,  5.37it/s]\nPredicting:   6%|‚ñå         | 11/200 [00:02<00:35,  5.36it/s]\nPredicting:   6%|‚ñå         | 12/200 [00:02<00:32,  5.80it/s]\nPredicting:   6%|‚ñã         | 13/200 [00:03<00:38,  4.87it/s]\nPredicting:   7%|‚ñã         | 14/200 [00:03<00:39,  4.66it/s]\nPredicting:   8%|‚ñä         | 15/200 [00:03<00:45,  4.05it/s]\nPredicting:   8%|‚ñä         | 16/200 [00:03<00:41,  4.47it/s]\nPredicting:   8%|‚ñä         | 17/200 [00:03<00:35,  5.09it/s]\nPredicting:   9%|‚ñâ         | 18/200 [00:04<00:42,  4.27it/s]\nPredicting:  10%|‚ñâ         | 19/200 [00:04<00:37,  4.85it/s]\nPredicting:  10%|‚ñà         | 20/200 [00:04<00:34,  5.16it/s]\nPredicting:  10%|‚ñà         | 21/200 [00:04<00:34,  5.16it/s]\nPredicting:  11%|‚ñà         | 22/200 [00:04<00:31,  5.59it/s]\nPredicting:  12%|‚ñà‚ñè        | 23/200 [00:05<00:30,  5.89it/s]\nPredicting:  12%|‚ñà‚ñè        | 24/200 [00:05<00:27,  6.45it/s]\nPredicting:  12%|‚ñà‚ñé        | 25/200 [00:05<00:27,  6.35it/s]\nPredicting:  13%|‚ñà‚ñé        | 26/200 [00:05<00:44,  3.93it/s]\nPredicting:  14%|‚ñà‚ñé        | 27/200 [00:05<00:37,  4.63it/s]\nPredicting:  14%|‚ñà‚ñç        | 28/200 [00:06<00:33,  5.14it/s]\nPredicting:  14%|‚ñà‚ñç        | 29/200 [00:06<00:34,  4.99it/s]\nPredicting:  15%|‚ñà‚ñå        | 30/200 [00:06<00:29,  5.82it/s]\nPredicting:  16%|‚ñà‚ñå        | 31/200 [00:06<00:28,  5.85it/s]\nPredicting:  16%|‚ñà‚ñå        | 32/200 [00:06<00:34,  4.88it/s]\nPredicting:  16%|‚ñà‚ñã        | 33/200 [00:07<00:37,  4.50it/s]\nPredicting:  17%|‚ñà‚ñã        | 34/200 [00:07<00:32,  5.19it/s]\nPredicting:  18%|‚ñà‚ñä        | 35/200 [00:07<00:46,  3.55it/s]\nPredicting:  18%|‚ñà‚ñä        | 36/200 [00:07<00:38,  4.31it/s]\nPredicting:  18%|‚ñà‚ñä        | 37/200 [00:08<00:45,  3.57it/s]\nPredicting:  19%|‚ñà‚ñâ        | 38/200 [00:08<00:46,  3.52it/s]\nPredicting:  20%|‚ñà‚ñâ        | 39/200 [00:09<00:53,  3.00it/s]\nPredicting:  20%|‚ñà‚ñà        | 40/200 [00:09<00:48,  3.31it/s]\nPredicting:  20%|‚ñà‚ñà        | 41/200 [00:09<00:39,  4.00it/s]\nPredicting:  21%|‚ñà‚ñà        | 42/200 [00:09<00:34,  4.61it/s]\nPredicting:  22%|‚ñà‚ñà‚ñè       | 43/200 [00:09<00:30,  5.13it/s]\nPredicting:  22%|‚ñà‚ñà‚ñè       | 44/200 [00:09<00:29,  5.26it/s]\nPredicting:  22%|‚ñà‚ñà‚ñé       | 45/200 [00:10<00:38,  3.99it/s]\nPredicting:  23%|‚ñà‚ñà‚ñé       | 46/200 [00:10<00:32,  4.73it/s]\nPredicting:  24%|‚ñà‚ñà‚ñé       | 47/200 [00:10<00:32,  4.64it/s]\nPredicting:  24%|‚ñà‚ñà‚ñç       | 48/200 [00:10<00:27,  5.49it/s]\nPredicting:  24%|‚ñà‚ñà‚ñç       | 49/200 [00:10<00:24,  6.06it/s]\nPredicting:  25%|‚ñà‚ñà‚ñå       | 50/200 [00:10<00:25,  5.93it/s]\nPredicting:  26%|‚ñà‚ñà‚ñå       | 51/200 [00:11<00:26,  5.69it/s]\nPredicting:  26%|‚ñà‚ñà‚ñå       | 52/200 [00:11<00:32,  4.54it/s]\nPredicting:  26%|‚ñà‚ñà‚ñã       | 53/200 [00:11<00:34,  4.25it/s]\nPredicting:  27%|‚ñà‚ñà‚ñã       | 54/200 [00:12<00:37,  3.85it/s]\nPredicting:  28%|‚ñà‚ñà‚ñä       | 55/200 [00:12<00:40,  3.62it/s]\nPredicting:  28%|‚ñà‚ñà‚ñä       | 56/200 [00:12<00:37,  3.81it/s]\nPredicting:  28%|‚ñà‚ñà‚ñä       | 57/200 [00:12<00:31,  4.51it/s]\nPredicting:  29%|‚ñà‚ñà‚ñâ       | 58/200 [00:13<00:42,  3.35it/s]\nPredicting:  29%|‚ñà‚ñà‚ñâ       | 58/200 [00:13<00:32,  4.38it/s]\nTraceback (most recent call last):\n  File \"/workspaces/ISE-challenge/Test/code_runner_7l98gqas/project/main.py\", line 37, in <module>\n    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspaces/ISE-challenge/Test/code_runner_7l98gqas/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2867, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspaces/ISE-challenge/Test/code_runner_7l98gqas/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2927, in _call_one\n    raise ValueError(\nValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[151]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    101\u001b[39m         \u001b[38;5;66;03m# Step 7: Cleanup temp folder\u001b[39;00m\n\u001b[32m    102\u001b[39m         shutil.rmtree(temp_dir, ignore_errors=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43mcode_execute_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpredictions.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoding_state\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcode\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m```python\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m```\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuggingface\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuggingface_hub\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtransformers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtqdm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpandas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnumpy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafetensors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtf-keras\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpillow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtensorflow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m    \u001b[49m\u001b[32;43m1200000\u001b[39;49m\n\u001b[32m    111\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[151]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mcode_execute_agent\u001b[39m\u001b[34m(input_path, output_path, code, requirements, timeout)\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCode execution timed out.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m process.returncode != \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExecution failed:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m.join(log_lines)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Step 6: Copy specific output file to output_path\u001b[39;00m\n\u001b[32m     90\u001b[39m produced_file = project_dir / output_path.name\n",
      "\u001b[31mRuntimeError\u001b[39m: Execution failed:\n2025-06-20 10:05:58.310465: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750413958.368278   65725 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750413958.384897   65725 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1750413958.492040   65725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1750413958.492074   65725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1750413958.492076   65725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1750413958.492078   65725 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-06-20 10:05:58.502375: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-06-20 10:06:01.796371: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n2025-06-20 10:06:01.865865: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n2025-06-20 10:06:02.082749: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n2025-06-20 10:06:02.107848: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n2025-06-20 10:06:03.411647: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\nSome layers from the model checkpoint at thanhtlx/text_classification_1 were not used when initializing TFDistilBertForSequenceClassification: ['dropout_79']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at thanhtlx/text_classification_1 and are newly initialized: ['dropout_19']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\nPredicting:   0%|          | 0/200 [00:00<?, ?it/s]\nPredicting:   0%|          | 1/200 [00:00<01:05,  3.06it/s]\nPredicting:   1%|          | 2/200 [00:00<01:05,  3.02it/s]\nPredicting:   2%|‚ñè         | 3/200 [00:00<00:50,  3.92it/s]\nPredicting:   2%|‚ñè         | 4/200 [00:01<01:09,  2.82it/s]\nPredicting:   2%|‚ñé         | 5/200 [00:01<00:51,  3.76it/s]\nPredicting:   3%|‚ñé         | 6/200 [00:01<01:05,  2.94it/s]\nPredicting:   4%|‚ñé         | 7/200 [00:02<00:55,  3.50it/s]\nPredicting:   4%|‚ñç         | 8/200 [00:02<00:45,  4.18it/s]\nPredicting:   5%|‚ñå         | 10/200 [00:02<00:35,  5.37it/s]\nPredicting:   6%|‚ñå         | 11/200 [00:02<00:35,  5.36it/s]\nPredicting:   6%|‚ñå         | 12/200 [00:02<00:32,  5.80it/s]\nPredicting:   6%|‚ñã         | 13/200 [00:03<00:38,  4.87it/s]\nPredicting:   7%|‚ñã         | 14/200 [00:03<00:39,  4.66it/s]\nPredicting:   8%|‚ñä         | 15/200 [00:03<00:45,  4.05it/s]\nPredicting:   8%|‚ñä         | 16/200 [00:03<00:41,  4.47it/s]\nPredicting:   8%|‚ñä         | 17/200 [00:03<00:35,  5.09it/s]\nPredicting:   9%|‚ñâ         | 18/200 [00:04<00:42,  4.27it/s]\nPredicting:  10%|‚ñâ         | 19/200 [00:04<00:37,  4.85it/s]\nPredicting:  10%|‚ñà         | 20/200 [00:04<00:34,  5.16it/s]\nPredicting:  10%|‚ñà         | 21/200 [00:04<00:34,  5.16it/s]\nPredicting:  11%|‚ñà         | 22/200 [00:04<00:31,  5.59it/s]\nPredicting:  12%|‚ñà‚ñè        | 23/200 [00:05<00:30,  5.89it/s]\nPredicting:  12%|‚ñà‚ñè        | 24/200 [00:05<00:27,  6.45it/s]\nPredicting:  12%|‚ñà‚ñé        | 25/200 [00:05<00:27,  6.35it/s]\nPredicting:  13%|‚ñà‚ñé        | 26/200 [00:05<00:44,  3.93it/s]\nPredicting:  14%|‚ñà‚ñé        | 27/200 [00:05<00:37,  4.63it/s]\nPredicting:  14%|‚ñà‚ñç        | 28/200 [00:06<00:33,  5.14it/s]\nPredicting:  14%|‚ñà‚ñç        | 29/200 [00:06<00:34,  4.99it/s]\nPredicting:  15%|‚ñà‚ñå        | 30/200 [00:06<00:29,  5.82it/s]\nPredicting:  16%|‚ñà‚ñå        | 31/200 [00:06<00:28,  5.85it/s]\nPredicting:  16%|‚ñà‚ñå        | 32/200 [00:06<00:34,  4.88it/s]\nPredicting:  16%|‚ñà‚ñã        | 33/200 [00:07<00:37,  4.50it/s]\nPredicting:  17%|‚ñà‚ñã        | 34/200 [00:07<00:32,  5.19it/s]\nPredicting:  18%|‚ñà‚ñä        | 35/200 [00:07<00:46,  3.55it/s]\nPredicting:  18%|‚ñà‚ñä        | 36/200 [00:07<00:38,  4.31it/s]\nPredicting:  18%|‚ñà‚ñä        | 37/200 [00:08<00:45,  3.57it/s]\nPredicting:  19%|‚ñà‚ñâ        | 38/200 [00:08<00:46,  3.52it/s]\nPredicting:  20%|‚ñà‚ñâ        | 39/200 [00:09<00:53,  3.00it/s]\nPredicting:  20%|‚ñà‚ñà        | 40/200 [00:09<00:48,  3.31it/s]\nPredicting:  20%|‚ñà‚ñà        | 41/200 [00:09<00:39,  4.00it/s]\nPredicting:  21%|‚ñà‚ñà        | 42/200 [00:09<00:34,  4.61it/s]\nPredicting:  22%|‚ñà‚ñà‚ñè       | 43/200 [00:09<00:30,  5.13it/s]\nPredicting:  22%|‚ñà‚ñà‚ñè       | 44/200 [00:09<00:29,  5.26it/s]\nPredicting:  22%|‚ñà‚ñà‚ñé       | 45/200 [00:10<00:38,  3.99it/s]\nPredicting:  23%|‚ñà‚ñà‚ñé       | 46/200 [00:10<00:32,  4.73it/s]\nPredicting:  24%|‚ñà‚ñà‚ñé       | 47/200 [00:10<00:32,  4.64it/s]\nPredicting:  24%|‚ñà‚ñà‚ñç       | 48/200 [00:10<00:27,  5.49it/s]\nPredicting:  24%|‚ñà‚ñà‚ñç       | 49/200 [00:10<00:24,  6.06it/s]\nPredicting:  25%|‚ñà‚ñà‚ñå       | 50/200 [00:10<00:25,  5.93it/s]\nPredicting:  26%|‚ñà‚ñà‚ñå       | 51/200 [00:11<00:26,  5.69it/s]\nPredicting:  26%|‚ñà‚ñà‚ñå       | 52/200 [00:11<00:32,  4.54it/s]\nPredicting:  26%|‚ñà‚ñà‚ñã       | 53/200 [00:11<00:34,  4.25it/s]\nPredicting:  27%|‚ñà‚ñà‚ñã       | 54/200 [00:12<00:37,  3.85it/s]\nPredicting:  28%|‚ñà‚ñà‚ñä       | 55/200 [00:12<00:40,  3.62it/s]\nPredicting:  28%|‚ñà‚ñà‚ñä       | 56/200 [00:12<00:37,  3.81it/s]\nPredicting:  28%|‚ñà‚ñà‚ñä       | 57/200 [00:12<00:31,  4.51it/s]\nPredicting:  29%|‚ñà‚ñà‚ñâ       | 58/200 [00:13<00:42,  3.35it/s]\nPredicting:  29%|‚ñà‚ñà‚ñâ       | 58/200 [00:13<00:32,  4.38it/s]\nTraceback (most recent call last):\n  File \"/workspaces/ISE-challenge/Test/code_runner_7l98gqas/project/main.py\", line 37, in <module>\n    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=True)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspaces/ISE-challenge/Test/code_runner_7l98gqas/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2867, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspaces/ISE-challenge/Test/code_runner_7l98gqas/venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py\", line 2927, in _call_one\n    raise ValueError(\nValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import venv\n",
    "from pathlib import Path\n",
    "\n",
    "def code_execute_agent(\n",
    "    input_path,\n",
    "    output_path,\n",
    "    code: str,\n",
    "    requirements=None,\n",
    "    timeout: int = 120000\n",
    "):\n",
    "    requirements = requirements or []\n",
    "\n",
    "    input_path = Path(input_path)\n",
    "    output_path = Path(output_path)\n",
    "\n",
    "    temp_dir = Path(tempfile.mkdtemp(prefix=\"code_runner_\", dir=Path.cwd()))\n",
    "    venv_dir = temp_dir / \"venv\"\n",
    "    project_dir = temp_dir / \"project\"\n",
    "\n",
    "    try:\n",
    "        # Step 1: Create project structure\n",
    "        os.makedirs(project_dir, exist_ok=True)\n",
    "\n",
    "        # Step 2: Copy input to temp folder\n",
    "        # if input_path.exists():\n",
    "        #     if input_path.is_dir():\n",
    "        #         shutil.copytree(input_path, project_dir / \"input\", dirs_exist_ok=True)\n",
    "        #     else:\n",
    "        #         shutil.copy2(input_path, project_dir / input_path.name)\n",
    "\n",
    "        if input_path.exists():\n",
    "            if input_path.is_dir():\n",
    "                dest = project_dir / input_path.name\n",
    "                shutil.copytree(input_path, dest, dirs_exist_ok=True)\n",
    "            else:\n",
    "                shutil.copy2(input_path, project_dir / input_path.name)\n",
    "\n",
    "        # Step 3: Write code into main.py\n",
    "        main_file = project_dir / \"main.py\"\n",
    "        main_file.write_text(code)\n",
    "\n",
    "        # Step 4: Setup virtual environment\n",
    "        venv.create(venv_dir, with_pip=True)\n",
    "        pip_path = venv_dir / \"bin\" / \"pip\"\n",
    "        subprocess.check_call([str(pip_path), \"install\", *requirements])\n",
    "\n",
    "        # # Step 5: Execute the code\n",
    "        # python_path = venv_dir / \"bin\" / \"python\"\n",
    "        # result = subprocess.run(\n",
    "        #     [str(python_path), \"main.py\"],\n",
    "        #     cwd=project_dir,\n",
    "        #     stdout=subprocess.PIPE,\n",
    "        #     stderr=subprocess.STDOUT,\n",
    "        #     capture_output=True,\n",
    "        #     text=True,\n",
    "        #     timeout=timeout\n",
    "        # )\n",
    "\n",
    "        # if result.returncode != 0:\n",
    "        #     raise RuntimeError(f\"[Error]\\n{result.stderr}\")\n",
    "\n",
    "        # Step 5: Execute the code with realtime logging\n",
    "        python_path = venv_dir / \"bin\" / \"python\"\n",
    "        process = subprocess.Popen(\n",
    "            [str(python_path), \"main.py\"],\n",
    "            cwd=project_dir,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.STDOUT,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        log_lines = []\n",
    "        try:\n",
    "            for line in process.stdout:\n",
    "                print(line, end=\"\")              # Realtime print\n",
    "                log_lines.append(line)           # Collect for return\n",
    "            process.wait(timeout=timeout)\n",
    "        except subprocess.TimeoutExpired:\n",
    "            process.kill()\n",
    "            raise RuntimeError(\"Code execution timed out.\")\n",
    "\n",
    "        if process.returncode != 0:\n",
    "            raise RuntimeError(f\"Execution failed:\\n{''.join(log_lines)}\")\n",
    "\n",
    "        # Step 6: Copy specific output file to output_path\n",
    "        produced_file = project_dir / output_path.name\n",
    "        if not produced_file.exists():\n",
    "            raise FileNotFoundError(f\"Expected output file not found: {produced_file}\")\n",
    "\n",
    "        # Ensure parent folder for output_path exists\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(produced_file, output_path)\n",
    "\n",
    "        return ''.join(log_lines)\n",
    "\n",
    "    finally:\n",
    "        # Step 7: Cleanup temp folder\n",
    "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "\n",
    "\n",
    "code_execute_agent(\n",
    "    \"test.csv\",\n",
    "    \"predictions.csv\",\n",
    "    coding_state[\"code\"].replace(\"```python\", \"\").replace(\"```\", \"\"),\n",
    "    [\"torch\", \"huggingface\", \"huggingface_hub\", \"transformers\" , \"tqdm\", \"pandas\", \"numpy\", \"safetensors\", \"tf-keras\", \"pillow\", \"tensorflow\"],\n",
    "    1200000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd531b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
