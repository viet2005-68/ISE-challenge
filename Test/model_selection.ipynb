{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d3e60a",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "535f3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY='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'\n",
    "FIRECRAWL_API_KEY='fc-e36282d62d1c4ee8971ec0649f3862c7'\n",
    "BASE_URL='https://api.groq.com/openai/v1'\n",
    "MODEL=\"meta-llama/llama-4-scout-17b-16e-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42744f82",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "91bed7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python312.zip', '/usr/lib/python3.12', '/usr/lib/python3.12/lib-dynload', '', '/mnt/g/Ai agents course/LangGraph/venv/lib/python3.12/site-packages', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..', '..']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "print(sys.path)\n",
    "from tools.fetching_description_from_huggingface import fetching_description_from_huggingface\n",
    "from typing import TypedDict, Annotated, List\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import Tool\n",
    "from firecrawl import FirecrawlApp\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "import pandas as pd\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36830718",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "5827b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task = pd.read_csv(\"/mnt/g/Ai agents course/LangGraph/Challenge2/data/ISE - AutoCode Challenge 2 Public - Public task.csv\")\n",
    "df_model = pd.read_csv(\"/mnt/g/Ai agents course/LangGraph/Challenge2/data/ISE - AutoCode Challenge 2 Public - Model zoo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "22abc01f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STT</th>\n",
       "      <th>Task</th>\n",
       "      <th>Validation data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt tay ...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nTrong th·ªùi ƒë·∫°i th√¥ng tin...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nƒê√¢y l√† m·ªôt nhi·ªám v·ª• h·ªìi ...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nƒêi·ªÅu quan tr·ªçng l√† c√°c c...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nTrong nhi·ªÅu ·ª©ng d·ª•ng th·ª±...</td>\n",
       "      <td>Link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   STT                                               Task Validation data\n",
       "0    1  \"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt tay ...            Link\n",
       "1    2  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nTrong th·ªùi ƒë·∫°i th√¥ng tin...            Link\n",
       "2    3  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nƒê√¢y l√† m·ªôt nhi·ªám v·ª• h·ªìi ...            Link\n",
       "3    4  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nƒêi·ªÅu quan tr·ªçng l√† c√°c c...            Link\n",
       "4    5  B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nTrong nhi·ªÅu ·ª©ng d·ª•ng th·ª±...            Link"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "697add1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = df_task[\"Task\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "953cae02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"B·ªëi c·∫£nh c·ªßa v·∫•n ƒë·ªÅ:\\nNh·∫≠n d·∫°ng ch·ªØ vi·∫øt tay l√† m·ªôt b√†i to√°n c∆° b·∫£n trong lƒ©nh v·ª±c h·ªçc m√°y v√† x·ª≠ l√Ω ·∫£nh, v·ªõi nhi·ªÅu ·ª©ng d·ª•ng th·ª±c ti·ªÖn nh∆∞ nh·∫≠n d·∫°ng ch·ªØ s·ªë tr√™n phi·∫øu kh·∫£o s√°t, h√≥a ƒë∆°n hay b√†i thi t·ª± ƒë·ªông. Vi·ªác ph√¢n lo·∫°i ch·ªØ s·ªë vi·∫øt tay th√†nh s·ªë nguy√™n t·ªë ho·∫∑c kh√¥ng gi√∫p m·ªü r·ªông kh·∫£ nƒÉng ·ª©ng d·ª•ng trong c√°c b√†i to√°n to√°n h·ªçc t·ª± ƒë·ªông, ki·ªÉm tra b√†i t·∫≠p, ho·∫∑c c√°c ·ª©ng d·ª•ng gi√°o d·ª•c.\\n\\nY√™u c·∫ßu c·ª• th·ªÉ c·∫ßn ƒë·∫°t ƒë∆∞·ª£c:\\nX√¢y d·ª±ng m·ªôt h·ªá th·ªëng c√≥ kh·∫£ nƒÉng nh·∫≠n di·ªán ch·ªØ s·ªë vi·∫øt tay t·ª´ ·∫£nh v√† x√°c ƒë·ªãnh xem s·ªë ƒë√≥ c√≥ ph·∫£i l√† s·ªë nguy√™n t·ªë hay kh√¥ng.\\nƒê·∫ßu v√†o l√† ·∫£nh ch·ª©a m·ªôt ch·ªØ s·ªë vi·∫øt tay.\\nƒê·∫ßu ra l√† nh√£n ph√¢n lo·∫°i nh·ªã ph√¢n: \"\"nguy√™n t·ªë\"\" ho·∫∑c \"\"kh√¥ng nguy√™n t·ªë\"\".\\n\\nƒê·ªãnh d·∫°ng d·ªØ li·ªáu ƒë·∫ßu v√†o cho b√†i to√°n t·ªïng th·ªÉ:\\nM·ªôt th∆∞ m·ª•c c√≥ t√™n l√† \"\"images\"\" ch·ª©a c√°c ·∫£nh grayscale 28x28 pixel, m·ªói ·∫£nh ch·ª©a m·ªôt ch·ªØ s·ªë vi·∫øt tay t·ª´ 0 ƒë·∫øn 9.\\n\\nƒê·ªãnh d·∫°ng k·∫øt qu·∫£ ƒë·∫ßu ra mong mu·ªën cho b√†i to√°n t·ªïng th·ªÉ:\\nFile output.csv m·ªói h√†ng l√† k·∫øt qu·∫£ d·ª± ƒëo√°n m·ªói ·∫£nh\\nc√≥ c√°c c·ªôt:\\nfile_name: t√™n file ·∫£nh\\nprediction: nh√£n c·ªßa ·∫£nh:\\n\"\"s·ªë nguy√™n t·ªë\"\" n·∫øu s·ªë trong ·∫£nh l√† s·ªë nguy√™n t·ªë.\\n\"\"kh√¥ng nguy√™n t·ªë\"\" n·∫øu s·ªë trong ·∫£nh kh√¥ng ph·∫£i s·ªë nguy√™n t·ªë.\"'"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "b6909b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_list(df):\n",
    "    model_strings = []\n",
    "    for i, row in df.iterrows():\n",
    "        desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        url = str(row[2])\n",
    "        model_strings.append(f\"{i + 1} - {desc} [More info]({url})\")\n",
    "    return \"\\n\".join(model_strings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c40886",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "af3bcff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraper(url: str) -> str:\n",
    "    \"\"\"Get more details about the model using the model url\"\"\"\n",
    "    # scraper = FirecrawlApp(api_key=FIRECRAWL_API_KEY)\n",
    "    # scrape_status = scraper.scrape_url(\n",
    "    #     url,\n",
    "    #     formats=['markdown']\n",
    "    # )\n",
    "    # return scrape_status.markdown\n",
    "    result = fetching_description_from_huggingface(url)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "77f3a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraperParameters(BaseModel):\n",
    "    url: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "b6a0e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = web_scraper('https://huggingface.co/thanhtlx/image_classification_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "52a9c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"web_scrapper\",\n",
    "        func=web_scraper,\n",
    "        description=\"Use to get more detail about the model\",\n",
    "        args_schema=WebScraperParameters\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c409f8b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "eca82430",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI( \n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "6511a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tasks(BaseModel):\n",
    "    subtask_one: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 1\"\n",
    "    )\n",
    "    subtask_two: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 2\"\n",
    "    )\n",
    "    subtask_three: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 3\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "0de6f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: HumanMessage | None\n",
    "    data: str | None\n",
    "    messages: Annotated[BaseMessage, add_messages]\n",
    "    subtasks: Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "ef593964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_agent(state: AgentState) -> AgentState:\n",
    "    structured_llm = llm.with_structured_output(Tasks)\n",
    "    system_prompt = \"\"\"\n",
    "                    You are an expert in task formulation. Given a machine learning task, you will\n",
    "                    help divide the task into these subtasks:\n",
    "                    - Subtask1: for classifying the problem type and task description\n",
    "                    - Subtask2: for identifying the input of the problem\n",
    "                    - Subtask3: for identifying the output of the problem\n",
    "                    describe the each subtask as specifically as possible\n",
    "                    \"\"\"\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    messages.append(state['task'])\n",
    "    result = structured_llm.invoke(messages)\n",
    "    state['subtasks'] = result\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "14b15c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"task\": HumanMessage(content=task),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "78675acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = classify_agent(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab3d7f",
   "metadata": {},
   "source": [
    "# Modeling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "f656ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInfo(BaseModel):\n",
    "    model: str = Field(..., description=\"Model name\")\n",
    "    url: str = Field(..., description=\"URL to the model\")\n",
    "\n",
    "class ModelSelection(BaseModel):\n",
    "    models: List[ModelInfo] = Field(..., description=\"List of available models with name and URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "18dcc46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selection_parser = PydanticOutputParser(pydantic_object=ModelSelection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "de4ed792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the suitable model for a given task.\n",
    "                    Given:\n",
    "                    - A user description about the task\n",
    "                    - A list of available model (names and links)\n",
    "                    Your job is to choose ALL suitable models provided in the list for user's specific tasks.\n",
    "                    Return the answer in the format\n",
    "                    {structured_output}\n",
    "                    Here are the list of model:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"user\", \"{input}\")\n",
    "    ]).partial(structured_output=model_selection_parser.get_format_instructions())\n",
    "\n",
    "    chain = prompt | llm | model_selection_parser\n",
    "\n",
    "    result = chain.invoke({\"input\": task, \"model_list\": format_model_list(df_model)})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "fd097a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27136/7332665.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
      "/tmp/ipykernel_27136/7332665.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  url = str(row[2])\n"
     ]
    }
   ],
   "source": [
    "res = model_selection_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "03a8819c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ModelInfo(model='Nh·∫≠n di·ªán ·∫£nh ch·ªØ vi·∫øt tay', url='https://huggingface.co/thanhtlx/image_classification_01')]\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "3b0dc638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_selection_using_tool_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a machine learning expert assigned to select the **Best-fit model** for a given task.\n",
    "                    Given:\n",
    "                    - A user task description\n",
    "                    - A list of available models (names and url)\n",
    "                    - A tool that fetches model details from a provided URL (it help gathers model description, input/output format, code sample usage)\n",
    "\n",
    "                    Your job is to:\n",
    "                    1. Use the **provided tool** to retrieve real details about each candidate model:\n",
    "                        - ‚úÖ Model description\n",
    "                        - üì• Input format\n",
    "                        - üì§ Output format\n",
    "                        - üõ†Ô∏è Library requirements\n",
    "                        - üß™ Code sample (usage code snippet)\n",
    "                    2. Select the best model for user given task based on the data that the tool gives you.\n",
    "\n",
    "                    ‚ö†Ô∏è VERY IMPORTANT RULES\n",
    "\n",
    "                    - ‚ùå DO NOT assume or invent any part of the model's description, input/output format, or code\n",
    "                    - ‚ùå DO NOT generate fake code or use your own knowledge about the model\n",
    "                    - ‚úÖ ONLY use the **actual output** returned from the tool\n",
    "                    - ‚úÖ Include tool content in your final answer exactly as returned (especially code)\n",
    "\n",
    "                    ## üß† Output Structure (Final Answer)\n",
    "\n",
    "                    After retrieving tool results, choose ONE best-fit model and output the result in the following format:\n",
    "\n",
    "                    **‚úÖ Model name and link**  \n",
    "                    `<model name>` ‚Äî `<link>`\n",
    "\n",
    "                    **üìù Description (from tool):**  \n",
    "                    <model description>\n",
    "\n",
    "                    **üì• Input format (from tool):**  \n",
    "                    <description of expected input>\n",
    "\n",
    "                    **üì§ Output format (from tool):**  \n",
    "                    <description of model output>\n",
    "\n",
    "                    **üõ†Ô∏è Library Requirements (from tool) **\n",
    "                    <requirements to use the model>\n",
    "\n",
    "                    **üß™ Example code (from tool):**  \n",
    "                    ```python\n",
    "                    <exact code snippet from tool>\n",
    "\n",
    "                    Here are the list of models and there corresponding URL:\n",
    "                    {model_list}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"ai\", \"{agent_scratchpad}\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "    task_input = state[\"task\"].content if isinstance(state[\"task\"], SystemMessage) else state[\"task\"]\n",
    "\n",
    "    # result = executor.invoke({\"input\": task_input})\n",
    "    result = executor.invoke({\"input\": task_input, \"model_list\": res.models})\n",
    "\n",
    "    return {\"output\": result[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "e8b0f513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/thanhtlx/image_classification_01'}`\n",
      "responded: I will use the provided tool to gather information on this model. Let's start.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "Image Classification Model - Handwritten Digit Recognition (Color Images)\n",
      "üìù M√¥ t·∫£\n",
      "ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c ch·ªØ s·ªë vi·∫øt tay (t·ª´ 0 ƒë·∫øn 9) trong ·∫£nh m√†u. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c Vision Transformer (ViT) v√† ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay ƒë∆∞·ª£c ch·ª•p ho·∫∑c scan d∆∞·ªõi d·∫°ng ·∫£nh m√†u.\n",
      "üìå Chi ti·∫øt c√°c m√¥ h√¨nh\n",
      "Ph√¢n lo·∫°i ·∫£nh ch·ªØ s·ªë vi·∫øt tay t·ª´ 0 ƒë·∫øn 9\n",
      "üì• ƒê·∫ßu v√†o\n",
      "ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u (RGB)\n",
      "Ki·ªÉu d·ªØ li·ªáu: PIL.Image.Image\n",
      "K√≠ch th∆∞·ªõc ·∫£nh: M√¥ h√¨nh s·∫Ω t·ª± resize theo c·∫•u h√¨nh c·ªßa ViTImageProcessor (th∆∞·ªùng l√† 224x224)\n",
      "Gi√° tr·ªã pixel: [0, 255] (chu·∫©n h√≥a t·ª± ƒë·ªông b·ªüi processor)\n",
      "üì§ ƒê·∫ßu ra\n",
      "M·ªôt s·ªë nguy√™n t·ª´ 0 ƒë·∫øn 9, l√† k·∫øt qu·∫£ ph√¢n lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi ch·ªØ s·ªë xu·∫•t hi·ªán trong ·∫£nh.\n",
      "üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng pip:\n",
      "pip install transformers torch\n",
      "üß™ S·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n m√£ m·∫´u ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh:\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "import torch\n",
      "model_name = \"thanhtlx/image_classification_01\"\n",
      "model = ViTForImageClassification.from_pretrained(model_name)\n",
      "processor = ViTImageProcessor.from_pretrained(model_name)\n",
      "# ƒê·ªçc ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"images.png\")\n",
      "# X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "# L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
      "predicted_class = outputs.logits.argmax(-1).item()\n",
      "print(f\"prediction: {predicted_class}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/thanhtlx/image_classification_01'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "Image Classification Model - Handwritten Digit Recognition (Color Images)\n",
      "üìù M√¥ t·∫£\n",
      "ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c ch·ªØ s·ªë vi·∫øt tay (t·ª´ 0 ƒë·∫øn 9) trong ·∫£nh m√†u. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c Vision Transformer (ViT) v√† ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay ƒë∆∞·ª£c ch·ª•p ho·∫∑c scan d∆∞·ªõi d·∫°ng ·∫£nh m√†u.\n",
      "üìå Chi ti·∫øt c√°c m√¥ h√¨nh\n",
      "Ph√¢n lo·∫°i ·∫£nh ch·ªØ s·ªë vi·∫øt tay t·ª´ 0 ƒë·∫øn 9\n",
      "üì• ƒê·∫ßu v√†o\n",
      "ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u (RGB)\n",
      "Ki·ªÉu d·ªØ li·ªáu: PIL.Image.Image\n",
      "K√≠ch th∆∞·ªõc ·∫£nh: M√¥ h√¨nh s·∫Ω t·ª± resize theo c·∫•u h√¨nh c·ªßa ViTImageProcessor (th∆∞·ªùng l√† 224x224)\n",
      "Gi√° tr·ªã pixel: [0, 255] (chu·∫©n h√≥a t·ª± ƒë·ªông b·ªüi processor)\n",
      "üì§ ƒê·∫ßu ra\n",
      "M·ªôt s·ªë nguy√™n t·ª´ 0 ƒë·∫øn 9, l√† k·∫øt qu·∫£ ph√¢n lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi ch·ªØ s·ªë xu·∫•t hi·ªán trong ·∫£nh.\n",
      "üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng pip:\n",
      "pip install transformers torch\n",
      "üß™ S·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n m√£ m·∫´u ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh:\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "import torch\n",
      "model_name = \"thanhtlx/image_classification_01\"\n",
      "model = ViTForImageClassification.from_pretrained(model_name)\n",
      "processor = ViTImageProcessor.from_pretrained(model_name)\n",
      "# ƒê·ªçc ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"images.png\")\n",
      "# X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "# L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
      "predicted_class = outputs.logits.argmax(-1).item()\n",
      "print(f\"prediction: {predicted_class}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/thanhtlx/image_classification_01'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m\n",
      "Image Classification Model - Handwritten Digit Recognition (Color Images)\n",
      "üìù M√¥ t·∫£\n",
      "ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c ch·ªØ s·ªë vi·∫øt tay (t·ª´ 0 ƒë·∫øn 9) trong ·∫£nh m√†u. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c Vision Transformer (ViT) v√† ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay ƒë∆∞·ª£c ch·ª•p ho·∫∑c scan d∆∞·ªõi d·∫°ng ·∫£nh m√†u.\n",
      "üìå Chi ti·∫øt c√°c m√¥ h√¨nh\n",
      "Ph√¢n lo·∫°i ·∫£nh ch·ªØ s·ªë vi·∫øt tay t·ª´ 0 ƒë·∫øn 9\n",
      "üì• ƒê·∫ßu v√†o\n",
      "ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u (RGB)\n",
      "Ki·ªÉu d·ªØ li·ªáu: PIL.Image.Image\n",
      "K√≠ch th∆∞·ªõc ·∫£nh: M√¥ h√¨nh s·∫Ω t·ª± resize theo c·∫•u h√¨nh c·ªßa ViTImageProcessor (th∆∞·ªùng l√† 224x224)\n",
      "Gi√° tr·ªã pixel: [0, 255] (chu·∫©n h√≥a t·ª± ƒë·ªông b·ªüi processor)\n",
      "üì§ ƒê·∫ßu ra\n",
      "M·ªôt s·ªë nguy√™n t·ª´ 0 ƒë·∫øn 9, l√† k·∫øt qu·∫£ ph√¢n lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi ch·ªØ s·ªë xu·∫•t hi·ªán trong ·∫£nh.\n",
      "üõ† Y√™u c·∫ßu th∆∞ vi·ªán\n",
      "C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng pip:\n",
      "pip install transformers torch\n",
      "üß™ S·ª≠ d·ª•ng m√¥ h√¨nh\n",
      "D∆∞·ªõi ƒë√¢y l√† ƒëo·∫°n m√£ m·∫´u ƒë·ªÉ s·ª≠ d·ª•ng m√¥ h√¨nh:\n",
      "from transformers import ViTForImageClassification, ViTImageProcessor\n",
      "from PIL import Image\n",
      "import torch\n",
      "model_name = \"thanhtlx/image_classification_01\"\n",
      "model = ViTForImageClassification.from_pretrained(model_name)\n",
      "processor = ViTImageProcessor.from_pretrained(model_name)\n",
      "# ƒê·ªçc ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      "image = Image.open(\"images.png\")\n",
      "# X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\n",
      "inputs = processor(images=image, return_tensors=\"pt\")\n",
      "# D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      "with torch.no_grad():\n",
      "    outputs = model(**inputs)\n",
      "# L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
      "predicted_class = outputs.logits.argmax(-1).item()\n",
      "print(f\"prediction: {predicted_class}\")\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mThe model provided is for handwritten digit recognition, which can classify handwritten digits from 0 to 9. However, the task requires not only recognizing the digit but also determining if the recognized digit is a prime number or not.\n",
      "\n",
      "The model's output is a single digit from 0 to 9. To determine if this digit is prime, we can use a simple mathematical function to check for primality.\n",
      "\n",
      "Given the model's capabilities and the task's requirements, we can use the model as is for digit recognition and then apply a primality test on the recognized digit.\n",
      "\n",
      "Here is the final answer:\n",
      "\n",
      " **‚úÖ Model name and link**  \n",
      " `Nh·∫≠n di·ªán ·∫£nh ch·ªØ vi·∫øt tay` ‚Äî `https://huggingface.co/thanhtlx/image_classification_01`\n",
      "\n",
      " **üìù Description (from tool):**  \n",
      " ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c ch·ªØ s·ªë vi·∫øt tay (t·ª´0 ƒë·∫øn9) trong ·∫£nh m√†u. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c Vision Transformer (ViT) v√† ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay ƒë∆∞·ª£c ch·ª•p ho·∫∑c scan d∆∞·ªõi d·∫°ng ·∫£nh m√†u.\n",
      "\n",
      " **üì• Input format (from tool):**  \n",
      " ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u (RGB)\n",
      " Ki·ªÉu d·ªØ li·ªáu: PIL.Image.Image\n",
      " K√≠ch th∆∞·ªõc ·∫£nh: M√¥ h√¨nh s·∫Ω t·ª± resize theo c·∫•u h√¨nh c·ªßa ViTImageProcessor (th∆∞·ªùng l√†224x224)\n",
      " Gi√° tr·ªã pixel: [0,255] (chu·∫©n h√≥a t·ª± ƒë·ªông b·ªüi processor)\n",
      "\n",
      " **üì§ Output format (from tool):**  \n",
      " M·ªôt s·ªë nguy√™n t·ª´0 ƒë·∫øn9, l√† k·∫øt qu·∫£ ph√¢n lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi ch·ªØ s·ªë xu·∫•t hi·ªán trong ·∫£nh.\n",
      "\n",
      " **üõ†Ô∏è Library Requirements (from tool) **\n",
      " C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng pip:\n",
      " pip install transformers torch\n",
      "\n",
      " **üß™ Example code (from tool):**  \n",
      " ```python\n",
      " from transformers import ViTForImageClassification, ViTImageProcessor\n",
      " from PIL import Image\n",
      " import torch\n",
      " model_name = \"thanhtlx/image_classification_01\"\n",
      " model = ViTForImageClassification.from_pretrained(model_name)\n",
      " processor = ViTImageProcessor.from_pretrained(model_name)\n",
      " # ƒê·ªçc ·∫£nh c·∫ßn ph√¢n lo·∫°i\n",
      " image = Image.open(\"images.png\")\n",
      " # X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\n",
      " inputs = processor(images=image, return_tensors=\"pt\")\n",
      " # D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\n",
      " with torch.no_grad():\n",
      " outputs = model(**inputs)\n",
      " # L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
      " predicted_class = outputs.logits.argmax(-1).item()\n",
      " print(f\"prediction: {predicted_class}\")\n",
      " ```\n",
      "\n",
      "To complete the task, we would need to:\n",
      "\n",
      "1. Use the model to recognize the handwritten digit.\n",
      "2. Write a function to check if the recognized digit is a prime number.\n",
      "\n",
      "Here is a simple function to check for primality:\n",
      "\n",
      "```python\n",
      "def is_prime(n):\n",
      " if n <= 1:\n",
      " return False\n",
      " if n == 2:\n",
      " return True\n",
      " if n % 2 == 0:\n",
      " return False\n",
      " max_divisor = int(n**0.5) + 1\n",
      " for d in range(3, max_divisor, 2):\n",
      " if n % d == 0:\n",
      " return False\n",
      " return True\n",
      "```\n",
      "\n",
      "We can then use this function to classify the output of the model as \"nguy√™n t·ªë\" (prime) or \"kh√¥ng nguy√™n t·ªë\" (not prime).\n",
      "\n",
      "```python\n",
      "def classify_as_prime(digit):\n",
      " if is_prime(digit):\n",
      " return \"nguy√™n t·ªë\"\n",
      " else:\n",
      " return \"kh√¥ng nguy√™n t·ªë\"\n",
      "\n",
      "# Assuming 'predicted_class' is the output from the model\n",
      "print(classify_as_prime(predicted_class))\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res2 = model_selection_using_tool_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "cb681a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The model provided is for handwritten digit recognition, which can classify '\n",
      " 'handwritten digits from 0 to 9. However, the task requires not only '\n",
      " 'recognizing the digit but also determining if the recognized digit is a '\n",
      " 'prime number or not.\\n'\n",
      " '\\n'\n",
      " \"The model's output is a single digit from 0 to 9. To determine if this digit \"\n",
      " 'is prime, we can use a simple mathematical function to check for primality.\\n'\n",
      " '\\n'\n",
      " \"Given the model's capabilities and the task's requirements, we can use the \"\n",
      " 'model as is for digit recognition and then apply a primality test on the '\n",
      " 'recognized digit.\\n'\n",
      " '\\n'\n",
      " 'Here is the final answer:\\n'\n",
      " '\\n'\n",
      " ' **‚úÖ Model name and link**  \\n'\n",
      " ' `Nh·∫≠n di·ªán ·∫£nh ch·ªØ vi·∫øt tay` ‚Äî '\n",
      " '`https://huggingface.co/thanhtlx/image_classification_01`\\n'\n",
      " '\\n'\n",
      " ' **üìù Description (from tool):**  \\n'\n",
      " ' ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c ch·ªØ s·ªë '\n",
      " 'vi·∫øt tay (t·ª´0 ƒë·∫øn9) trong ·∫£nh m√†u. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c Vision '\n",
      " 'Transformer (ViT) v√† ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay '\n",
      " 'ƒë∆∞·ª£c ch·ª•p ho·∫∑c scan d∆∞·ªõi d·∫°ng ·∫£nh m√†u.\\n'\n",
      " '\\n'\n",
      " ' **üì• Input format (from tool):**  \\n'\n",
      " ' ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u (RGB)\\n'\n",
      " ' Ki·ªÉu d·ªØ li·ªáu: PIL.Image.Image\\n'\n",
      " ' K√≠ch th∆∞·ªõc ·∫£nh: M√¥ h√¨nh s·∫Ω t·ª± resize theo c·∫•u h√¨nh c·ªßa ViTImageProcessor '\n",
      " '(th∆∞·ªùng l√†224x224)\\n'\n",
      " ' Gi√° tr·ªã pixel: [0,255] (chu·∫©n h√≥a t·ª± ƒë·ªông b·ªüi processor)\\n'\n",
      " '\\n'\n",
      " ' **üì§ Output format (from tool):**  \\n'\n",
      " ' M·ªôt s·ªë nguy√™n t·ª´0 ƒë·∫øn9, l√† k·∫øt qu·∫£ ph√¢n lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi ch·ªØ s·ªë xu·∫•t hi·ªán '\n",
      " 'trong ·∫£nh.\\n'\n",
      " '\\n'\n",
      " ' **üõ†Ô∏è Library Requirements (from tool) **\\n'\n",
      " ' C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng pip:\\n'\n",
      " ' pip install transformers torch\\n'\n",
      " '\\n'\n",
      " ' **üß™ Example code (from tool):**  \\n'\n",
      " ' ```python\\n'\n",
      " ' from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " ' from PIL import Image\\n'\n",
      " ' import torch\\n'\n",
      " ' model_name = \"thanhtlx/image_classification_01\"\\n'\n",
      " ' model = ViTForImageClassification.from_pretrained(model_name)\\n'\n",
      " ' processor = ViTImageProcessor.from_pretrained(model_name)\\n'\n",
      " ' # ƒê·ªçc ·∫£nh c·∫ßn ph√¢n lo·∫°i\\n'\n",
      " ' image = Image.open(\"images.png\")\\n'\n",
      " ' # X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\\n'\n",
      " ' inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " ' # D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\\n'\n",
      " ' with torch.no_grad():\\n'\n",
      " ' outputs = model(**inputs)\\n'\n",
      " ' # L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n\\n'\n",
      " ' predicted_class = outputs.logits.argmax(-1).item()\\n'\n",
      " ' print(f\"prediction: {predicted_class}\")\\n'\n",
      " ' ```\\n'\n",
      " '\\n'\n",
      " 'To complete the task, we would need to:\\n'\n",
      " '\\n'\n",
      " '1. Use the model to recognize the handwritten digit.\\n'\n",
      " '2. Write a function to check if the recognized digit is a prime number.\\n'\n",
      " '\\n'\n",
      " 'Here is a simple function to check for primality:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'def is_prime(n):\\n'\n",
      " ' if n <= 1:\\n'\n",
      " ' return False\\n'\n",
      " ' if n == 2:\\n'\n",
      " ' return True\\n'\n",
      " ' if n % 2 == 0:\\n'\n",
      " ' return False\\n'\n",
      " ' max_divisor = int(n**0.5) + 1\\n'\n",
      " ' for d in range(3, max_divisor, 2):\\n'\n",
      " ' if n % d == 0:\\n'\n",
      " ' return False\\n'\n",
      " ' return True\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'We can then use this function to classify the output of the model as \"nguy√™n '\n",
      " 't·ªë\" (prime) or \"kh√¥ng nguy√™n t·ªë\" (not prime).\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'def classify_as_prime(digit):\\n'\n",
      " ' if is_prime(digit):\\n'\n",
      " ' return \"nguy√™n t·ªë\"\\n'\n",
      " ' else:\\n'\n",
      " ' return \"kh√¥ng nguy√™n t·ªë\"\\n'\n",
      " '\\n'\n",
      " \"# Assuming 'predicted_class' is the output from the model\\n\"\n",
      " 'print(classify_as_prime(predicted_class))\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res2['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2874a1c7",
   "metadata": {},
   "source": [
    "## Output Parser Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "3658cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelingOutput(BaseModel):\n",
    "    model_description: str = Field(\n",
    "        ...,\n",
    "        description=\"The detailed description of the model.\"\n",
    "    )\n",
    "    model_input_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed input format\"\n",
    "    )\n",
    "    model_output_format: str = Field(\n",
    "        ...,\n",
    "        description=\"Model's detailed output format\"\n",
    "    )\n",
    "    model_requirements: str = Field(\n",
    "        ...,\n",
    "        description=\"Requirement libraries need to install to use the model.\"\n",
    "    )\n",
    "    model_sample_code: str = Field(\n",
    "        ...,\n",
    "        description=\"Example code to use the model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "0b610e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=ModelingOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "707428ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser_llm = ChatOpenAI(\n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "5a1f51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_parser_agent(state: AgentState):\n",
    "    system_prompt = \"\"\"\n",
    "                    You are a smart AI tasked with extracting structured technical details about a machine learning model from a reasoning result.\n",
    "                    You are given a detailed text description about a model.\n",
    "                    Your goal is to fill the following fields based on the text:\n",
    "\n",
    "                    - `model_description`: A detailed explanation of what the model is and what it does.\n",
    "                    - `model_input_format`: A detailed description of the model's input format, including dimensions, data types, and expected preprocessing if mentioned.\n",
    "                    - `model_output_format`: A detailed description of the output format including dimensions, data types, label name and its meaning.\n",
    "                    - `model_requirements`: A detailed description about the requirements needed to be sastified in order to use the model\n",
    "                    - `model_sample_code`: A sample code on how to use the model.\n",
    "\n",
    "                    You MUSTN'T create any data on your own, only using the data provided in the text.\n",
    "\n",
    "                    Return the data as a JSON object matching the following structure:\n",
    "                    {formatted_output}\n",
    "                    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        ('system', system_prompt),\n",
    "        ('human', \"{input}\")\n",
    "    ]).partial(formatted_output=parser.get_format_instructions())\n",
    "    chain = prompt | llm | parser\n",
    "    response = chain.invoke({\"input\": res2['output']})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "0d82f96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = output_parser_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "583a7bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The model provided is for handwritten digit recognition, which can classify '\n",
      " 'handwritten digits from 0 to 9. However, the task requires not only '\n",
      " 'recognizing the digit but also determining if the recognized digit is a '\n",
      " 'prime number or not.\\n'\n",
      " '\\n'\n",
      " \"The model's output is a single digit from 0 to 9. To determine if this digit \"\n",
      " 'is prime, we can use a simple mathematical function to check for primality.\\n'\n",
      " '\\n'\n",
      " \"Given the model's capabilities and the task's requirements, we can use the \"\n",
      " 'model as is for digit recognition and then apply a primality test on the '\n",
      " 'recognized digit.\\n'\n",
      " '\\n'\n",
      " 'Here is the final answer:\\n'\n",
      " '\\n'\n",
      " ' **‚úÖ Model name and link**  \\n'\n",
      " ' `Nh·∫≠n di·ªán ·∫£nh ch·ªØ vi·∫øt tay` ‚Äî '\n",
      " '`https://huggingface.co/thanhtlx/image_classification_01`\\n'\n",
      " '\\n'\n",
      " ' **üìù Description (from tool):**  \\n'\n",
      " ' ƒê√¢y l√† m·ªôt m√¥ h√¨nh ph√¢n lo·∫°i ·∫£nh ƒë∆∞·ª£c hu·∫•n luy·ªán ƒë·ªÉ nh·∫≠n di·ªán c√°c ch·ªØ s·ªë '\n",
      " 'vi·∫øt tay (t·ª´0 ƒë·∫øn9) trong ·∫£nh m√†u. M√¥ h√¨nh s·ª≠ d·ª•ng ki·∫øn tr√∫c Vision '\n",
      " 'Transformer (ViT) v√† ƒë∆∞·ª£c hu·∫•n luy·ªán v·ªõi t·∫≠p d·ªØ li·ªáu g·ªìm c√°c ch·ªØ s·ªë vi·∫øt tay '\n",
      " 'ƒë∆∞·ª£c ch·ª•p ho·∫∑c scan d∆∞·ªõi d·∫°ng ·∫£nh m√†u.\\n'\n",
      " '\\n'\n",
      " ' **üì• Input format (from tool):**  \\n'\n",
      " ' ƒê·ªãnh d·∫°ng: ·∫¢nh m√†u (RGB)\\n'\n",
      " ' Ki·ªÉu d·ªØ li·ªáu: PIL.Image.Image\\n'\n",
      " ' K√≠ch th∆∞·ªõc ·∫£nh: M√¥ h√¨nh s·∫Ω t·ª± resize theo c·∫•u h√¨nh c·ªßa ViTImageProcessor '\n",
      " '(th∆∞·ªùng l√†224x224)\\n'\n",
      " ' Gi√° tr·ªã pixel: [0,255] (chu·∫©n h√≥a t·ª± ƒë·ªông b·ªüi processor)\\n'\n",
      " '\\n'\n",
      " ' **üì§ Output format (from tool):**  \\n'\n",
      " ' M·ªôt s·ªë nguy√™n t·ª´0 ƒë·∫øn9, l√† k·∫øt qu·∫£ ph√¢n lo·∫°i t∆∞∆°ng ·ª©ng v·ªõi ch·ªØ s·ªë xu·∫•t hi·ªán '\n",
      " 'trong ·∫£nh.\\n'\n",
      " '\\n'\n",
      " ' **üõ†Ô∏è Library Requirements (from tool) **\\n'\n",
      " ' C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán sau b·∫±ng pip:\\n'\n",
      " ' pip install transformers torch\\n'\n",
      " '\\n'\n",
      " ' **üß™ Example code (from tool):**  \\n'\n",
      " ' ```python\\n'\n",
      " ' from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " ' from PIL import Image\\n'\n",
      " ' import torch\\n'\n",
      " ' model_name = \"thanhtlx/image_classification_01\"\\n'\n",
      " ' model = ViTForImageClassification.from_pretrained(model_name)\\n'\n",
      " ' processor = ViTImageProcessor.from_pretrained(model_name)\\n'\n",
      " ' # ƒê·ªçc ·∫£nh c·∫ßn ph√¢n lo·∫°i\\n'\n",
      " ' image = Image.open(\"images.png\")\\n'\n",
      " ' # X·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o\\n'\n",
      " ' inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " ' # D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh\\n'\n",
      " ' with torch.no_grad():\\n'\n",
      " ' outputs = model(**inputs)\\n'\n",
      " ' # L·∫•y k·∫øt qu·∫£ d·ª± ƒëo√°n\\n'\n",
      " ' predicted_class = outputs.logits.argmax(-1).item()\\n'\n",
      " ' print(f\"prediction: {predicted_class}\")\\n'\n",
      " ' ```\\n'\n",
      " '\\n'\n",
      " 'To complete the task, we would need to:\\n'\n",
      " '\\n'\n",
      " '1. Use the model to recognize the handwritten digit.\\n'\n",
      " '2. Write a function to check if the recognized digit is a prime number.\\n'\n",
      " '\\n'\n",
      " 'Here is a simple function to check for primality:\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'def is_prime(n):\\n'\n",
      " ' if n <= 1:\\n'\n",
      " ' return False\\n'\n",
      " ' if n == 2:\\n'\n",
      " ' return True\\n'\n",
      " ' if n % 2 == 0:\\n'\n",
      " ' return False\\n'\n",
      " ' max_divisor = int(n**0.5) + 1\\n'\n",
      " ' for d in range(3, max_divisor, 2):\\n'\n",
      " ' if n % d == 0:\\n'\n",
      " ' return False\\n'\n",
      " ' return True\\n'\n",
      " '```\\n'\n",
      " '\\n'\n",
      " 'We can then use this function to classify the output of the model as \"nguy√™n '\n",
      " 't·ªë\" (prime) or \"kh√¥ng nguy√™n t·ªë\" (not prime).\\n'\n",
      " '\\n'\n",
      " '```python\\n'\n",
      " 'def classify_as_prime(digit):\\n'\n",
      " ' if is_prime(digit):\\n'\n",
      " ' return \"nguy√™n t·ªë\"\\n'\n",
      " ' else:\\n'\n",
      " ' return \"kh√¥ng nguy√™n t·ªë\"\\n'\n",
      " '\\n'\n",
      " \"# Assuming 'predicted_class' is the output from the model\\n\"\n",
      " 'print(classify_as_prime(predicted_class))\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(res2['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "530f5c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This is a handwritten digit recognition model that can classify handwritten '\n",
      " 'digits from 0 to 9. The model uses a Vision Transformer (ViT) architecture '\n",
      " 'and was trained on a dataset of handwritten digits. The task requires not '\n",
      " 'only recognizing the digit but also determining if the recognized digit is a '\n",
      " 'prime number or not.')\n",
      "(\"The model's input format is a color image (RGB) with a PIL.Image.Image data \"\n",
      " 'type. The image size is automatically resized to 224x224 by the '\n",
      " 'ViTImageProcessor. The pixel values range from [0,255] and are normalized '\n",
      " 'automatically by the processor.')\n",
      "(\"The model's output format is a single integer from 0 to 9, representing the \"\n",
      " 'classified digit. To determine if the digit is prime, a simple mathematical '\n",
      " 'function can be applied to the output.')\n",
      "('The required libraries to use the model are transformers and torch, which '\n",
      " 'can be installed using pip: pip install transformers torch')\n",
      "('```python\\n'\n",
      " 'from transformers import ViTForImageClassification, ViTImageProcessor\\n'\n",
      " 'from PIL import Image\\n'\n",
      " 'import torch\\n'\n",
      " '\\n'\n",
      " 'model_name = \"thanhtlx/image_classification_01\"\\n'\n",
      " 'model = ViTForImageClassification.from_pretrained(model_name)\\n'\n",
      " 'processor = ViTImageProcessor.from_pretrained(model_name)\\n'\n",
      " '\\n'\n",
      " '# Read the image to classify\\n'\n",
      " 'image = Image.open(\"images.png\")\\n'\n",
      " '\\n'\n",
      " '# Process the input image\\n'\n",
      " 'inputs = processor(images=image, return_tensors=\"pt\")\\n'\n",
      " '\\n'\n",
      " '# Make a prediction with the model\\n'\n",
      " 'with torch.no_grad():\\n'\n",
      " '  outputs = model(**inputs)\\n'\n",
      " '\\n'\n",
      " '# Get the predicted class\\n'\n",
      " 'predicted_class = outputs.logits.argmax(-1).item()\\n'\n",
      " 'print(f\"prediction: {predicted_class}\")\\n'\n",
      " '\\n'\n",
      " '# Define a function to check for primality\\n'\n",
      " 'def is_prime(n):\\n'\n",
      " '  if n <= 1:\\n'\n",
      " '    return False\\n'\n",
      " '  if n == 2:\\n'\n",
      " '    return True\\n'\n",
      " '  if n % 2 == 0:\\n'\n",
      " '    return False\\n'\n",
      " '  max_divisor = int(n**0.5) + 1\\n'\n",
      " '  for d in range(3, max_divisor, 2):\\n'\n",
      " '    if n % d == 0:\\n'\n",
      " '      return False\\n'\n",
      " '  return True\\n'\n",
      " '\\n'\n",
      " '# Classify the output as prime or not prime\\n'\n",
      " 'def classify_as_prime(digit):\\n'\n",
      " '  if is_prime(digit):\\n'\n",
      " '    return \"nguy√™n t·ªë\"\\n'\n",
      " '  else:\\n'\n",
      " '    return \"kh√¥ng nguy√™n t·ªë\"\\n'\n",
      " '\\n'\n",
      " 'print(classify_as_prime(predicted_class))\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(ans.model_description)\n",
    "pprint.pprint(ans.model_input_format)\n",
    "pprint.pprint(ans.model_output_format)\n",
    "pprint.pprint(ans.model_requirements)\n",
    "pprint.pprint(ans.model_sample_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
