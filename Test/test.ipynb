{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42d3e60a",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "535f3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY='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'\n",
    "FIRECRAWL_API_KEY='fc-e36282d62d1c4ee8971ec0649f3862c7'\n",
    "BASE_URL='https://api.groq.com/openai/v1'\n",
    "MODEL=\"meta-llama/llama-4-scout-17b-16e-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42744f82",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "91bed7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import Tool\n",
    "from firecrawl import FirecrawlApp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36830718",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5827b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_task = pd.read_csv(\"/mnt/g/Ai agents course/LangGraph/Challenge2/data/ISE - AutoCode Challenge 2 Public - Public task.csv\")\n",
    "df_model = pd.read_csv(\"/mnt/g/Ai agents course/LangGraph/Challenge2/data/ISE - AutoCode Challenge 2 Public - Model zoo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697add1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = df_task[\"Task\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "953cae02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bối cảnh của vấn đề:\\nĐiều quan trọng là các công ty thẻ tín dụng có thể nhận ra các giao dịch thẻ tín dụng gian lận để khách hàng không phải trả tiền cho những mặt hàng mà họ không mua. Một vấn đề của task này là dữ liệu training khá là ít cho giao dịch gian lận gây vấn đề mất cân bằng nhãn dữ liệu.\\n\\nYêu cầu cụ thể cần đạt được:\\nXây dựng một hệ thống có khả năng nhận đầu vào là một list các features liên quan đến sự gian lận của giao dịch và đầu ra hệ thống là phân loại giao dịch cụ thể đó có gian lận hay không\\nMô hình cần xử lý đầu vào là 1 list các feature liên quan đến gian lận của giao dịch\\nĐầu ra là nhãn của giao dịch đó thuộc về\\n\\nĐịnh dạng dữ liệu đầu vào cho bài toán tổng thể:\\nMột file test.csv. Mỗi hàng là features cho từng task bao gồm các cột:\\n28 đặc trưng V1, V2, … V28 là các thành phần chính thu được từ PCA (Không được công bố cụ thể);  hai đặc trưng không được biến đổi bằng PCA là 'Time' và 'Amount'. Đặc trưng 'Time' thể hiện số giây đã trôi qua giữa mỗi giao dịch và giao dịch đầu tiên trong tập dữ liệu. Đặc trưng 'Amount' là số tiền của giao dịch, đặc trưng này có thể được sử dụng cho học tập nhạy cảm với chi phí phụ thuộc vào ví dụ và cuối cùng là ID: id của task\\n \\nĐịnh dạng kết quả đầu ra mong muốn cho bài toán tổng thể:\\nFile output.csv mỗi hàng là kết quả dự đoán mỗi task\\ncó các cột:\\nID: id của task\\nclass: 1 nếu là giao dịch gian lận, 0 với trường hợp ngược lại\\n\""
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b6909b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_model_list(df):\n",
    "    model_strings = []\n",
    "    for i, row in df.iterrows():\n",
    "        desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
    "        url = str(row[2])\n",
    "        model_strings.append(f\"{i + 1} - {desc} [More info]({url})\")\n",
    "    return \"\\n\".join(model_strings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c40886",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "af3bcff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def web_scraper(url: str) -> str:\n",
    "    \"\"\"Get more details about the model using the model url\"\"\"\n",
    "    scraper = FirecrawlApp(api_key=FIRECRAWL_API_KEY)\n",
    "    scrape_status = scraper.scrape_url(\n",
    "        url,\n",
    "        formats=['markdown']\n",
    "    )\n",
    "    return scrape_status.markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "77f3a676",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScraperParameters(BaseModel):\n",
    "    url: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b6a0e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = web_scraper('https://huggingface.co/thanhtlx/image_classification_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "52a9c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"web_scrapper\",\n",
    "        func=web_scraper,\n",
    "        description=\"Use to get more detail about the model\",\n",
    "        args_schema=WebScraperParameters\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c409f8b",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "eca82430",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI( \n",
    "    base_url=BASE_URL,\n",
    "    model=MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6511a16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tasks(BaseModel):\n",
    "    subtask_one: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 1\"\n",
    "    )\n",
    "    subtask_two: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 2\"\n",
    "    )\n",
    "    subtask_three: str = Field(\n",
    "        ...,\n",
    "        description=\"Detailed description of task 3\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0de6f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    task: HumanMessage | None\n",
    "    data: str | None\n",
    "    messages: Annotated[BaseMessage, add_messages]\n",
    "    subtasks: Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ef593964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_agent(state: AgentState) -> AgentState:\n",
    "    structured_llm = llm.with_structured_output(Tasks)\n",
    "    system_prompt = \"\"\"\n",
    "                    You are an expert in task formulation. Given a machine learning task, you will\n",
    "                    help divide the task into these subtasks:\n",
    "                    - Subtask1: for classifying the problem type and task description\n",
    "                    - Subtask2: for identifying the input of the problem\n",
    "                    - Subtask3: for identifying the output of the problem\n",
    "                    describe the each subtask as specifically as possible\n",
    "                    \"\"\"\n",
    "    messages = [SystemMessage(content=system_prompt)]\n",
    "    messages.append(state['task'])\n",
    "    result = structured_llm.invoke(messages)\n",
    "    state['subtasks'] = result\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "14b15c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"task\": HumanMessage(content=task),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "78675acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = classify_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd41986",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling_agent(state: AgentState) -> AgentState:\n",
    "    system_prompt = f\"\"\"\n",
    "                    You are a machine learning expert assigned to select the **best-fit model** for a given task.\n",
    "\n",
    "                    ---\n",
    "\n",
    "                    ## 🎯 Your Goal\n",
    "\n",
    "                    Given:\n",
    "                    - A user task description\n",
    "                    - A list of available models (names and links)\n",
    "                    - A tool that fetches model details from a provided URL (description, input/output format, usage code)\n",
    "\n",
    "                    Your job is to:\n",
    "                    1. Analyze the task and identify what type of problem it is (e.g., classification, regression, NLP, etc.)\n",
    "                    2. Select a small number of candidate models whose **name or purpose suggests relevance**\n",
    "                    3. Use the **provided tool** to retrieve real details about each candidate model:\n",
    "                        - ✅ Model description\n",
    "                        - 📥 Input format\n",
    "                        - 📤 Output format\n",
    "                        - 🧪 Code sample (usage code snippet)\n",
    "\n",
    "                    ---\n",
    "\n",
    "                    ## ⚠️ VERY IMPORTANT RULES\n",
    "\n",
    "                    - ❌ DO NOT assume or invent any part of the model's description, input/output format, or code\n",
    "                    - ❌ DO NOT generate fake code or use your own knowledge about the model\n",
    "                    - ✅ ONLY use the **actual output** returned from the tool\n",
    "                    - ✅ Include tool content in your final answer exactly as returned (especially code)\n",
    "\n",
    "                    ---\n",
    "\n",
    "                    ## 📦 Available Models:\n",
    "\n",
    "                    {format_model_list(df_model)}\n",
    "\n",
    "                    ---\n",
    "\n",
    "                    ## 🧠 Output Structure (Final Answer)\n",
    "\n",
    "                    After retrieving tool results, choose ONE best-fit model and output the result in the following format:\n",
    "\n",
    "                    **✅ Model name and link**  \n",
    "                    `<model name>` — `<link>`\n",
    "\n",
    "                    **📝 Description (from tool):**  \n",
    "                    <model description>\n",
    "\n",
    "                    **📥 Input format (from tool):**  \n",
    "                    <description of expected input>\n",
    "\n",
    "                    **📤 Output format (from tool):**  \n",
    "                    <description of model output>\n",
    "\n",
    "                    **🧪 Example code (from tool):**  \n",
    "                    ```python\n",
    "                    <exact code snippet from tool>\n",
    "                    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"ai\", \"{agent_scratchpad}\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "\n",
    "    agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "    task_input = state[\"task\"].content if isinstance(state[\"task\"], SystemMessage) else state[\"task\"]\n",
    "\n",
    "    result = executor.invoke({\"input\": task_input})\n",
    "\n",
    "    return {\"output\": result[\"output\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d8f92e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34131/7332665.py:4: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  desc = str(row[1]).replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n",
      "/tmp/ipykernel_34131/7332665.py:5: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  url = str(row[2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_4'}`\n",
      "responded: To address the task of building a system that can identify fraudulent credit card transactions, we first need to understand the nature of the problem. It's a classification task where the goal is to predict whether a transaction is fraudulent or not based on a list of features related to the transaction.\n",
      "\n",
      "Given the problem description, the task seems to involve handling imbalanced data, where instances of fraudulent transactions are less common than legitimate ones. This is a common challenge in fraud detection.\n",
      "\n",
      "The input to the model is a list of features related to the transaction, specifically:\n",
      "- 28 features (V1, V2, …, V28) derived from PCA\n",
      "- 'Time': the number of seconds elapsed between the transaction and the first transaction in the dataset\n",
      "- 'Amount': the amount of the transaction\n",
      "\n",
      "The output is a classification of the transaction as either fraudulent (1) or not (0).\n",
      "\n",
      "Let's analyze the available models:\n",
      "\n",
      "1. **Nhận diện ảnh chữ viết tay** — This seems unrelated to the task at hand.\n",
      "2. **Phân loại văn bản thành các topic** — This is about text classification into topics, not relevant here.\n",
      "3. **Dự đoán giá trung bình nhà ở các quận Cali** — This involves regression, predicting house prices, not relevant.\n",
      "4. **Dự đoán giá trung bình nhà ở các quận Cali** — Similar to model 3, not relevant.\n",
      "5. **Phân loại gian lận thẻ tín dụng** — **This directly matches our task**.\n",
      "6. **Phân loại hình ảnh** — This is about image classification, not relevant.\n",
      "7. **Phân loại hình ảnh thành100 lớp đối tượng chi tiết (fine-grained),** — Also about image classification, not relevant.\n",
      "\n",
      "Based on the task description, the most suitable model seems to be model 5: **Phân loại gian lận thẻ tín dụng**.\n",
      "\n",
      "Let's use the `web_scrapper` tool to get more details about model 5.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m**YAML Metadata**\n",
      "**Warning:**\n",
      "empty or missing yaml metadata in repo card\n",
      "( [https://huggingface.co/docs/hub/model-cards#model-card-metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata))\n",
      "\n",
      "# 🛡️ Mô hình Phát hiện Gian lận Thẻ Tín dụng (Imbalanced Classification)\n",
      "\n",
      "## 📝 Mô tả\n",
      "\n",
      "Đây là mô hình nhị phân được huấn luyện để phát hiện gian lận trong giao dịch thẻ tín dụng, sử dụng tập dữ liệu từ Kaggle Credit Card Fraud Detection. Mô hình được xây dựng bằng Keras và xử lý vấn đề dữ liệu mất cân bằng nghiêm trọng (chỉ 0.18% giao dịch là gian lận).\n",
      "keras.io\n",
      "\n",
      "## 📌 Nhiệm vụ\n",
      "\n",
      "Loại bài toán: Phân loại nhị phân (gian lận / không gian lận)\n",
      "Dữ liệu đầu vào: Dữ liệu dạng bảng với 30 đặc trưng đã được PCA hóa\n",
      "Dữ liệu đầu ra: Xác suất giao dịch là gian lận\n",
      "\n",
      "## 📥 Đầu vào\n",
      "\n",
      "Shape: \\[batch\\_size, 30\\]\n",
      "Kiểu dữ liệu: np.ndarray hoặc tf.Tensor\n",
      "Chuẩn hóa: Dữ liệu đầu vào cần được chuẩn hóa tương tự như trong quá trình huấn luyện\n",
      "\n",
      "## 📤 Đầu ra\n",
      "\n",
      "Shape: \\[batch\\_size, 1\\]\n",
      "Ý nghĩa: Xác suất giao dịch là gian lận (giá trị từ 0 đến 1)\n",
      "\n",
      "🛠 Yêu cầu thư viện\n",
      "Cài đặt các thư viện cần thiết bằng:\n",
      "\n",
      "```bash\n",
      "pip install tensorflow numpy\n",
      "\n",
      "```\n",
      "\n",
      "## 🧪 Cách sử dụng mô hình\n",
      "\n",
      "Dưới đây là ví dụ về cách sử dụng mô hình:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from huggingface_hub import from_pretrained_keras\n",
      "\n",
      "# Tải mô hình từ Hugging Face Hub\n",
      "model = from_pretrained_keras(\"zhaospei/Model_4\")\n",
      "model.summary()\n",
      "\n",
      "# Tạo dữ liệu đầu vào mẫu (giá trị ngẫu nhiên)\n",
      "x_input = np.random.normal(size=(1, 30)).astype(\"float32\")\n",
      "\n",
      "# Dự đoán với mô hình\n",
      "prediction = model.predict(x_input)\n",
      "print(f\"Xác suất gian lận: {pred.flatten()[0][0]:.4f}\")\n",
      "\n",
      "```\n",
      "\n",
      "Downloads last month12\n",
      "\n",
      "\n",
      "Inference Providers [NEW](https://huggingface.co/docs/inference-providers)\n",
      "\n",
      "This model isn't deployed by any Inference Provider. [🙋Ask for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=zhaospei/Model_4&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bzhaospei%2FModel_4%5D(%2Fzhaospei%2FModel_4)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A)\n",
      "\n",
      "Inference providers allow you to run inference using different serverless providers.\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `web_scrapper` with `{'url': 'https://huggingface.co/zhaospei/Model_4'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m**YAML Metadata**\n",
      "**Warning:**\n",
      "empty or missing yaml metadata in repo card\n",
      "( [https://huggingface.co/docs/hub/model-cards#model-card-metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata))\n",
      "\n",
      "# 🛡️ Mô hình Phát hiện Gian lận Thẻ Tín dụng (Imbalanced Classification)\n",
      "\n",
      "## 📝 Mô tả\n",
      "\n",
      "Đây là mô hình nhị phân được huấn luyện để phát hiện gian lận trong giao dịch thẻ tín dụng, sử dụng tập dữ liệu từ Kaggle Credit Card Fraud Detection. Mô hình được xây dựng bằng Keras và xử lý vấn đề dữ liệu mất cân bằng nghiêm trọng (chỉ 0.18% giao dịch là gian lận).\n",
      "keras.io\n",
      "\n",
      "## 📌 Nhiệm vụ\n",
      "\n",
      "Loại bài toán: Phân loại nhị phân (gian lận / không gian lận)\n",
      "Dữ liệu đầu vào: Dữ liệu dạng bảng với 30 đặc trưng đã được PCA hóa\n",
      "Dữ liệu đầu ra: Xác suất giao dịch là gian lận\n",
      "\n",
      "## 📥 Đầu vào\n",
      "\n",
      "Shape: \\[batch\\_size, 30\\]\n",
      "Kiểu dữ liệu: np.ndarray hoặc tf.Tensor\n",
      "Chuẩn hóa: Dữ liệu đầu vào cần được chuẩn hóa tương tự như trong quá trình huấn luyện\n",
      "\n",
      "## 📤 Đầu ra\n",
      "\n",
      "Shape: \\[batch\\_size, 1\\]\n",
      "Ý nghĩa: Xác suất giao dịch là gian lận (giá trị từ 0 đến 1)\n",
      "\n",
      "🛠 Yêu cầu thư viện\n",
      "Cài đặt các thư viện cần thiết bằng:\n",
      "\n",
      "```bash\n",
      "pip install tensorflow numpy\n",
      "\n",
      "```\n",
      "\n",
      "## 🧪 Cách sử dụng mô hình\n",
      "\n",
      "Dưới đây là ví dụ về cách sử dụng mô hình:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from huggingface_hub import from_pretrained_keras\n",
      "\n",
      "# Tải mô hình từ Hugging Face Hub\n",
      "model = from_pretrained_keras(\"zhaospei/Model_4\")\n",
      "model.summary()\n",
      "\n",
      "# Tạo dữ liệu đầu vào mẫu (giá trị ngẫu nhiên)\n",
      "x_input = np.random.normal(size=(1, 30)).astype(\"float32\")\n",
      "\n",
      "# Dự đoán với mô hình\n",
      "prediction = model.predict(x_input)\n",
      "print(f\"Xác suất gian lận: {pred.flatten()[0][0]:.4f}\")\n",
      "\n",
      "```\n",
      "\n",
      "Downloads last month12\n",
      "\n",
      "\n",
      "Inference Providers [NEW](https://huggingface.co/docs/inference-providers)\n",
      "\n",
      "This model isn't deployed by any Inference Provider. [🙋Ask for provider support](https://huggingface.co/spaces/huggingface/InferenceSupport/discussions/new?title=zhaospei/Model_4&description=React%20to%20this%20comment%20with%20an%20emoji%20to%20vote%20for%20%5Bzhaospei%2FModel_4%5D(%2Fzhaospei%2FModel_4)%20to%20be%20supported%20by%20Inference%20Providers.%0A%0A(optional)%20Which%20providers%20are%20you%20interested%20in%3F%20(Novita%2C%20Hyperbolic%2C%20Together%E2%80%A6)%0A)\n",
      "\n",
      "Inference providers allow you to run inference using different serverless providers.\u001b[0m\u001b[32;1m\u001b[1;3m**✅ Model name and link**  \n",
      " `Phân loại gian lận thẻ tín dụng` — `https://huggingface.co/zhaospei/Model_4`\n",
      "\n",
      " **📝 Description (from tool):**  \n",
      " Đây là mô hình nhị phân được huấn luyện để phát hiện gian lận trong giao dịch thẻ tín dụng, sử dụng tập dữ liệu từ Kaggle Credit Card Fraud Detection. Mô hình được xây dựng bằng Keras và xử lý vấn đề dữ liệu mất cân bằng nghiêm trọng (chỉ 0.18% giao dịch là gian lận).\n",
      "\n",
      " **📥 Input format (from tool):**  \n",
      " Shape: [batch_size, 30]  \n",
      " Kiểu dữ liệu: np.ndarray hoặc tf.Tensor  \n",
      " Chuẩn hóa: Dữ liệu đầu vào cần được chuẩn hóa tương tự như trong quá trình huấn luyện\n",
      "\n",
      " **📤 Output format (from tool):**  \n",
      " Shape: [batch_size, 1]  \n",
      " Ý nghĩa: Xác suất giao dịch là gian lận (giá trị từ 0 đến 1)\n",
      "\n",
      " **🧪 Example code (from tool):**  \n",
      " ```python\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from huggingface_hub import from_pretrained_keras\n",
      "\n",
      "# Tải mô hình từ Hugging Face Hub\n",
      "model = from_pretrained_keras(\"zhaospei/Model_4\")\n",
      "model.summary()\n",
      "\n",
      "# Tạo dữ liệu đầu vào mẫu (giá trị ngẫu nhiên)\n",
      "x_input = np.random.normal(size=(1, 30)).astype(\"float32\")\n",
      "\n",
      "# Dự đoán với mô hình\n",
      "prediction = model.predict(x_input)\n",
      "print(f\"Xác suất gian lận: {pred.flatten()[0][0]:.4f}\")\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "res = modeling_agent(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3da68bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'output': '**✅ Model name and link**  \\n'\n",
      "           ' `Phân loại gian lận thẻ tín dụng` — '\n",
      "           '`https://huggingface.co/zhaospei/Model_4`\\n'\n",
      "           '\\n'\n",
      "           ' **📝 Description (from tool):**  \\n'\n",
      "           ' Đây là mô hình nhị phân được huấn luyện để phát hiện gian lận '\n",
      "           'trong giao dịch thẻ tín dụng, sử dụng tập dữ liệu từ Kaggle Credit '\n",
      "           'Card Fraud Detection. Mô hình được xây dựng bằng Keras và xử lý '\n",
      "           'vấn đề dữ liệu mất cân bằng nghiêm trọng (chỉ 0.18% giao dịch là '\n",
      "           'gian lận).\\n'\n",
      "           '\\n'\n",
      "           ' **📥 Input format (from tool):**  \\n'\n",
      "           ' Shape: [batch_size, 30]  \\n'\n",
      "           ' Kiểu dữ liệu: np.ndarray hoặc tf.Tensor  \\n'\n",
      "           ' Chuẩn hóa: Dữ liệu đầu vào cần được chuẩn hóa tương tự như trong '\n",
      "           'quá trình huấn luyện\\n'\n",
      "           '\\n'\n",
      "           ' **📤 Output format (from tool):**  \\n'\n",
      "           ' Shape: [batch_size, 1]  \\n'\n",
      "           ' Ý nghĩa: Xác suất giao dịch là gian lận (giá trị từ 0 đến 1)\\n'\n",
      "           '\\n'\n",
      "           ' **🧪 Example code (from tool):**  \\n'\n",
      "           ' ```python\\n'\n",
      "           'import numpy as np\\n'\n",
      "           'import tensorflow as tf\\n'\n",
      "           'from huggingface_hub import from_pretrained_keras\\n'\n",
      "           '\\n'\n",
      "           '# Tải mô hình từ Hugging Face Hub\\n'\n",
      "           'model = from_pretrained_keras(\"zhaospei/Model_4\")\\n'\n",
      "           'model.summary()\\n'\n",
      "           '\\n'\n",
      "           '# Tạo dữ liệu đầu vào mẫu (giá trị ngẫu nhiên)\\n'\n",
      "           'x_input = np.random.normal(size=(1, 30)).astype(\"float32\")\\n'\n",
      "           '\\n'\n",
      "           '# Dự đoán với mô hình\\n'\n",
      "           'prediction = model.predict(x_input)\\n'\n",
      "           'print(f\"Xác suất gian lận: {pred.flatten()[0][0]:.4f}\")\\n'\n",
      "           '```'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pprint.pprint(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "41077f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**✅ Model name and link**  \n",
      " `Phân loại gian lận thẻ tín dụng` — `https://huggingface.co/zhaospei/Model_4`\n",
      "\n",
      " **📝 Description (from tool):**  \n",
      " Đây là mô hình nhị phân được huấn luyện để phát hiện gian lận trong giao dịch thẻ tín dụng, sử dụng tập dữ liệu từ Kaggle Credit Card Fraud Detection. Mô hình được xây dựng bằng Keras và xử lý vấn đề dữ liệu mất cân bằng nghiêm trọng (chỉ 0.18% giao dịch là gian lận).\n",
      "\n",
      " **📥 Input format (from tool):**  \n",
      " Shape: [batch_size, 30]  \n",
      " Kiểu dữ liệu: np.ndarray hoặc tf.Tensor  \n",
      " Chuẩn hóa: Dữ liệu đầu vào cần được chuẩn hóa tương tự như trong quá trình huấn luyện\n",
      "\n",
      " **📤 Output format (from tool):**  \n",
      " Shape: [batch_size, 1]  \n",
      " Ý nghĩa: Xác suất giao dịch là gian lận (giá trị từ 0 đến 1)\n",
      "\n",
      " **🧪 Example code (from tool):**  \n",
      " ```python\n",
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from huggingface_hub import from_pretrained_keras\n",
      "\n",
      "# Tải mô hình từ Hugging Face Hub\n",
      "model = from_pretrained_keras(\"zhaospei/Model_4\")\n",
      "model.summary()\n",
      "\n",
      "# Tạo dữ liệu đầu vào mẫu (giá trị ngẫu nhiên)\n",
      "x_input = np.random.normal(size=(1, 30)).astype(\"float32\")\n",
      "\n",
      "# Dự đoán với mô hình\n",
      "prediction = model.predict(x_input)\n",
      "print(f\"Xác suất gian lận: {pred.flatten()[0][0]:.4f}\")\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(res[\"output\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
